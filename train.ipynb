{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files\n",
    "TRAINING_DIRECTORY = 'cnn/stories/'\n",
    "EXTENSION = '.story'\n",
    "MAX_FILES = 50\n",
    "\n",
    "# tokenization\n",
    "FILTERS = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'  # default minus >, <\n",
    "OOV_TOKEN = '<unk>'\n",
    "BEGIN_TOKEN = 1\n",
    "END_TOKEN = 2\n",
    "NUM_WORDS = 600\n",
    "\n",
    "# MODEL_PARAMS\n",
    "MAX_SEQUENCE_LEN = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['cnn/stories/0001d1afc246a7964130f43ae940af6bc6c57f01.story',\n",
       " 'cnn/stories/0002095e55fcbd3a2f366d9bf92a95433dc305ef.story',\n",
       " 'cnn/stories/00027e965c8264c35cc1bc55556db388da82b07f.story',\n",
       " 'cnn/stories/0002c17436637c4fe1837c935c04de47adb18e9a.story',\n",
       " 'cnn/stories/0003ad6ef0c37534f80b55b4235108024b407f0b.story',\n",
       " 'cnn/stories/0004306354494f090ee2d7bc5ddbf80b63e80de6.story',\n",
       " 'cnn/stories/0005d61497d21ff37a17751829bd7e3b6e4a7c5c.story',\n",
       " 'cnn/stories/0006021f772fad0aa78a977ce4a31b3faa6e6fe5.story',\n",
       " 'cnn/stories/00083697263e215e5e7eda753070f08aa374dd45.story',\n",
       " 'cnn/stories/000940f2bb357ac04a236a232156d8b9b18d1667.story',\n",
       " 'cnn/stories/0009ebb1967511741629926ef9f5faea2bb6be24.story',\n",
       " 'cnn/stories/000c835555db62e319854d9f8912061cdca1893e.story',\n",
       " 'cnn/stories/000ca3fc9d877f8d4bb2ebd1d6858c69be571fd8.story',\n",
       " 'cnn/stories/000cd1ee0098c4d510a03ddc97d11764448ebac2.story',\n",
       " 'cnn/stories/000e009f6b1d954d827c9a550f3f24a5474ee82b.story',\n",
       " 'cnn/stories/001097a19e2c96de11276b3cce11566ccfed0030.story',\n",
       " 'cnn/stories/0010c870d3fc53ea7f2a4a50f6496dc2df17e02f.story',\n",
       " 'cnn/stories/00120f91cfcab17bac165f7a4719019a628a9db3.story',\n",
       " 'cnn/stories/00128f1ba30d5e9e0f17df83285a1bc2072e2f01.story',\n",
       " 'cnn/stories/00142065a6c92f788eacd40c9023184808a7e2d1.story',\n",
       " 'cnn/stories/0015194573f9b4430319683cde41e4aa17092a9d.story',\n",
       " 'cnn/stories/00156d9892fb27f1d2e100cbdd8a3997f8273781.story',\n",
       " 'cnn/stories/001732b374f362d3961a510da315601e4b5e7e84.story',\n",
       " 'cnn/stories/001789cf9b865dcac3d9fc032a6b1533e3318eda.story',\n",
       " 'cnn/stories/00189f37b1c8bdc2b132b40270bb28ffcc622af1.story',\n",
       " 'cnn/stories/001a6162391594e2a8607fba135bdfa154e57904.story',\n",
       " 'cnn/stories/001adf6209be103cb198b8599f236b4d5760a5fe.story',\n",
       " 'cnn/stories/001b4673dbb3437282cd2ea58d9eca471e25780f.story',\n",
       " 'cnn/stories/001be24b2db1c04f62386f98997fee725c5fd2fb.story',\n",
       " 'cnn/stories/001c839e1d76c400129f6c2799957c74e9895815.story',\n",
       " 'cnn/stories/001cdbaf0607878f332e0202fadf5b82d2997c02.story',\n",
       " 'cnn/stories/001d9259673bd2ffb613217d19b98ca3563874ac.story',\n",
       " 'cnn/stories/001e8bda2f7ab73bf81314c1639a97dae2751703.story',\n",
       " 'cnn/stories/001ee59c375363263821474d40e4386ab91d5145.story',\n",
       " 'cnn/stories/001f2856c2bca7de7918eb155b2e1bc8aa0d8695.story',\n",
       " 'cnn/stories/001f9c554f1a29169413d0d2f138212a14c6dcf1.story',\n",
       " 'cnn/stories/001fb4ca3bd3a0c1cd91fdc813f0ebeeac678e76.story',\n",
       " 'cnn/stories/00200e794fa41d3f7ce92cbf43e9fd4cd652bb09.story',\n",
       " 'cnn/stories/0020ede07ee7ad1f6cf654c7dc678e7341d0c0e5.story',\n",
       " 'cnn/stories/00211175819295755ed12e89791b4d543442981a.story',\n",
       " 'cnn/stories/002124147bd10996f410d1a117ddf298018dbced.story',\n",
       " 'cnn/stories/002175ac42ef0c91b9fb7e07259413a8ee3979a3.story',\n",
       " 'cnn/stories/0021eb1696f522ae0605ba630568bed5f655b740.story',\n",
       " 'cnn/stories/0021fe8d65bd0d6d76d5fefba2ac02f0c48a43f4.story',\n",
       " 'cnn/stories/0022944fc603b8c03c02f2e67a874b067db28196.story',\n",
       " 'cnn/stories/00237a1d4477c976ee17d82d438e2650e0bff9a9.story',\n",
       " 'cnn/stories/0024a4404120db4544060c11050acaaae185f315.story',\n",
       " 'cnn/stories/002509a01890dd51476aa84c634b6c1db306f995.story',\n",
       " 'cnn/stories/00258c300ca824ab90db12db58480b00040a96d1.story',\n",
       " 'cnn/stories/002756fbd876d0da1c7d6e6d464c25d5316de3e8.story',\n",
       " 'cnn/stories/0028aa521d2b3363a2a2d7d74b2961c2c5a8f514.story',\n",
       " 'cnn/stories/0028e335e8d06f2bab786a15965621db28e2ba55.story',\n",
       " 'cnn/stories/0029837648b848c9cbde4c26744609e85f528a6c.story',\n",
       " 'cnn/stories/002a083c3893b1fde734280b9eec28d428a02d2b.story',\n",
       " 'cnn/stories/002a48549068d7db596738c4f3fa2f5562372ab3.story',\n",
       " 'cnn/stories/002a6553123aa9c80a49b1e6e54c5a684975e452.story',\n",
       " 'cnn/stories/002ab555011a771d8390288495066e30028292ee.story',\n",
       " 'cnn/stories/002b864bf68d00d74d0cad76be4cbc049f7321ca.story',\n",
       " 'cnn/stories/002c115c31577d0e9443794d27c2df51312858b4.story',\n",
       " 'cnn/stories/002c715ea1428373cc432c9508d4a48d2e6069f4.story',\n",
       " 'cnn/stories/002c962834b7886c600a31a35053543e324883bd.story',\n",
       " 'cnn/stories/002cceec34994ff6ff91f6232054e5f71b9eb4b5.story',\n",
       " 'cnn/stories/0031f262451b4b335dfda3aca9b533d244244e1e.story',\n",
       " 'cnn/stories/0032962a65bf404388581ef85d501c048eb67987.story',\n",
       " 'cnn/stories/0032d07e91b8ce3d5f8ac313d799341df7556234.story',\n",
       " 'cnn/stories/00330ad079a460a329fa0977f2db3810044f23af.story',\n",
       " 'cnn/stories/0033829349643e2964a8c3ce5fd997116ce43f7c.story',\n",
       " 'cnn/stories/003477088ee670dee05e53d0b24ebfcf9a692cec.story',\n",
       " 'cnn/stories/00359f516cdf8b1800c7102711bd9aa400d1c749.story',\n",
       " 'cnn/stories/0036c48d80c270465bffced3e233fe39e5950431.story',\n",
       " 'cnn/stories/0036d1138155e2644813f9c218c743e982e27a27.story',\n",
       " 'cnn/stories/00377ab9d3caafb18464c47d0535ae2781aeef15.story',\n",
       " 'cnn/stories/00385e5382f413e0c4234ddf71d692ed1e89622f.story',\n",
       " 'cnn/stories/003a70daf9f99e871ba06482726176d7776d0c07.story',\n",
       " 'cnn/stories/003ae6f81b90eae41831361deae8a5da3705dc4f.story',\n",
       " 'cnn/stories/003b856704634429282caa28ef8b0c052e04bf18.story',\n",
       " 'cnn/stories/003be5a8108fcd3f133ddef14a5fc593701566f3.story',\n",
       " 'cnn/stories/003c99210ffb08a21ff004000cac7e2efcfd48de.story',\n",
       " 'cnn/stories/003cefcdcf96dbca76aa17d72ee612fa3b40651d.story',\n",
       " 'cnn/stories/003d23a256ce34d73e05968a04727c0ed4a2a456.story',\n",
       " 'cnn/stories/003d773c00279077fc83986a725be77c3ef0740f.story',\n",
       " 'cnn/stories/003eaac389a2450171f99f216b95f8741b58cbf0.story',\n",
       " 'cnn/stories/003f8c8953025e086fa773c9b40d9b8cd6d9754c.story',\n",
       " 'cnn/stories/0040eb79bd5424f52e1ec43050539e4e12189cbf.story',\n",
       " 'cnn/stories/004135e34c22bc5c1954f0cfa3c627943486d0b9.story',\n",
       " 'cnn/stories/0041698b4463a633f912681b96f73648cb012e33.story',\n",
       " 'cnn/stories/0043419182bdb005875dd3da575ebe38570907e8.story',\n",
       " 'cnn/stories/0043612320aaede50bf515eacc66c7daeee9a21b.story',\n",
       " 'cnn/stories/0044e296ecfe3ba57a351ad2a36d034491e878ce.story',\n",
       " 'cnn/stories/00459587adf060c93966ce7f1e2e33fae1d87f7c.story',\n",
       " 'cnn/stories/004636d2b06ffc786148189e4d8e5e950bd46e8f.story',\n",
       " 'cnn/stories/00464528a306e3c679fdaf61f60c42ba6f7db6ba.story',\n",
       " 'cnn/stories/00465603227f7f56fcd37e10f4cd44e57d7647d8.story',\n",
       " 'cnn/stories/0046a356cc90e393be41135da101f83da40ee6a3.story',\n",
       " 'cnn/stories/004971d00a8b29b67597b2baad67e813dbbf4b66.story',\n",
       " 'cnn/stories/004b513bb2f93dca70386dad16c9fb5770ebdc49.story',\n",
       " 'cnn/stories/004b7045c61fab2a7189f8c09cae11ef1d4c5cfc.story',\n",
       " 'cnn/stories/004cc1a4ecd2ce6327d67b801756847063e48c28.story',\n",
       " 'cnn/stories/004d2253b243331ec330b4bc188f9289fc5730f6.story',\n",
       " 'cnn/stories/004e6e14b9498ae85e92cbb81adf76ac8b21f472.story',\n",
       " 'cnn/stories/004e6e935ea530b0992a89fd1307f7f41f4a234d.story',\n",
       " 'cnn/stories/004f0f8c694c4b546b29565a8993a555537ff561.story',\n",
       " 'cnn/stories/004fc12e7cd2505a013d96e816afae3f3ce5015d.story',\n",
       " 'cnn/stories/00504275ede73591d94a6c1f994fd4856610421c.story',\n",
       " 'cnn/stories/00512126d65bf2a36801e4ef37f28c86c29deb28.story',\n",
       " 'cnn/stories/0054d6d30dbcad772e20b22771153a2a9cbeaf62.story',\n",
       " 'cnn/stories/0055114867593ed8c267876b2d4bf0d2f5a97b88.story',\n",
       " 'cnn/stories/005538eba9ddc63dc5d11ee3ef19643f5a2b6af3.story',\n",
       " 'cnn/stories/0055d0207fa47067895280bb1066720c68872ed7.story',\n",
       " 'cnn/stories/005670c4a85a3122965180585a88682692b8d1b9.story',\n",
       " 'cnn/stories/0056b7222ef0324ef31cdf2dc605a60744d56d78.story',\n",
       " 'cnn/stories/0057863e126ceb0f22053aa1570a14977e5803ff.story',\n",
       " 'cnn/stories/00579a91246db0df52a7106cc6650c56c9fbc604.story',\n",
       " 'cnn/stories/0057e887ec89de912dd8141a60b746194f95ea62.story',\n",
       " 'cnn/stories/005804231660cfba04a959d6d88cae81d144c2e2.story',\n",
       " 'cnn/stories/0058a370ff12386a55b24ee56dd277e9c3b23a72.story',\n",
       " 'cnn/stories/0059bb8b2718a83dfffa071bfbc6b49c2bc0ed13.story',\n",
       " 'cnn/stories/005a250b7a98f9a5b26906b62279940b050cf8c8.story',\n",
       " 'cnn/stories/005a56d12cf2c1335c83e387ded82fc61eac780e.story',\n",
       " 'cnn/stories/005acf7ab98656d8a9df9cc305cf0de84545b35a.story',\n",
       " 'cnn/stories/005b8761aacb2354318700e781da8e3a70796617.story',\n",
       " 'cnn/stories/005d4294912831a4a6d30fbaab0a50e1d0d72072.story',\n",
       " 'cnn/stories/005d650be8cfb6db3eee51d92188ac64509eb59c.story',\n",
       " 'cnn/stories/005ed10e2fa927c69aa9d881c51ccca9ce2c789c.story',\n",
       " 'cnn/stories/005fd0fe0dda2d0e94cc8279b91cef2c0fb4a704.story',\n",
       " 'cnn/stories/00611a031b1863643d38a007ed01ce5f40e88c79.story',\n",
       " 'cnn/stories/00611c530db65df5cd6f2cd4fc9307567bd6cfc1.story',\n",
       " 'cnn/stories/0061446395a8011d5d924c63bd1f4c63d0f68404.story',\n",
       " 'cnn/stories/00618e25f8475c67e5be3528022efa26bdd5258f.story',\n",
       " 'cnn/stories/0061dddffc23dab36031475efdc679ab13be3a50.story',\n",
       " 'cnn/stories/0061e7c11acc1f8f4c4f423db11f38df39b748b7.story',\n",
       " 'cnn/stories/0064cb4483aea62ced1d8ed67c8129497bb303cb.story',\n",
       " 'cnn/stories/006652b4ba431e44baec92d84fe62c4727983642.story',\n",
       " 'cnn/stories/006684f2e2fa98b707502c549ab1a0ea0d22a3db.story',\n",
       " 'cnn/stories/0066b77d259f645ce9226011d4aa13121a8ac57b.story',\n",
       " 'cnn/stories/00672d5c2055608b747a90a0f5ae32c5b340173e.story',\n",
       " 'cnn/stories/006754d905db204ca0ab0d99ac8a2594bdcffe62.story',\n",
       " 'cnn/stories/0067f67951e7fcf49b83de3544ef53f8d8d34c85.story',\n",
       " 'cnn/stories/0068a4b536d5c968ae74540d550a6adacee6a8f9.story',\n",
       " 'cnn/stories/00697ea6b46a074bd1d94417297b8f9c95db4d31.story',\n",
       " 'cnn/stories/0069a5a18a299389ea9fa6d48ea308056fe08e13.story',\n",
       " 'cnn/stories/006a59c662a82db6c90b550b81abc7d7490311aa.story',\n",
       " 'cnn/stories/006bca7d18b3c6889ce567133566d22e491d27c1.story',\n",
       " 'cnn/stories/006dd02051a6d228fa3841ab555fced21fccdf1e.story',\n",
       " 'cnn/stories/006e02fe8bb2b880c9ac1c2547dcaca5bff57263.story',\n",
       " 'cnn/stories/006f05c3183eacea289414270c02872627f4a0b4.story',\n",
       " 'cnn/stories/006f475a6f4c972d20c491433156a6c28cfacd99.story',\n",
       " 'cnn/stories/006f6b4b073292295b28ef6e1a86d7a335496a5c.story',\n",
       " 'cnn/stories/006f72f0ad668d923074eac5fdd5a11d5931ca95.story',\n",
       " 'cnn/stories/006fca550042888454e7261eb578ea37febed6d1.story',\n",
       " 'cnn/stories/00711381cc1b5f7644d97d4a0829244fbdbd3d57.story',\n",
       " 'cnn/stories/00716be72be8cf48cc23ac3b4b8924e569628be2.story',\n",
       " 'cnn/stories/0071739400e34d34f53c075502a4dbdd47a5f686.story',\n",
       " 'cnn/stories/00719cbc72803002a6a5d83e5f260dbe1f52990d.story',\n",
       " 'cnn/stories/0071c3032bb2555ad11df4662be90094a643cb09.story',\n",
       " 'cnn/stories/00724c0377fc4af3032c3d9bed712ed283917d3c.story',\n",
       " 'cnn/stories/0074ce973a86e2c417990950a9c9d24b4699f858.story',\n",
       " 'cnn/stories/0075795e722ebab52dca94aa77e57d5697f3b5bf.story',\n",
       " 'cnn/stories/00759239d90551d978bbe5298d344fc0c1317dff.story',\n",
       " 'cnn/stories/00764d9499baed37590be445138c643edf121dcb.story',\n",
       " 'cnn/stories/0076c94a296cbd799c3ae46f55072fac726c3a4a.story',\n",
       " 'cnn/stories/0076f01b59fbd43bb1f6fe3d00e70f30622befdb.story',\n",
       " 'cnn/stories/007880654accdde3b5cdbf102d44104c9c62f3ea.story',\n",
       " 'cnn/stories/00790f55c293d0544e93bcba78bef3635e1f6a63.story',\n",
       " 'cnn/stories/00795ad78e907ccc39d66c327b1509b3cdf99f1c.story',\n",
       " 'cnn/stories/0079c51f12f563e5224ac9cb5125f5ee15f5747f.story',\n",
       " 'cnn/stories/007a3697265477abf9f8f467366a0303bec5a446.story',\n",
       " 'cnn/stories/007a8f8c0af4b027d9aaf3e7b7937e51e53bfea7.story',\n",
       " 'cnn/stories/007ae95fd859ef30117b9c80232b7b134b370ea3.story',\n",
       " 'cnn/stories/007b8fb3014d8d7b4266cf85ba1f368cfba44801.story',\n",
       " 'cnn/stories/007c4a5dfd21e5339f96c9c244c6c7d91168a4be.story',\n",
       " 'cnn/stories/007ddc6d5805b3c515bc2ca8843dcff28ec4903c.story',\n",
       " 'cnn/stories/0080a94a2c7a15e2e9d6e1705510e1b2e662b894.story',\n",
       " 'cnn/stories/0080ea67d5fbfb2648a23ee827d84b80a9b0a37a.story',\n",
       " 'cnn/stories/00817ca0e09d8e05ba582ded19256d2573b1561a.story',\n",
       " 'cnn/stories/0081d83c35c6822e64bdf61c878dfdce783294d0.story',\n",
       " 'cnn/stories/00820005201f0513a5ff6defb5494c1c5160ef79.story',\n",
       " 'cnn/stories/008208e9995f28b903d54a64278449f8de4cbe61.story',\n",
       " 'cnn/stories/00828c37629fb44c262d55642efb4a008291b79b.story',\n",
       " 'cnn/stories/0082f772bb8d19120e5df5a125a2da77fbc31679.story',\n",
       " 'cnn/stories/0084322d7508217cfb07b397557cfdf7add2b4df.story',\n",
       " 'cnn/stories/0085473257680f8f27508d3f286e51aaccd0e8d8.story',\n",
       " 'cnn/stories/0085ffb92f2b33b9dc68fd86388429efe9c77bea.story',\n",
       " 'cnn/stories/0087fadf3c9f54d10eb0db0b034d0d0c50c494e1.story',\n",
       " 'cnn/stories/0089064db0f5619d38cad4aa831cdd32e7616356.story',\n",
       " 'cnn/stories/008926ebc65930f5ff1ddc076941b67cc3ff5fb7.story',\n",
       " 'cnn/stories/008938023918790ae724ddbff92ab6b43de68ce9.story',\n",
       " 'cnn/stories/008baa186fecc3a0f957b01a8120a97bb93a535e.story',\n",
       " 'cnn/stories/008c60559b77797f8220759f35f5f6e8381ee030.story',\n",
       " 'cnn/stories/008fc24ca9f4c48a54623bef423a3f2f8db8451a.story',\n",
       " 'cnn/stories/008fe7b1213d06fbfe9fd7ebb04cf7dc2d824195.story',\n",
       " 'cnn/stories/009030808b13ef12a2a19d59833d0e696ebeb68d.story',\n",
       " 'cnn/stories/0090a2bf1f1ce8a6ef96cebb966a0d465ea0da79.story',\n",
       " 'cnn/stories/0090fab6c9e3b8eea64878eac047e8380d332c71.story',\n",
       " 'cnn/stories/00916d7657c98e138640795011f4136014425977.story',\n",
       " 'cnn/stories/0093686605ee0b9d97c23350a4ccf30b834d162f.story',\n",
       " 'cnn/stories/0095ce085581314285f894af73a55ea9ef003412.story',\n",
       " 'cnn/stories/00962710a65712639b1732041fd9553f5b658749.story',\n",
       " 'cnn/stories/00971e34b7c8fb70119ac5c1886bf024b020dca1.story',\n",
       " 'cnn/stories/009a362400a857382e1556a5f4d862e02ee25a50.story',\n",
       " 'cnn/stories/009ab18d1c06b1a31a98fa73b5124132d37555cc.story',\n",
       " 'cnn/stories/009b92670e692dfa2056a70aad8fed9cff643346.story',\n",
       " 'cnn/stories/009bc4ed85ee998f320502ab3138d5f18bec0f90.story',\n",
       " 'cnn/stories/009c6499ac040e30a18b547bb153f180255ba8f6.story',\n",
       " 'cnn/stories/009c67a0ee08dbc76c46f5af3315ff9f86779deb.story',\n",
       " 'cnn/stories/009de808cb24801b7dc1f85ed875b3d92edbc477.story',\n",
       " 'cnn/stories/009e2c2f7a557cd6c431255707cf863f617076d6.story',\n",
       " 'cnn/stories/009e53253b0ba1823c4a08dfd6e4446ca9b02388.story',\n",
       " 'cnn/stories/009e7d73f3131b3d814ba6599858be4f2d69ddfb.story',\n",
       " 'cnn/stories/00a2aef1e18d125960da51e167a3d22ed8416c09.story',\n",
       " 'cnn/stories/00a308681faf9c82a0e62a89b21fcdefb84b88fa.story',\n",
       " 'cnn/stories/00a340f3a884fcbdad7c0399782d9ca9d2d68ef7.story',\n",
       " 'cnn/stories/00a39c134080b6f215a81c15d46c3ac7cc7bdcf3.story',\n",
       " 'cnn/stories/00a51966da4726dc1a3fc7b8c148866a9a23581a.story',\n",
       " 'cnn/stories/00a51d5454f2ef7dbf4c53471223a27fb9c20681.story',\n",
       " 'cnn/stories/00a570272e671a8fad94ffe0927a04c1bfc4110e.story',\n",
       " 'cnn/stories/00a57bef588a7e7efd0438fba62f07356c59b036.story',\n",
       " 'cnn/stories/00a6c0823ba92e5a055d90133e8b66fa9028c2d7.story',\n",
       " 'cnn/stories/00a7132b9c9a646852ac4a606acb71e3a6acb8c6.story',\n",
       " 'cnn/stories/00a79ca59be700e845bf9e90bf08c5106b4a72bb.story',\n",
       " 'cnn/stories/00a7e6767322697aa9236e2135aecc9785c6b410.story',\n",
       " 'cnn/stories/00a9a975bde07882d77b6fc4382589791b92ef00.story',\n",
       " 'cnn/stories/00aa8e3ba59f5fe71a096a5549f80faece10aa83.story',\n",
       " 'cnn/stories/00aafe70cc9129b3215b62c161d5692164a07494.story',\n",
       " 'cnn/stories/00ac6fa0167e99167565ab9a12c1c96bf48ac414.story',\n",
       " 'cnn/stories/00ac882e1a7f4862fa9f1e863b738966625f554d.story',\n",
       " 'cnn/stories/00ac91a5794537f1f37a8962a41fea6dbea2779b.story',\n",
       " 'cnn/stories/00ae31ff5ede1de6cd39eccf36bae935c882527b.story',\n",
       " 'cnn/stories/00aee24828cbceb3857579c59c9a85f381e358a5.story',\n",
       " 'cnn/stories/00aef73f30847c2ae2326e26811065f0308157df.story',\n",
       " 'cnn/stories/00af3b469010e46f154734cfba77bb001791517f.story',\n",
       " 'cnn/stories/00afa094e59946ddc8129b36146e416c02109030.story',\n",
       " 'cnn/stories/00afa371dfbfc7832433f22da65ca96fe9997560.story',\n",
       " 'cnn/stories/00b0ebf0c27ebcc181e6c60070a9031f41b7d1d8.story',\n",
       " 'cnn/stories/00b164fcf3e920125153d4e4bb59f4ceb5ddb1f7.story',\n",
       " 'cnn/stories/00b287c19561791776e970936f96273b9ececc15.story',\n",
       " 'cnn/stories/00b2c695b7e98960539f962e7e99c20616382314.story',\n",
       " 'cnn/stories/00b3cebdfa5caccbddb4c701689e2dfddd89a3d4.story',\n",
       " 'cnn/stories/00b4620df3af0867c75d25f09ded465961d0606d.story',\n",
       " 'cnn/stories/00b4cdefd9bc4dcdd55a6162f0d61e16d1b8ae8d.story',\n",
       " 'cnn/stories/00b5faa0f7261d78ef02a9f23262b5a1eb82a6d0.story',\n",
       " 'cnn/stories/00b8ba6068767f8072d47b4b01847195a35d00f1.story',\n",
       " 'cnn/stories/00b98ede70285565df7b273b04b8ac92de6bae71.story',\n",
       " 'cnn/stories/00b9bf421e847b9b77436076ccc7297bc9c9ed0d.story',\n",
       " 'cnn/stories/00b9fd732f0e48dc658acc01e4651cc7f5f04f2c.story',\n",
       " 'cnn/stories/00ba4ffac8d6adb056293063ada921a22bfa3f33.story',\n",
       " 'cnn/stories/00ba519cf87a2d49dfb48fadf2af0112dc0e4947.story',\n",
       " 'cnn/stories/00bad06b94b77274e6f7e33e2a84e473f938cdac.story',\n",
       " 'cnn/stories/00bb48094f8aed9b117bea36ef649b95b2bd823f.story',\n",
       " 'cnn/stories/00bc05e4268dac71709ec667b4f91e5b11f8c264.story',\n",
       " 'cnn/stories/00bc0995c4cae067d6791a8854b12ce7688564b3.story',\n",
       " 'cnn/stories/00bc0db28043d0d0fdedce94a6a1bc41835a9326.story',\n",
       " 'cnn/stories/00bcc69870cdffe5e108a7be95754c3476bf9082.story',\n",
       " 'cnn/stories/00be40289d0aea65afd2da6431db8a9fc8efc9c0.story',\n",
       " 'cnn/stories/00becc1bada9c481a7bfc7d4f972ce62db322e26.story',\n",
       " 'cnn/stories/00c0b04d497ec89d3a3fcce4d708cbe6b551a6e3.story',\n",
       " 'cnn/stories/00c2a214ffb5790f4c2c7915ebaa973b38e854e7.story',\n",
       " 'cnn/stories/00c39e8fc9a109be765e67361199c9e10a34dc34.story',\n",
       " 'cnn/stories/00c45eb98a06f9218170edf5767617cc20991840.story',\n",
       " 'cnn/stories/00c478df3d3395655a87de8b5f638a69924c4158.story',\n",
       " 'cnn/stories/00c483f15bec92e9da8f69c653f0855dd964caa6.story',\n",
       " 'cnn/stories/00c4ea03538bce1605e48ba2b9bfeb0063328434.story',\n",
       " 'cnn/stories/00c5289a96ac169303bb05c3b76cbe2c0cf94ba2.story',\n",
       " 'cnn/stories/00c59c2992fecbda9a3b172b46bff6ccb03f935c.story',\n",
       " 'cnn/stories/00c67840a853c432c2eae369b58a2119d960ada5.story',\n",
       " 'cnn/stories/00c782536b21482ace8a627e2965e0060ffc1dbb.story',\n",
       " 'cnn/stories/00c8b8b4e75ad7cb540e6bf7034942452c043d53.story',\n",
       " 'cnn/stories/00ca161ba34c2a37c4dbe0bf1fa85d6dece7d77f.story',\n",
       " 'cnn/stories/00cb03ed4ee4f0b4fb5467e5cbbab6eb28c773b8.story',\n",
       " 'cnn/stories/00cb16eb70b0124bb61e45b4df696a2d3598213f.story',\n",
       " 'cnn/stories/00cbf6c491d54dfbbe06c18d8ec2a728b82c3e16.story',\n",
       " 'cnn/stories/00cdede32ff010099a4e8c8288d1f494458cb941.story',\n",
       " 'cnn/stories/00ce20a94d52dcdfc7f18f90cbea677857c90434.story',\n",
       " 'cnn/stories/00ce61a751fa38e7e09d3496fb737dff33da3e58.story',\n",
       " 'cnn/stories/00ce620aadad194da337e972749dba7d602da206.story',\n",
       " 'cnn/stories/00cecccb0ec47f45c40c80e530b2bc03f819ffc7.story',\n",
       " 'cnn/stories/00d09a220e0c94981fd4b908f61b3fa7bbfbf7b6.story',\n",
       " 'cnn/stories/00d0cef3aa2f1e6204896380413263a32a2407a9.story',\n",
       " 'cnn/stories/00d10c9be83b619a4b8f9127d86ad7a4d15b3b09.story',\n",
       " 'cnn/stories/00d3bd3f71f18dc7ccddf0b655550dfeff898961.story',\n",
       " 'cnn/stories/00d48bf5089a093edf12ef5f3836780e97be6be0.story',\n",
       " 'cnn/stories/00d4b35f860ceb2b8b9af2202841da09e8b8f058.story',\n",
       " 'cnn/stories/00d6c23d8d98f405514749f06899587cc821fb1c.story',\n",
       " 'cnn/stories/00d6d510d38442787f99a722e750d65a66607284.story',\n",
       " 'cnn/stories/00d74e3677bd255c5ab794753f4bbffcfae19aef.story',\n",
       " 'cnn/stories/00d84c304738744d8339c586c4f07f65009b3424.story',\n",
       " 'cnn/stories/00d8a762a619f1ce4031932adc067328691eb729.story',\n",
       " 'cnn/stories/00d95d676d70a8f6b4a4619f021465a01c2b404f.story',\n",
       " 'cnn/stories/00d9f218a16079131179488b98c3b801ddf94fa1.story',\n",
       " 'cnn/stories/00dad0b2f84e7aa5c935a65130f90d64de8a65a6.story',\n",
       " 'cnn/stories/00dad5d893f836f53080de50e9b983ad2f9bcd22.story',\n",
       " 'cnn/stories/00db4c8603de157b4da42886b946378ff2275712.story',\n",
       " 'cnn/stories/00dc77ce6c1d10bf1160829109b4f7d2a450823b.story',\n",
       " 'cnn/stories/00ddb03229403b968878387f0025db684f0b9682.story',\n",
       " 'cnn/stories/00dddbedf41ec993a8b976f3cce2dd8ca2c7efed.story',\n",
       " 'cnn/stories/00de294185f9dbece4a827347c4f44f3c9770ed9.story',\n",
       " 'cnn/stories/00e08179e8271c59babd5eac35797a4b4b835cc4.story',\n",
       " 'cnn/stories/00e0938100e0be74e844d2f0a9fce75444b8ebb7.story',\n",
       " 'cnn/stories/00e0afc8576abbd4c3971d1a796f8e8fbcdf7f3c.story',\n",
       " 'cnn/stories/00e0e7f15a58849ead95c1afd11d9cebf047a848.story',\n",
       " 'cnn/stories/00e17a1d97c45423c4139c4cea15180cc2c8b3b9.story',\n",
       " 'cnn/stories/00e25cd73a0992fca39f7477d0d906b43d7525f6.story',\n",
       " 'cnn/stories/00e2741c0cd0c660fcc241e79a2aedc0af5469e0.story',\n",
       " 'cnn/stories/00e2a99fac3d40621d262d9314b50ad94ded0536.story',\n",
       " 'cnn/stories/00e2f25d74e2b6c2db4ddf978f8e1170fd765d82.story',\n",
       " 'cnn/stories/00e3fc2a5217a99769fce321775bb8db73cc2737.story',\n",
       " 'cnn/stories/00e45018727b94b80f2d04301b9956616d70f562.story',\n",
       " 'cnn/stories/00e4f4ea4579fb9752134e5390e3bb24f8aa8e3d.story',\n",
       " 'cnn/stories/00e5eb6c1af59233b661e59b0954a4f84a2f3904.story',\n",
       " 'cnn/stories/00e68157235e58b02d724851afe4cd2aa12640ed.story',\n",
       " 'cnn/stories/00e87dec09d988e3c457ae44c318c79fe7160de8.story',\n",
       " 'cnn/stories/00e8a1c6e038a833ed30d92f911267a2f8f238be.story',\n",
       " 'cnn/stories/00e99c98b893f988818f5f846ed2253b317b5030.story',\n",
       " 'cnn/stories/00ea73b90918a93451d4f2b4f1dd19f4e02c2bfe.story',\n",
       " 'cnn/stories/00eaf1c06f1e3ecd48160b43f3b3cd196a319c8a.story',\n",
       " 'cnn/stories/00eb52284bc064db000901988fb36173446e22c5.story',\n",
       " 'cnn/stories/00eba10e15485b053b2246b587b83a8c51394601.story',\n",
       " 'cnn/stories/00ebb7109898734bde3ead005ebbc591cf8dbcc9.story',\n",
       " 'cnn/stories/00ec130aa4857442169aa027c5b38aadbebfd035.story',\n",
       " 'cnn/stories/00eca9c1c3adbd297f9582003396db45b41ac710.story',\n",
       " 'cnn/stories/00ed935ebf9d55f2f888be9c5a3bfa7d22a33e59.story',\n",
       " 'cnn/stories/00ee21a748990441c0e8691f0cc9b54b98ece497.story',\n",
       " 'cnn/stories/00ef8d03b1e4caa590ac68f43d69cae5dc5733bb.story',\n",
       " 'cnn/stories/00efdec5a2182499a8453a1c64ea5351a5550104.story',\n",
       " 'cnn/stories/00f222d27b211c4d83e77f297ccfebe6a652f900.story',\n",
       " 'cnn/stories/00f23c2ac2d8a4d208be808f6b5ed5077d5a274d.story',\n",
       " 'cnn/stories/00f4d5146e4ddea6f7879dc2f5ffa576a7eaf221.story',\n",
       " 'cnn/stories/00f4f2e971a3464255f317e5edaab0a0a3bfa9db.story',\n",
       " 'cnn/stories/00f57381e1ee3d67cb1498181c2100932d56af16.story',\n",
       " 'cnn/stories/00f5dfb4cafc85b50b652132f460e16b8ba61642.story',\n",
       " 'cnn/stories/00f5ff4f7779f01de1bc6a2ac785895cb5f602fd.story',\n",
       " 'cnn/stories/00f6432042db57bdc5c69479361e401aebb1bc61.story',\n",
       " 'cnn/stories/00f74207e7a8531be17f38b85eadf4a875467072.story',\n",
       " 'cnn/stories/00f8026e718cca411412d02e5bd940c6c4f00bb8.story',\n",
       " 'cnn/stories/00f83e0ba8db01ee2280938997a7c3d50ee29fed.story',\n",
       " 'cnn/stories/00f88099cf4ede416782f845635a3578081e2fa5.story',\n",
       " 'cnn/stories/00f893c908ed9c4d27fc9342b2d1e3fd7ad52f54.story',\n",
       " 'cnn/stories/00f969abbc42a459375391e05139630732905e07.story',\n",
       " 'cnn/stories/00f99f82c18f0de487dc73390b8cf71b0c8347c0.story',\n",
       " 'cnn/stories/00fa48a209242751af7814a89fd893895387a81d.story',\n",
       " 'cnn/stories/00fac17a8812644788bbf28cbeaf81cb783e952d.story',\n",
       " 'cnn/stories/00fb691a26a54c060cff232a7616723823e95245.story',\n",
       " 'cnn/stories/00fba75d30a50fb449de6df10e2dea1c3a270a5d.story',\n",
       " 'cnn/stories/00fc2d66206c7c849601806d0c5c4861a12f354b.story',\n",
       " 'cnn/stories/00fc56b8beb41ffede1b319b184b751bb2873ce8.story',\n",
       " 'cnn/stories/00fc7a9bfa433b0dbfb87e77e33635a054140b8e.story',\n",
       " 'cnn/stories/00fcb94b937a62afc6d4b8efac3828dbc1c78549.story',\n",
       " 'cnn/stories/00fe9bc7ef1fa3e5d4dd7f09871e7165ace17aaf.story',\n",
       " 'cnn/stories/010178277d04a94875853b0db1e58cc7846f7f70.story',\n",
       " 'cnn/stories/01019ef4116d76fe94945890cf777f206a072a85.story',\n",
       " 'cnn/stories/0101b5249ff9f601639a70aea1138af9d16ccd7c.story',\n",
       " 'cnn/stories/0101e62ee60bde1ca2ba34413f680bd1f71e990d.story',\n",
       " 'cnn/stories/0101e95643d00082c667dc1e393cdc9775e3f51e.story',\n",
       " 'cnn/stories/01034b1a55245a26c275a6ad409bd4222902d57a.story',\n",
       " 'cnn/stories/01039c671208a4d5f05fd5eeff97d8b9a178cb48.story',\n",
       " 'cnn/stories/0103efaf0741c10bef99df6d7a9d26a4fe6fbe38.story',\n",
       " 'cnn/stories/010446098e67c9f93a5916646df3c3fe76c6da79.story',\n",
       " 'cnn/stories/010498738c0d529311486c5fc25eb938c5e29d02.story',\n",
       " 'cnn/stories/010551dc8450fd90aee7b9b10541e230e493156a.story',\n",
       " 'cnn/stories/0105f4555a30d5840f76358b352d53023293b3b8.story',\n",
       " 'cnn/stories/01076dbd82d585bc868a3d0eb08a1f280d73f088.story',\n",
       " 'cnn/stories/01078a3dcdde35c465c741ac95f10323c6068ff6.story',\n",
       " 'cnn/stories/0108788e2e0d0da342bc9f9a6dccadb16b8ad56b.story',\n",
       " 'cnn/stories/01092242ce7fd97dc2acde70ce9fe8b708f67e05.story',\n",
       " 'cnn/stories/01098a6115982c00b6295c9c3cd26b7f51a3498a.story',\n",
       " 'cnn/stories/010a42b81da3f3fab9c82900200127dc748d38ea.story',\n",
       " 'cnn/stories/010b20cf1697b9809a7fe848871a51be02070cfc.story',\n",
       " 'cnn/stories/010c1ef212f56f27b86fd0d5d1b40d98748396b0.story',\n",
       " 'cnn/stories/010c531483a9846097912fdfd802c82546731174.story',\n",
       " 'cnn/stories/010cb4b1c6f32c2a909cd6032edb8cb23df18d5f.story',\n",
       " 'cnn/stories/010cd75a5b587285f7697cdb6db6526bcc0320b2.story',\n",
       " 'cnn/stories/010d464b7a2a3d0beb1c064a1a1a0c3c583aece1.story',\n",
       " 'cnn/stories/010e9046b8bf3d817e91bbe817bed854b630b1f8.story',\n",
       " 'cnn/stories/010f2e5073e692f924a0fccdf17ae079794996f3.story',\n",
       " 'cnn/stories/010f702d290fad77dfc35f4e188c2dfcc6f9b87c.story',\n",
       " 'cnn/stories/010f73477cd20c14cab78ad9cef350ac8c0f55b3.story',\n",
       " 'cnn/stories/0110884d2a279c6249d21c1480548ac9bf985cfb.story',\n",
       " 'cnn/stories/0110d245c9f9da9fc17de11bd32c433f9314533c.story',\n",
       " 'cnn/stories/0110d5f739e969d2ad83175786078e9a2b795970.story',\n",
       " 'cnn/stories/0110e6a6724d51ee21c64e138ef7a41027a2cd39.story',\n",
       " 'cnn/stories/011135a42297e5212206d0209b8aba31859787a6.story',\n",
       " 'cnn/stories/01132c28c2bc8a8b7246b8784f5e65d0f9489da2.story',\n",
       " 'cnn/stories/0113354e89ebda83b483c1af954ba86498174bee.story',\n",
       " 'cnn/stories/011404d8724d92181b63250cea9b6e801041efbd.story',\n",
       " 'cnn/stories/01166936e87455f7b7e66a4f8c14f60043287b15.story',\n",
       " 'cnn/stories/01170d9c6028d852e4240b27f99f46ae5336414a.story',\n",
       " 'cnn/stories/01176065ec41c9c63280f36a4d0b3b27e9143570.story',\n",
       " 'cnn/stories/011828b5f67bdd49df4bc786f8070c4d341a656c.story',\n",
       " 'cnn/stories/011846d425fe51206040208bb9ad2282e9e6b960.story',\n",
       " 'cnn/stories/011987a371b5f29de9582ca8371152699bf51bd9.story',\n",
       " 'cnn/stories/01198938399e94c1d4810e962b60a6014d28ab43.story',\n",
       " 'cnn/stories/0119b8517cfe7ae6fdc40631c9cbaea617453bce.story',\n",
       " 'cnn/stories/011a75eaed3c114857f6da9d56900c2879cf5b5d.story',\n",
       " 'cnn/stories/011bbaa3b1b61a2ce1912fd62a31f516dd8316e8.story',\n",
       " 'cnn/stories/011c6a42c94c5949a234d1a35d88d904ee9db463.story',\n",
       " 'cnn/stories/011ce55ece7d96895c09fec5b3aa3352a67e21df.story',\n",
       " 'cnn/stories/011cfe059bfddd7dc3be107e5ec4502bc5b09abd.story',\n",
       " 'cnn/stories/011f4daee8769ab22e1b70b0a02bbc0f06239cf3.story',\n",
       " 'cnn/stories/01209d29c982c9ced091bcee3768b285ecfa8af2.story',\n",
       " 'cnn/stories/01227f8316c543eebc3058884ad14aab7039c479.story',\n",
       " 'cnn/stories/01241b679cd9cef111e735550f3ae1d209a6fd6b.story',\n",
       " 'cnn/stories/01244c2c92f5f1a6deb8c1d92f20df22f00c329f.story',\n",
       " 'cnn/stories/012493ecb0990390005b751fe76984e6ea41ea86.story',\n",
       " 'cnn/stories/0124b043ee09f3029a1ffab20e53cbc7da42da49.story',\n",
       " 'cnn/stories/0125419b924417a3c298e0e4a2f665f5271901f5.story',\n",
       " 'cnn/stories/01266530a77b160357d6a3d8e86d04fc80d41b13.story',\n",
       " 'cnn/stories/0126a31d2fd50c5bccbd00670f0d3a46b8136a64.story',\n",
       " 'cnn/stories/0126b202b8333bc2c56edffebb05fcd4234377ea.story',\n",
       " 'cnn/stories/012819ffa2547138101055add33deebe7beaa3d4.story',\n",
       " 'cnn/stories/012824cd1c5ad71f3f985ab6091124970c08e113.story',\n",
       " 'cnn/stories/01291466b0e4ce2a8da55f209ebe50b84a4d6ad2.story',\n",
       " 'cnn/stories/0129aa6c6f670ce0728c24ef8d4bea7a3b4dfec4.story',\n",
       " 'cnn/stories/012c5bfe1ef1cf80d159dab760ff32fcbb270b70.story',\n",
       " 'cnn/stories/012deed72132ed0ae83bfc2f77a5f3714d56373b.story',\n",
       " 'cnn/stories/012e66126878710de4704bd506931d702582afcc.story',\n",
       " 'cnn/stories/012eeff83a404a0dbcd7e854e67564277b1c53fa.story',\n",
       " 'cnn/stories/012f08802d718608a9ef468e750bb844da8650ba.story',\n",
       " 'cnn/stories/012f3ed0f56446afffc140e327952ed6b9faeaab.story',\n",
       " 'cnn/stories/012fcc3657d881fec5df0bb533815485f76dd3cc.story',\n",
       " 'cnn/stories/013014b6116265bc6fb30847cf9162d89a63dba2.story',\n",
       " 'cnn/stories/01304eb05cf60c357fb194fdd91814e4ca38e17a.story',\n",
       " 'cnn/stories/01307bfd9eda40466465b19b4cabe4dc9e936abf.story',\n",
       " 'cnn/stories/0130f10c1d700cf42cad5fd24b242667342c86be.story',\n",
       " 'cnn/stories/0131c19f4124202c8b2603029b5bc642d996c851.story',\n",
       " 'cnn/stories/01320e7d24406f4c858dbb205e3582681f621c31.story',\n",
       " 'cnn/stories/013293cfb3d50b6f688f3c7faf298c9e83e821d8.story',\n",
       " 'cnn/stories/0132a4a7b0ae6842a36af6eff1dfdaf8f2beb3a0.story',\n",
       " 'cnn/stories/013385d8cd2d0848ac70c922a93eec7c2d77818a.story',\n",
       " 'cnn/stories/0134b852198cd1f0463447397944dd90ad913b14.story',\n",
       " 'cnn/stories/0134e69ba90fea45f72677e65a1002a7dd943bbd.story',\n",
       " 'cnn/stories/013545dc3523ed81e5a0d8c4a98a39e464c6951c.story',\n",
       " 'cnn/stories/0135c365464e93a6c91d37bd82e8df6e264daa17.story',\n",
       " 'cnn/stories/01367c41d5478cc8b93c5f2fd87124bb48d5b9ee.story',\n",
       " 'cnn/stories/0136eebb656b1d04c18eade5a694844d069c97e7.story',\n",
       " 'cnn/stories/0137a5b22cf54f6ee7f55dd29d1a8a8bfc76d38a.story',\n",
       " 'cnn/stories/0137bc0e904ee06dc926704ac3c919bec247ef3a.story',\n",
       " 'cnn/stories/0138881de84d4cb5d45169f6b3b093fdf3adb20c.story',\n",
       " 'cnn/stories/0138a55a7d243b692fb98da23b8cdcc6be232d2f.story',\n",
       " 'cnn/stories/0138f062afbd50afb16d1c9af28c306c156a6845.story',\n",
       " 'cnn/stories/0139524211de51f23bed66156da66d61abeef0b1.story',\n",
       " 'cnn/stories/013b4d8104c422222344bc8263022820e6524ad1.story',\n",
       " 'cnn/stories/013c80b65c2b177ab3d6508dd951e71e2618b72a.story',\n",
       " 'cnn/stories/013d30619cf213637ab69ce7b77df5e47af380cc.story',\n",
       " 'cnn/stories/013d48fd79a5120474349721b049f071a10c0a4c.story',\n",
       " 'cnn/stories/013dd73813f6cecf1c78a1da5b7095047aaad2db.story',\n",
       " 'cnn/stories/013ece13e21dc228a58609738b5b90179f2cf046.story',\n",
       " 'cnn/stories/013f6c2e1ebe8e511ac07d1096fec5b096aef439.story',\n",
       " 'cnn/stories/014136b976abf7cab02d2441611ec915960ca466.story',\n",
       " 'cnn/stories/0143de4500c8bf36c397425d3c7164580febf8dc.story',\n",
       " 'cnn/stories/01442297824e24a4bc69af5079593f17f1f491ba.story',\n",
       " 'cnn/stories/014552e5a65ae0a97cde284d0b59012dafd5e13b.story',\n",
       " 'cnn/stories/01462daeb1d2b20447926ffd47368bdf0116db4a.story',\n",
       " 'cnn/stories/01481086a0345380b54bc7e2ba6acb72d61b3a67.story',\n",
       " 'cnn/stories/0148b68b66d86dde6d48abafba56ef5de917c67c.story',\n",
       " 'cnn/stories/014a660dc5a7874a56d27d3a6283379072538a07.story',\n",
       " 'cnn/stories/014b6d9853523dd97e7a009c3e5428eb6033c0cc.story',\n",
       " 'cnn/stories/014b85e17edc4916b2c42376e83fddabb6449538.story',\n",
       " 'cnn/stories/014c08b37cc7977e1407d363550d6de612eeb265.story',\n",
       " 'cnn/stories/014c40bffc4990556c2ca34665cda2c270da49d7.story',\n",
       " 'cnn/stories/014c954467cc351b6320f03e06d954e2b3f0bf54.story',\n",
       " 'cnn/stories/014cfb5391b7eeec17b3254dfa61ac5bf637fcdd.story',\n",
       " 'cnn/stories/014d4197e01acdf596a573a792f23dc1a6165a10.story',\n",
       " 'cnn/stories/014d4983a767e15b18555d21e236aa53f7c23410.story',\n",
       " 'cnn/stories/014f827745fc2c49424e2c1c0bf5bb8ca78892f4.story',\n",
       " 'cnn/stories/014ff0181e2db96c5ac5be23e652677c1297dcb8.story',\n",
       " 'cnn/stories/015007887b47cb2267f7082c2ef90b58fe96589f.story',\n",
       " 'cnn/stories/0150b84a06d2bd0f1614ea80bcf356ed8aac46dd.story',\n",
       " 'cnn/stories/0150c9bf4797679c3c73d60685c4e0f5799baaf4.story',\n",
       " 'cnn/stories/0152840f052afbe3e42c82082aaaf7580b0cd621.story',\n",
       " 'cnn/stories/015438649d489b4ec6f1411d06e561a000a18c39.story',\n",
       " 'cnn/stories/01543ed99c947ad051ae6fd72b9a28aa51882f6e.story',\n",
       " 'cnn/stories/01545b1602bda0c6b45e6b8a419d6831ec960764.story',\n",
       " 'cnn/stories/015463975f4653362fbd9386335907e17246abee.story',\n",
       " 'cnn/stories/0154aaedee1bc417efaf6b1cfa3b5471df1fe7c3.story',\n",
       " 'cnn/stories/0154b1e53b4de57c9c53bae550ee2dd2742c78d0.story',\n",
       " 'cnn/stories/0154e048824faeb8955360943adeb9ef10ec10ec.story',\n",
       " 'cnn/stories/01554ebb431ce8ba20ff4ded5e6db196990364a0.story',\n",
       " 'cnn/stories/01561a338967af4c691f179d46d4fbbba997af99.story',\n",
       " 'cnn/stories/015676ab8ff8282a6acd422f73b6f098260e17f2.story',\n",
       " 'cnn/stories/0159e6fc2c528a7d8137ade6299cd35400043be6.story',\n",
       " 'cnn/stories/015a8c6844420202dafaf8aa88e51f83c2747ab0.story',\n",
       " 'cnn/stories/015ab35f3c2c108f8625df2cce84fa0d631f5461.story',\n",
       " 'cnn/stories/015afc4660fd3b64909815d2dafaba679e834675.story',\n",
       " 'cnn/stories/015b45fdd8a4a50afacedfb6061fa9171ad56252.story',\n",
       " 'cnn/stories/015c43b3b18e5db4d824a9aea30182be3091fbd1.story',\n",
       " 'cnn/stories/015c560294bfc03f1b262e3a031d66817087fb8f.story',\n",
       " 'cnn/stories/015ce688d7722d4ba099e3bfb7a4e2c86c6d2b15.story',\n",
       " 'cnn/stories/015d09aeb199f5982f94ee0fda44c01d12258afb.story',\n",
       " 'cnn/stories/015dbd8619fc328a1a43148ffa72f615884c59cf.story',\n",
       " 'cnn/stories/015f5fb5594e66082715ae281ef8d22066df57f7.story',\n",
       " 'cnn/stories/0160b5e1ba9a05ec771d2b24a2cf3f77e44744c5.story',\n",
       " 'cnn/stories/01610c5d388d8682cf26afb121e473cbd68cc7f8.story',\n",
       " 'cnn/stories/01610cd282802c4defea43ff4839015c15f026d7.story',\n",
       " 'cnn/stories/016163428c6e9f0eef6a08d243e6f0b286f8796e.story',\n",
       " 'cnn/stories/016229d10bc9bbdc24e196505882043a333a9405.story',\n",
       " 'cnn/stories/01630a9f439c39c2e49f898fef3fc33fa9947774.story',\n",
       " 'cnn/stories/0165014e696a45a24a4fced48466b57faa176a3d.story',\n",
       " 'cnn/stories/0166279bbebb21968834db995e218de90a3478a5.story',\n",
       " 'cnn/stories/01672a244484a9089e3955d2876559260a663c74.story',\n",
       " 'cnn/stories/0167c0035c07e839ab63d9d19a80549824943719.story',\n",
       " 'cnn/stories/01682ab7fab98c2aed3d8c2c3ad65f926c9d369e.story',\n",
       " 'cnn/stories/01690abb8c28d539830c5565ce21958f9295fbbb.story',\n",
       " 'cnn/stories/016b990aa67b3edfed8be8f036222fb9856eaf6c.story',\n",
       " 'cnn/stories/016d31535a8682470d18ddf8572114c89309dc90.story',\n",
       " 'cnn/stories/016d4e3406a41ec5d5fd499657763c5b2cb3fcab.story',\n",
       " 'cnn/stories/016e1bbc996b35af1cb2358a05e8814305911012.story',\n",
       " 'cnn/stories/016f170646aa50abbf56df655cb4ae03b52a651e.story',\n",
       " 'cnn/stories/0170f15f7bda5e1972ad86d4923c81a0851fe7c3.story',\n",
       " 'cnn/stories/01721ddcd0cb09df4890ca25bbc3692ef640ca7c.story',\n",
       " 'cnn/stories/0173a6b77e0c0362ad64a7257a05bdb9cf969bac.story',\n",
       " 'cnn/stories/0173e54b0647318f9de4f52422fdf92f5b7dfa1d.story',\n",
       " 'cnn/stories/01748b89aacbbd4ba0710b36c34389789aabb77e.story',\n",
       " 'cnn/stories/0175a46d5ca3ba0b3e17aadfbd198f3fbe90801e.story',\n",
       " 'cnn/stories/01770f26afded78b3ad54f2703ac378e0d5b60aa.story',\n",
       " 'cnn/stories/0177e011302ec1925adb4cd77a026577fd8206dd.story',\n",
       " 'cnn/stories/0179773e5a0acce1c05218d797138f9f03af46c7.story',\n",
       " 'cnn/stories/0179bec990a00bb489f847fa62f9fd4be3fa05d0.story',\n",
       " 'cnn/stories/017c35cb30a2a648994225c5c4d1f7014e9c1682.story',\n",
       " 'cnn/stories/017c7c4ac8bc2544c8616d741f19edacb033af2d.story',\n",
       " 'cnn/stories/017d27d00eb43678c15cb4a8dd4723a035323219.story',\n",
       " 'cnn/stories/017d63ab3cdd37ded241a3c355e9bb24fc4ba4e2.story',\n",
       " 'cnn/stories/017df5c4fe1e79eb26957ff6a8b4c1e41cd966ac.story',\n",
       " 'cnn/stories/017dff6a538bc59f61682eca12104524a26a31ee.story',\n",
       " 'cnn/stories/017e28482bc531ed2ba2eebe7327e8b54bbbe91e.story',\n",
       " 'cnn/stories/017e445ca3b43c00d7252a55bc39db7fcb5fcba2.story',\n",
       " 'cnn/stories/017f9c53df71949ad526c6b1639d8c8c6677622b.story',\n",
       " 'cnn/stories/017ff719d49d1a22576f86a6e9a2d31d45e51e28.story',\n",
       " 'cnn/stories/018053328ef15fe469f6b1a3f8491b06e7045cdd.story',\n",
       " 'cnn/stories/0181125559f8871916bf04c5f7f15dc3e42e5da4.story',\n",
       " 'cnn/stories/0182d7d55cf31fcd4e358f6466036779baff0108.story',\n",
       " 'cnn/stories/0182dab1e2c73e5f6c6dc16441f76752cb8f9fa3.story',\n",
       " 'cnn/stories/018439cc4a72c711bef718b3820c2ac89e1b3c53.story',\n",
       " 'cnn/stories/01847f247e49516a04d14e5aa7e1359485417bae.story',\n",
       " 'cnn/stories/0184ba43125db06807a3e64b64d5dbff83277665.story',\n",
       " 'cnn/stories/018501c9c6ed4c802553d3cf2c0a4aebf5040f96.story',\n",
       " 'cnn/stories/018544e74130a4d6da4f1a76583ee1a846e94fd3.story',\n",
       " 'cnn/stories/01857dc92760b74c9ec601848dc79fb3e49a52d1.story',\n",
       " 'cnn/stories/0185ee15bf61b29397e201a0be77faeb184fb230.story',\n",
       " 'cnn/stories/0185f071f72ba052f1aee17764cfec1a9c23f7fc.story',\n",
       " 'cnn/stories/0186672dc20879426a9e5b75923550b2d20283b0.story',\n",
       " 'cnn/stories/0186a4e9ef5cd3d4b50d6592b85aaf3d769530dd.story',\n",
       " 'cnn/stories/0186e9410d3bd12b8a41b61e8a59833924fed881.story',\n",
       " 'cnn/stories/018777aa95cba1743c4287d32be913a1a4b903db.story',\n",
       " 'cnn/stories/0188124e3cabfa6ac3abff173ffa58f90cc84191.story',\n",
       " 'cnn/stories/01882d6515873f5a991817406f04fa0583542f9b.story',\n",
       " 'cnn/stories/01886c549638375d76d0079488b1e5e6fba4d9eb.story',\n",
       " 'cnn/stories/01898581a1e7b543016027b303aaf0c8d0a2cb27.story',\n",
       " 'cnn/stories/0189c089ec3a8a3f055bb735986e7fb1902a05e7.story',\n",
       " 'cnn/stories/018a411afcc1986f4ea2c962015e6d6f0128b601.story',\n",
       " 'cnn/stories/018a8afb1be1b4ad751af352231a6b11e5af8eb9.story',\n",
       " 'cnn/stories/018b1c599d56aef2d715f197893d8e127b7ca187.story',\n",
       " 'cnn/stories/018b3cfd79987414646870a077e5116e82fe088c.story',\n",
       " 'cnn/stories/018db7f5995ef1e3c7a65f617f56ba509dc69224.story',\n",
       " 'cnn/stories/018e3016e48a4aaf442a2db16d4b1143d3b662ea.story',\n",
       " 'cnn/stories/018fac8aed5438c95d5f29e07d721b0f93dd033a.story',\n",
       " 'cnn/stories/0190007c4b6f095d97e5d734d077a089d77b381b.story',\n",
       " 'cnn/stories/0190395505bcaaf318c75e343206c631ec40e693.story',\n",
       " 'cnn/stories/019151a52351c7ef0e403cc7a276d53732afa9e5.story',\n",
       " 'cnn/stories/01919f4ca624c45500aca92c66aa6a197a8b80f5.story',\n",
       " 'cnn/stories/0191cfb8f79cf85b49d46dffd7033197ca0cdc15.story',\n",
       " 'cnn/stories/019224d69e5d1ecd533f87daeef60c17c021c84e.story',\n",
       " 'cnn/stories/01925fbb86a1995c9682fdcc7be27a1823c771da.story',\n",
       " 'cnn/stories/01934bdb5bc1651223a635ae68576e426559f554.story',\n",
       " 'cnn/stories/0195c98e735883f8e2679a695b3609da92065fc3.story',\n",
       " 'cnn/stories/0195ccf38de0f2ad9ced4c55c57d9638bd59b776.story',\n",
       " 'cnn/stories/0195f67b88e13308b0c462cd003b3824610b7dfe.story',\n",
       " 'cnn/stories/019a1b69224590a0e0f63e6546ac1b12e42f8767.story',\n",
       " 'cnn/stories/019a761cedf604712ca1c6390fe85d4f59b8a4e7.story',\n",
       " 'cnn/stories/019b4c554f50daea7968ebda30e86adada9499a9.story',\n",
       " 'cnn/stories/019bb86cd16c831beaa37fbbe0bcf484b3fee676.story',\n",
       " 'cnn/stories/019c31b4dc7c103ed42274a2f6d5efb2bafd406e.story',\n",
       " 'cnn/stories/019d12eb752cf8cfd4f974a487a6fede224c5b64.story',\n",
       " 'cnn/stories/019d874e2b508993052b2c29501c188799584c06.story',\n",
       " 'cnn/stories/019e95e15209076bbfd4315b86383883d9f0899d.story',\n",
       " 'cnn/stories/019ff937428c593d20c81243fecf298cffb2e752.story',\n",
       " 'cnn/stories/01a033acb35063f347289196cb89e8afaef802c2.story',\n",
       " 'cnn/stories/01a124ac4cf204df6c9440625765019670ae9812.story',\n",
       " 'cnn/stories/01a14827bf6b0b99dea932ae65658adafda54c26.story',\n",
       " 'cnn/stories/01a17b2ede1c89dd0619310554597c9c6e71a075.story',\n",
       " 'cnn/stories/01a2c12911cde0f04e0af8798ca7ff854c35a830.story',\n",
       " 'cnn/stories/01a33a152b10d57a6481869e652dc833a732dccd.story',\n",
       " 'cnn/stories/01a3ecf33b89551bd5513fcec634704b92f87904.story',\n",
       " 'cnn/stories/01a4c31112f1ceeeebbe8dce862f14f6792c03a3.story',\n",
       " 'cnn/stories/01a4c9771d9a487c68d969f81528093a8d06aa9c.story',\n",
       " 'cnn/stories/01a4cae29a805dba3437fb2b60900c263199922e.story',\n",
       " 'cnn/stories/01a543e0323a0d0f8458c512516a73372416ee2d.story',\n",
       " 'cnn/stories/01a5f7009feb69011e711a862b10e97475d13e9f.story',\n",
       " 'cnn/stories/01a62b88f1f63f24e2754b49de40526f1406b74c.story',\n",
       " 'cnn/stories/01a855c4672e9e1b68af69a571af14c2507d9d51.story',\n",
       " 'cnn/stories/01a9836f1743877135e049fbe705f7618c3953b7.story',\n",
       " 'cnn/stories/01aa21edf542ee1c562a10502e8be34f495ef4a0.story',\n",
       " 'cnn/stories/01ac645a3a3703a87569509f79ffc37d48a89043.story',\n",
       " 'cnn/stories/01acc87b5f1292aecec64030ae571591c154f073.story',\n",
       " 'cnn/stories/01ace4f8c335b9e2fb528d34bd26fedd85130bea.story',\n",
       " 'cnn/stories/01ad23112cc2166f40d45e10bebb33c631d7ed4f.story',\n",
       " 'cnn/stories/01ae1cae6c8925bc1258efef1f2120248bed75ba.story',\n",
       " 'cnn/stories/01aef964c6596282d390614ba0a44c637e9c420d.story',\n",
       " 'cnn/stories/01b212105d7c72cdb59c8ab3077f742bf3429e38.story',\n",
       " 'cnn/stories/01b3e560862f4c6591302e34c33b33ca92848513.story',\n",
       " 'cnn/stories/01b46637c0b63bc0f86406e57f080906d49dc39c.story',\n",
       " 'cnn/stories/01b4e36d8d4176328012d9006858236b2609602c.story',\n",
       " 'cnn/stories/01b587c1fcb465f6ae39d755aa472366326318c1.story',\n",
       " 'cnn/stories/01b5a7406ad32da8192e736bb4baf49b1c6804b5.story',\n",
       " 'cnn/stories/01b6139a9ed19d594a51210d3ac5a3052606b7ce.story',\n",
       " 'cnn/stories/01b747e2c5eda2982ac07dc097ad9397c619ad5c.story',\n",
       " 'cnn/stories/01b7ed864e966ca34db640a774a8288988f84ba3.story',\n",
       " 'cnn/stories/01b8b9f6fd67fa732b759bda829daf922b28e345.story',\n",
       " 'cnn/stories/01b8eef33c9b3cb95867d93124a54dd55f80aa45.story',\n",
       " 'cnn/stories/01b974660574064bd7caf8b0144798a63b8ad278.story',\n",
       " 'cnn/stories/01b9ae11838c79316b80a5bf581289450b01ea4d.story',\n",
       " 'cnn/stories/01ba12e566ce2a2fa5315b6a4e8ea9a8eb7aaeec.story',\n",
       " 'cnn/stories/01ba283764b099f18f490c553c3db054cea55391.story',\n",
       " 'cnn/stories/01ba8ba2c6002482b6a25b6e8fe1dbce8cbdecd6.story',\n",
       " 'cnn/stories/01bafadfdbe1b9c77619564e49788166f33ed717.story',\n",
       " 'cnn/stories/01bc8025c94c57a156469fbad202394a2ad9e37d.story',\n",
       " 'cnn/stories/01bd007aedd9c62cc62e20ae43367bf9bbaa199a.story',\n",
       " 'cnn/stories/01be76154d32254c4afe3a37f00648b8412873a9.story',\n",
       " 'cnn/stories/01bf74602a899672112ab97d24e629eaf104c6cf.story',\n",
       " 'cnn/stories/01bf98ba1fecd088f7b3f2ebff5e288f8eba1880.story',\n",
       " 'cnn/stories/01c01fc113650a9368f59d157ec7566df3585e23.story',\n",
       " 'cnn/stories/01c0a9d2ec6df2fe80dcc9ebd0138f702891539a.story',\n",
       " 'cnn/stories/01c219f44ac796ccb8d0bb802dfc9be201c95540.story',\n",
       " 'cnn/stories/01c23ad98986cf0d9dda17ba9d035bdbbf6aaac6.story',\n",
       " 'cnn/stories/01c49c859556151a7ef19fc39286e16d6ea57a90.story',\n",
       " 'cnn/stories/01c708128bccff2bbe6bf262a736d4a41963011a.story',\n",
       " 'cnn/stories/01c754ef046b965a1ab6bc156a461d903da2d4be.story',\n",
       " 'cnn/stories/01c78228c33a92b7a24e43642432db158e6924e8.story',\n",
       " 'cnn/stories/01c8128a3cb2d580d6a49f1b7d84b6f764a4ab43.story',\n",
       " 'cnn/stories/01c8326bd8d9cac2a794d3363fe5a933285cfd59.story',\n",
       " 'cnn/stories/01c859022eaefb83681929ea3fa699e2a7c06620.story',\n",
       " 'cnn/stories/01c953ec5662079f85a09664d478ae93578aa769.story',\n",
       " 'cnn/stories/01ca9d3e9fac3827776aaacaba25fe05d2a212ff.story',\n",
       " 'cnn/stories/01ccf4023d0a73b32dd3b21c2c054c29d7b2e393.story',\n",
       " 'cnn/stories/01cd6515607eee6ef4f01964a56960ab1b34b290.story',\n",
       " 'cnn/stories/01cd7f4631c6b8223e556224a9092d08b68c69a2.story',\n",
       " 'cnn/stories/01cdb93a75ea0da86ea2905543532d324d717a5e.story',\n",
       " 'cnn/stories/01cde715b94f7b966dc2f2ce5983e4302d277496.story',\n",
       " 'cnn/stories/01cfe9b24b55f199112656ba703c1d62acde5fd7.story',\n",
       " 'cnn/stories/01d05a067fcbe85e443c9570cb789ffc1520281b.story',\n",
       " 'cnn/stories/01d2590cdcb834112dd0763644ecdc76fa657e75.story',\n",
       " 'cnn/stories/01d2795c7cc958a302df0fffe8c426509d91ef17.story',\n",
       " 'cnn/stories/01d2bc258d5e13d1742cc75bbfa8b224c9760cd0.story',\n",
       " 'cnn/stories/01d384c5f5ebd0deef4398e17b6006fb5461e57b.story',\n",
       " 'cnn/stories/01d39bcbdbcccdaea7859ab2a13c7a2ef73b5507.story',\n",
       " 'cnn/stories/01d3ad38364ab897d92a37e950ac7ae3b5c3a8c4.story',\n",
       " 'cnn/stories/01d3f5950dd99b3069afd4e626f345e70f76a94f.story',\n",
       " 'cnn/stories/01d5230ca0de4e2faf09af2da91c45bbd675a0bb.story',\n",
       " 'cnn/stories/01d5b7cf16d87db5a2bed267d83aac584f58f325.story',\n",
       " 'cnn/stories/01d617742d268464ed04ab83ce0f498fcaa07420.story',\n",
       " 'cnn/stories/01d67e9460b09b19ab3d075ec8d20540c623a3a7.story',\n",
       " 'cnn/stories/01d7341b501dc858faad7609cd77772d3db82047.story',\n",
       " 'cnn/stories/01d791b7e3985359b05ec6fb178abb452dc1ac0f.story',\n",
       " 'cnn/stories/01d8082755cb24f3640666baf2068d0c98abdea9.story',\n",
       " 'cnn/stories/01d8971f7cf43d669e7ad2d21ee9ffb29f0f9cfe.story',\n",
       " 'cnn/stories/01d8aad806aea2796f4933d72dbf241afc5c5ea5.story',\n",
       " 'cnn/stories/01d924796f66c46029f6e6d3f5c265a1679d4424.story',\n",
       " 'cnn/stories/01db30d12f76c5d44b5dd6884c970b83dd7857f5.story',\n",
       " 'cnn/stories/01db4a175750fe003b18e49edf897978868cb562.story',\n",
       " 'cnn/stories/01dc8ca0612ec32cff2b380ffe23b1bb539b345a.story',\n",
       " 'cnn/stories/01dd572a8092a7343027b743a1e9188c51e2262d.story',\n",
       " 'cnn/stories/01de2184592b3f2f1ccda43f2ed779a1a22d3618.story',\n",
       " 'cnn/stories/01e01575704d6134f94eb26bc50b46ea8cca3292.story',\n",
       " 'cnn/stories/01e027a594dc4246f98357cfe0eeaf93a17f1932.story',\n",
       " 'cnn/stories/01e1ef273cf2aade9bcd6586b73e86fb56712280.story',\n",
       " 'cnn/stories/01e20c4ac8ca7820b7945e8b83bcf81d1291a30e.story',\n",
       " 'cnn/stories/01e32e5217646294d8d1a66eabf50faeb4485bad.story',\n",
       " 'cnn/stories/01e3a3436173d24f3c43f3b5f1be20d1fd565cc6.story',\n",
       " 'cnn/stories/01e3e6527e6e250c7c5eb7e5b8cb07a9aa7d9811.story',\n",
       " 'cnn/stories/01e41e83605194c2a4edd8e2ebc5a4441a2b3cc1.story',\n",
       " 'cnn/stories/01e53f9b7c308f4ff0c153156e93a99c9c8cd45f.story',\n",
       " 'cnn/stories/01e5409cd4a270b39fb54149634f5b7c20f9eaff.story',\n",
       " 'cnn/stories/01e58ba7f5a9e6dc9842c7a8dc2d21381ba2eaca.story',\n",
       " 'cnn/stories/01e6271f6d223360da0da3eb5bd209af759f498a.story',\n",
       " 'cnn/stories/01e911ac71b3f00555e2d2ff213521b7fa4d4e08.story',\n",
       " 'cnn/stories/01e98c90c19e364ac4961bdfe0e6b6b0b30331e4.story',\n",
       " 'cnn/stories/01e9f168bef2efb6ed4200a8eaf1c89df48042c5.story',\n",
       " 'cnn/stories/01eb746ddfa75a18650158e762406c0342a53ac5.story',\n",
       " 'cnn/stories/01eb761647ae724e47d946f21f8e136dafb97641.story',\n",
       " 'cnn/stories/01ebb1bcd5f11a6c36046ff82c954f96fc5e104c.story',\n",
       " 'cnn/stories/01ef8de26071b7a6ca02da9cd1513276febe5102.story',\n",
       " 'cnn/stories/01efc3d7fa9523c4223b0d72a17192c3830c6033.story',\n",
       " 'cnn/stories/01f00ad1fe458d0504f68fd4638222e9b42bb57c.story',\n",
       " 'cnn/stories/01f11c8be9f7e3cb58cea799c1cc87f4cc4a873e.story',\n",
       " 'cnn/stories/01f11f64cdcd71d1e52df1512664e7e14bb57b31.story',\n",
       " 'cnn/stories/01f131d19fb7ad353abafb80e9461e0c795bc0b4.story',\n",
       " 'cnn/stories/01f24ee8e55f0fae1db5082b7d3b0aa9e0391d06.story',\n",
       " 'cnn/stories/01f2a14b69e26349a6d05199703407de54d3a2a1.story',\n",
       " 'cnn/stories/01f37d1af9637b26019ed866e7f52bd5903b65a2.story',\n",
       " 'cnn/stories/01f41354ffb70e433681ad5fe4b6d69c48e2884b.story',\n",
       " 'cnn/stories/01f47139dd56a1c146020308148123155eda43d2.story',\n",
       " 'cnn/stories/01f4e9b3b92fb786f086ed606b34d5dabcd0daf4.story',\n",
       " 'cnn/stories/01f719b5efce0c1a99b276a2d08fe6d8da9f9428.story',\n",
       " 'cnn/stories/01f75530d1e28971b14facd1c94dcdb58d9904f1.story',\n",
       " 'cnn/stories/01f869599d8efa6bd845de8f7b4c1d95f1ec340b.story',\n",
       " 'cnn/stories/01f9ae9904f07bfaa784563be60bc1e5cf8f1cb1.story',\n",
       " 'cnn/stories/01fa3a4f8a95d7b1a63f82cb3348d8d2467a4835.story',\n",
       " 'cnn/stories/01fb4659b531060a311ed2c8478f3dbc1db3c065.story',\n",
       " 'cnn/stories/01fbaf3399de8741e75ef5a1d6499b39f9f23552.story',\n",
       " 'cnn/stories/01fcd6ac0374a261b461ed56911e5e1bb3edd4ca.story',\n",
       " 'cnn/stories/01fe33de5ae201204ea0a6192bf075e8a548ed33.story',\n",
       " 'cnn/stories/01fea1ba2ef913cd9615aba30df76c77719ecf85.story',\n",
       " 'cnn/stories/01fec10f5d7e013d0807b94ab8e4a232d74a00d5.story',\n",
       " 'cnn/stories/01ffaf507a54d354ecbbe4a7658d60a2e553e1d8.story',\n",
       " 'cnn/stories/02019342943a34cb078cc80c4d7b45153327298f.story',\n",
       " 'cnn/stories/0201f7931b6088ccef0083efc4cea2449ab06f39.story',\n",
       " 'cnn/stories/020202f344a76196c3bd606c4afa0aadf3a11caa.story',\n",
       " 'cnn/stories/02026017768c50470773370d985cf38d2bc2c427.story',\n",
       " 'cnn/stories/02037af85149fc16f053ff9f41db55852424dd8b.story',\n",
       " 'cnn/stories/02037c9d409e08309f54303114df2147b375a673.story',\n",
       " 'cnn/stories/0203b8f730d58be22bf55ceda0bd485d0f15f7c7.story',\n",
       " 'cnn/stories/0204ba616c32c31cbe7e6b8afc32d5798dd6888b.story',\n",
       " 'cnn/stories/02050bd3fa1b0443f768e909537de80cc40ea446.story',\n",
       " 'cnn/stories/02057d87a579e9524c4fb976ac41d0573344f72d.story',\n",
       " 'cnn/stories/0205def85e7869c527081768d3df137bf9b87d87.story',\n",
       " 'cnn/stories/02060be908e4894aa26922fbc14a2aea1a4163f1.story',\n",
       " 'cnn/stories/0206b26c519affa1f48a3109be7668fed3b2451e.story',\n",
       " 'cnn/stories/0206f486c821d39f12fa4d3b62b1fe8cbaf3ded6.story',\n",
       " 'cnn/stories/02075859860a3d3c752e8793365db9dc7bd984a2.story',\n",
       " 'cnn/stories/0208196636a0096bad7b86390a9fcad0f5fff4cb.story',\n",
       " 'cnn/stories/02082b9906f64fcea9c092a402aa25b39c1217bf.story',\n",
       " 'cnn/stories/02084f7548eee88d9e933a50f6353156bda16b4f.story',\n",
       " 'cnn/stories/02089bab749fa49f9fd49a2d277d5cfc3e2b19f5.story',\n",
       " 'cnn/stories/0209036ab39087ad73000f292bffb7a3f9c86b58.story',\n",
       " 'cnn/stories/020a64f74fdcdbacfba8095b51619f67016d2e22.story',\n",
       " 'cnn/stories/020af9a92c53cd8fb2c444262addaf770c0d3bb9.story',\n",
       " 'cnn/stories/020f6e1ed9ce08dd96dbbbd889f6615c44c957ac.story',\n",
       " 'cnn/stories/021011c1af37f0f56e53d92767557bcdea86754a.story',\n",
       " 'cnn/stories/02103da8c7842ca50cf3aa883576bc5e0b8b39c5.story',\n",
       " 'cnn/stories/021076a996c2a507dcd4fdaa2267c004684f7e18.story',\n",
       " 'cnn/stories/02109dcf31b933134752f8287b0c2674535a092e.story',\n",
       " 'cnn/stories/0211340c9ec457bcac301fedd0b6547a423363a5.story',\n",
       " 'cnn/stories/02130226ebfa1251cd2f476f6d9c66d355f0827d.story',\n",
       " 'cnn/stories/021344a80744e1b2dc565a3b2d703404788ab046.story',\n",
       " 'cnn/stories/02153906eefb3102a7dfb6d43c0a8f6e4c6af426.story',\n",
       " 'cnn/stories/021626ac4ee280d851ad65f61eeb10eef449a9cb.story',\n",
       " 'cnn/stories/0216bcb30adb8345f09b778215c3aca77be5a741.story',\n",
       " 'cnn/stories/021737af5caf5536ade0f41b2d5a8ce5fd26b421.story',\n",
       " 'cnn/stories/021762c16e4e3b65a336bf4a59c4534705d2dbfb.story',\n",
       " 'cnn/stories/0217c5be794ff6666a57601cd83833ddc02c27ac.story',\n",
       " 'cnn/stories/0218fecdaa26133b8e592fdfcdb0eb8e4fdb0f9a.story',\n",
       " 'cnn/stories/02191dfcec72d52666af989f6cb29d2952aa5543.story',\n",
       " 'cnn/stories/021bae344a04bca4c3320d205e0cfb8b165c67c9.story',\n",
       " 'cnn/stories/021c5d3181ad1e6efb6cc5a758a89ac25fe90016.story',\n",
       " 'cnn/stories/021cd65cb4fb4c3be1fd55d8a30ff08246e76574.story',\n",
       " 'cnn/stories/021dbce5d540fdc29198dffd2c3c77d8c3d62f14.story',\n",
       " 'cnn/stories/021ea5c8d96f0466c810ec304059095437886dab.story',\n",
       " 'cnn/stories/021efcc09e2ac83d389f700827b14a952550d170.story',\n",
       " 'cnn/stories/021f96004de0993b3c894625c3e327e04b9d9fd1.story',\n",
       " 'cnn/stories/022089e593b433b4380d55a51d56694717bafd9f.story',\n",
       " 'cnn/stories/02215d58fe2eb677930d63bc9d168b0d00ca0e34.story',\n",
       " 'cnn/stories/022174bd222ecb5e9030dcb18b1ccf19f1f95ab8.story',\n",
       " 'cnn/stories/0222a7e03881449449ed83747cc20c4d55b54eb9.story',\n",
       " 'cnn/stories/0222d97ad9ce4940230adebab2ece19b4276419d.story',\n",
       " 'cnn/stories/02233bb18a6253197d7c4fdebd40976562660a4e.story',\n",
       " 'cnn/stories/022372866822cdec460aae2f0966668096105a9c.story',\n",
       " 'cnn/stories/022494f2d168e20a5eb3934ba81423c0fcefe0aa.story',\n",
       " 'cnn/stories/0225c366d5abf615fd3388f4cd4f7f58a328b07e.story',\n",
       " 'cnn/stories/0226cd790152d3b68eadb4e11be794e6799c29ff.story',\n",
       " 'cnn/stories/0226e9462111993cd81ef588d239403585240200.story',\n",
       " 'cnn/stories/02271084074dc719dfa84594eba2eae7bc5ac51d.story',\n",
       " 'cnn/stories/022713fd20ac0ffd0ddccec6d9389beedd681893.story',\n",
       " 'cnn/stories/02282e29baa76d5902c18483f8f9f710a081f754.story',\n",
       " 'cnn/stories/022b50e462964776125b41050126739b8c492d4c.story',\n",
       " 'cnn/stories/022c18def42072e28a37979c622c6ab7e31371de.story',\n",
       " 'cnn/stories/022cd8d9fef3474225634d02589b4ee507299ae5.story',\n",
       " 'cnn/stories/022d4e6fd13f93714cbcc9126922bc1516c2e1b2.story',\n",
       " 'cnn/stories/022e25f17af1a1e7955259f58b5e8dd5588a9ee0.story',\n",
       " 'cnn/stories/022e806f684e32495e76033b34c4c3fb7cbbffc0.story',\n",
       " 'cnn/stories/022eb0baea24b76783800541e7e02eed760a39eb.story',\n",
       " 'cnn/stories/02300f962ee33c394c6e0f76a26d1bc36339b80e.story',\n",
       " 'cnn/stories/023054ed9358d85fcb1e5977cac35563556e4e0d.story',\n",
       " 'cnn/stories/023056f02b6a6323ba1ce70e7ed269a6155d5647.story',\n",
       " 'cnn/stories/0230cf90285e5ceabe40f014bf07d2a3a4828d9a.story',\n",
       " 'cnn/stories/0230dee900d5ef4d8c4228e5a9591406189736f3.story',\n",
       " 'cnn/stories/02311be4aab97a81d2262963896b975c27f6503d.story',\n",
       " 'cnn/stories/0232a5bd77293535934b4a35657bb5fb03bbe5b4.story',\n",
       " 'cnn/stories/02330c814c29b6809394b56b8936a98824dc55d8.story',\n",
       " 'cnn/stories/0233120a6a9102f49d4784014bd6343b2922b900.story',\n",
       " 'cnn/stories/02338f9fa2e94066392faf3766c9d04b03902962.story',\n",
       " 'cnn/stories/023474203ac6ae0694ee8c37a3ac40bcade8bec7.story',\n",
       " 'cnn/stories/023480a2b3d87ad32f71ccc2d978e78bbb468d0e.story',\n",
       " 'cnn/stories/02365d24caa12e01fecf819f8322002af0994328.story',\n",
       " 'cnn/stories/0236b569ba353d45f3bf89c447eb3c0b2f14006a.story',\n",
       " 'cnn/stories/0236ca2ddaf407a44da8ab640f8753f83abe81c6.story',\n",
       " 'cnn/stories/02377b6c36b250e3d1ce666fc495f1066999e67d.story',\n",
       " 'cnn/stories/023811f7c5f1f760239589e875446ff57243e81c.story',\n",
       " 'cnn/stories/02385271c1f0335d4e77e7a4912e7bbf2ba15fa5.story',\n",
       " 'cnn/stories/02390fafd882a0c90dd2d4f810fec81a84de015f.story',\n",
       " 'cnn/stories/0239342f05436085947bd0c5173a4636ee772b51.story',\n",
       " 'cnn/stories/023b36b615d08acb07f5624e2c5f2de2dd6d23a1.story',\n",
       " 'cnn/stories/023b63c15d39122daae59b4f46b4b19f72a32d05.story',\n",
       " 'cnn/stories/023b9cf4dfeca75635957a73408fde39705c9d4f.story',\n",
       " 'cnn/stories/023cd84001b33aed4ff0f3f5ecb0fdd2151cf543.story',\n",
       " 'cnn/stories/023d0f7ca0b5ca28d0266c9e0acd96df738f7079.story',\n",
       " 'cnn/stories/023d900e7ec5f82ab85139be235185117c8f560f.story',\n",
       " 'cnn/stories/023efd2b40b245ffca3a19cebc20f7a6da311454.story',\n",
       " 'cnn/stories/023f86ba6bc4c6927212f2c20aed0d9a2f90cab1.story',\n",
       " 'cnn/stories/023fac38fb00b209162b0cbd4bf7c2d9f0430620.story',\n",
       " 'cnn/stories/023fb613f278da6d78d189a842366252660b46ea.story',\n",
       " 'cnn/stories/023fc2139b843df3d0c390c5d51912673b53aa2b.story',\n",
       " 'cnn/stories/024117a352a60a59b702e035f0c88fcdc6135134.story',\n",
       " 'cnn/stories/0241727902091c02e3f47d48c2acb6eef279ccaf.story',\n",
       " 'cnn/stories/0243aa2938f84145fee92ebe4f05a8b607ca46f2.story',\n",
       " 'cnn/stories/0245d8c00a4418f0803e98134a0f996e8803d424.story',\n",
       " 'cnn/stories/0246f64a1da528ea94bd1d3b17562a2fda8938f6.story',\n",
       " 'cnn/stories/0247a5c0fedefd97af6d35042bc909379ae9a903.story',\n",
       " 'cnn/stories/0247c6e137fd3c9a1fc91d8f4a2f1be61dfc54ac.story',\n",
       " 'cnn/stories/0247ebdf9d051aa413fcd0c79d154ac6dea30a6e.story',\n",
       " 'cnn/stories/02487e2fe55b6e72dec9ad2321aeceda0bf440cc.story',\n",
       " 'cnn/stories/0249cbad3e98a9928aae06549302a28c5bc77c6d.story',\n",
       " 'cnn/stories/024a83a72a7102c5c81fe6ad8aa40c4963f7d324.story',\n",
       " 'cnn/stories/024b1c6a3646d9b60e8d9c807114485cf487ce6d.story',\n",
       " 'cnn/stories/024b78431894c2333604f784131f2580b974d7a8.story',\n",
       " 'cnn/stories/024b787a9e1b87020aafeb98d361516eb6f119ad.story',\n",
       " 'cnn/stories/024bfdaa5e37cea4edc43a0c85c9567c2faf1612.story',\n",
       " 'cnn/stories/024c0c0e312a44a9a09b241701f19519f2db5cef.story',\n",
       " 'cnn/stories/024ce3d2c8996bef5dc5af92d2bc7e899387b982.story',\n",
       " 'cnn/stories/024d6a59115d1d384ec548ec999d016739aca1e2.story',\n",
       " 'cnn/stories/024e75989c7aa0efea0a783c9f3ed124000d5687.story',\n",
       " 'cnn/stories/024eae075108ae834a2338bab32bf4ca732e1d43.story',\n",
       " 'cnn/stories/024eefc4ba54116247f329ae2c7c19903e0074a0.story',\n",
       " 'cnn/stories/024f0a70a894047492e31db944c081ee66258102.story',\n",
       " 'cnn/stories/024f670d75e7d2f9930d22b75cabed51979095fb.story',\n",
       " 'cnn/stories/025003ba15d7463db7d31324151cfbd7fbe6c047.story',\n",
       " 'cnn/stories/02508146279776c78e6aeab7492b3ae4c7b59ca4.story',\n",
       " 'cnn/stories/02510723978086825e670230d955102239f54547.story',\n",
       " 'cnn/stories/02513f615bdb7eed39e407edda28dc7e88d1c8e3.story',\n",
       " 'cnn/stories/02520639b8fb19c960b42643e37061c814834803.story',\n",
       " 'cnn/stories/02525623eb13cc42fea3fb55fb0c4d552c5a85ea.story',\n",
       " 'cnn/stories/025304fdff0e2174b8854f35aca17bc63d8730bb.story',\n",
       " 'cnn/stories/02530b65ff42f773e16a40799085af9e475739bb.story',\n",
       " 'cnn/stories/02537a02368d6d47173f48ca8fda91575d5532c3.story',\n",
       " 'cnn/stories/02539361c1e59708933aba0f3a22e9c5f6bc9b3f.story',\n",
       " 'cnn/stories/0254b59405a4bd611d20137ba4ffa579a3f872d1.story',\n",
       " 'cnn/stories/02564607fbd684c9d4e75627302ea1eed5b26bbc.story',\n",
       " 'cnn/stories/02566c1f6c22c3fa162d89ffeb37c3e64a61fb29.story',\n",
       " 'cnn/stories/0257aa00fa70f33ef7fea2120c749c94ea4a9d44.story',\n",
       " 'cnn/stories/025821fa75f749d4f9fbee8734e2991829c27942.story',\n",
       " 'cnn/stories/02585e26d02d16d5e1dabf24751e03212a554854.story',\n",
       " 'cnn/stories/02590e027714f18d195f9a10383cc3d1d2b4d888.story',\n",
       " 'cnn/stories/025a6bb299c1c6e1c8632e2b601e5a937ed2cba2.story',\n",
       " 'cnn/stories/025bee3924aab5c2349229ba62116bcc8d9761a7.story',\n",
       " 'cnn/stories/025c44cb4f88863ea8859d8f6fafc876b2a4f6c4.story',\n",
       " 'cnn/stories/025c8069f148d69490998d48ed65c2db49d06f1e.story',\n",
       " 'cnn/stories/025eefd2069953534a02800c4a8a02aac730a0fe.story',\n",
       " 'cnn/stories/025fbeb4e1ee6f3d9ce78ac14bfb1a5c2d51fa11.story',\n",
       " 'cnn/stories/025fe377cba5cddc3a7e97f4c03dabd957c4eef4.story',\n",
       " 'cnn/stories/026032740b6ebcd2d7e9c72cb9e7006852cd89df.story',\n",
       " 'cnn/stories/02604253bc8b83628381c7e6ba803f4bed2dd2ac.story',\n",
       " 'cnn/stories/026128913c8d8f0e982a36e9752577cc85c11813.story',\n",
       " 'cnn/stories/02625c8d96d090527fec268f59e75cefa0a6f11a.story',\n",
       " 'cnn/stories/0262c45007e116626eafbecfe3d9adff0adae961.story',\n",
       " 'cnn/stories/02634f407edd41bcb484fa0024e2cbecf72ac330.story',\n",
       " 'cnn/stories/02635d9c8d7d280bd484cbe14375e2275af396da.story',\n",
       " 'cnn/stories/0263d2540bdff860c998243fba2f12dc1caafa6a.story',\n",
       " 'cnn/stories/02641cb725e7f2f1d3a843c4738c01436ed95d8f.story',\n",
       " 'cnn/stories/0264c489ba126938bc91c07211df95804e41ff67.story',\n",
       " 'cnn/stories/0264d85da73237f1967bcab20b2f99313a00250e.story',\n",
       " 'cnn/stories/0267e4a4d09ba6da2220ebfaa5550c5531fdbc94.story',\n",
       " 'cnn/stories/02682f94649028300f028e904f885d2663faffa8.story',\n",
       " 'cnn/stories/0268cd2a2b8eef64cdc1c6d0bea4e7d18fcfee8f.story',\n",
       " 'cnn/stories/02695a50b6ac9587f3a806cb05dd297223deed12.story',\n",
       " 'cnn/stories/026976b0f7591f1c9f6dd1ad067d2aeae19bbe51.story',\n",
       " 'cnn/stories/0269d39abbea3edadd225fa97d818f5a789b4c72.story',\n",
       " 'cnn/stories/0269d98d9bc840782c54a05ad196af1bcd32945f.story',\n",
       " 'cnn/stories/026c282ca35d66d8d253b6793a37bbe71f260c63.story',\n",
       " 'cnn/stories/026c68ffba02df516ca37f97cf067e77a8af5df3.story',\n",
       " 'cnn/stories/026cea5048ae7df05c561022a9e8c57f6f16fc58.story',\n",
       " 'cnn/stories/026cf4facf827f583865ffc3c602d4adec50225c.story',\n",
       " 'cnn/stories/026de5f7212d678958df405f51ee916d455a88dd.story',\n",
       " 'cnn/stories/026f68e35fc34eaa033bf2957dc9f0653556650f.story',\n",
       " 'cnn/stories/026fcea25bc5f9cebb9c3d744362d20127c413cf.story',\n",
       " 'cnn/stories/026fd6276bb556edf984499ad8a345b3d6547e9a.story',\n",
       " 'cnn/stories/02705ce5de465586c1a007f76643edbcc87fb048.story',\n",
       " 'cnn/stories/02706a14901ca6c20512228f1a4fca370ba28b75.story',\n",
       " 'cnn/stories/02726119c3d21e2b0f7b3d9c2c7c391a0ecb5566.story',\n",
       " 'cnn/stories/0273000e573b45b48dea43be96aca5de17852211.story',\n",
       " 'cnn/stories/027388f2c138301038249446d8c71170f470e83a.story',\n",
       " 'cnn/stories/027581b68255b96c0d430f668467fa145aeca3b7.story',\n",
       " 'cnn/stories/027685bc7ffb98a19bf76d67adc5b2bb111f43bc.story',\n",
       " 'cnn/stories/02769885a005b2a4c26edaab9e7a0dad5036a5c4.story',\n",
       " 'cnn/stories/0276ac7a8bc00bdc47e1e99774ee7d5017f0f0a8.story',\n",
       " 'cnn/stories/0276ba238b029b225b43e657dc69b2847d1479a2.story',\n",
       " 'cnn/stories/0277703c0c2d15575a512cfef78a99396bf5ed7c.story',\n",
       " 'cnn/stories/027843e6477990c2ecd82deb46d45061b6ce72cb.story',\n",
       " 'cnn/stories/0278785267f8ba10cde592473cf46497223b75bd.story',\n",
       " 'cnn/stories/0278989ccfa6090a5db59e5a4036942e0ce2ffb7.story',\n",
       " 'cnn/stories/027936fedb5e785fe79e84cb6e55c9cc26042ad3.story',\n",
       " 'cnn/stories/027a0e39550b8fb4fd7e84252cebd8675526a14f.story',\n",
       " 'cnn/stories/027a99a889b38fdb935cd14c204a5fca8a329bc1.story',\n",
       " 'cnn/stories/027b5e1c2573b44e61521e094e4ae2008dfe2dff.story',\n",
       " 'cnn/stories/027bd1ebe1f9e9ede757f42ab955e4075565eaad.story',\n",
       " 'cnn/stories/027c332f910bdb08fcef15cffd385e44f054ce84.story',\n",
       " 'cnn/stories/027dacf0ab091e39ccebe8b809b8d285a93bdcfe.story',\n",
       " 'cnn/stories/027ddddbad5a45720cd5f01241acd3ab02f606da.story',\n",
       " 'cnn/stories/027e97472bc19a12819ca666d290b6eedde73a04.story',\n",
       " 'cnn/stories/027eb44752671b224e7236209eadfbddbd51749d.story',\n",
       " 'cnn/stories/0280767a62d5b0f2cedf4378636261744b663892.story',\n",
       " 'cnn/stories/0281c649039f99af337e91190d5bdcb5e9734969.story',\n",
       " 'cnn/stories/0281ccf2727093663d87773173a1a636cdd4f630.story',\n",
       " 'cnn/stories/02828c1ac3ee5e31b25e8c050904fab23ed0ed56.story',\n",
       " 'cnn/stories/028310a745cf62edade6d651479c75a46a1ab073.story',\n",
       " 'cnn/stories/0284735ba9a7ee2da8b13793b53df63fce94c484.story',\n",
       " 'cnn/stories/028552a935f12bdb0d650a40ecff15e053034c76.story',\n",
       " 'cnn/stories/028585356fbab01684fd1e2aa5a745c4e5e034fa.story',\n",
       " 'cnn/stories/028590f21458862ee63ca97973ebe1a039ae8346.story',\n",
       " 'cnn/stories/0286103f04a2400b86319251ae13b4f73515cdc0.story',\n",
       " 'cnn/stories/028672ffcd36907dbb0b75b87e188c71c6bc7429.story',\n",
       " 'cnn/stories/028754ab608b84d914ec1857a4824490738adf4f.story',\n",
       " 'cnn/stories/02878cf098084bc701898dd86ec37c7c5ccb3b01.story',\n",
       " 'cnn/stories/0287e6e3fdfea50a2ae3fb88fc36a729552cc8f1.story',\n",
       " 'cnn/stories/0288282329f6cc0b9bb490ffb4fc2d60c76f86af.story',\n",
       " 'cnn/stories/02886c4d6106f3db522c98882706505005cb5963.story',\n",
       " 'cnn/stories/02887117a166b724f3e086138f91715c505c724d.story',\n",
       " 'cnn/stories/0288d3bd3bd65bae7f8e9b29b63b14d045f3c4d5.story',\n",
       " 'cnn/stories/02891fb68db2d07b44fc75330d2da453bc979d37.story',\n",
       " 'cnn/stories/028b9f8f3c329bfed5a581baf35479b158723fe2.story',\n",
       " 'cnn/stories/028bc1d9578aa056042245dda153fe326754ff92.story',\n",
       " 'cnn/stories/028c5205b899cd26e0226b679e34f7a4baaa7c43.story',\n",
       " 'cnn/stories/028c5f418e08ed4e96d7139ba902f2eb9987d9c5.story',\n",
       " 'cnn/stories/028c96949c7d2bdf46e1ab89cd0c6ef4ded963ff.story',\n",
       " 'cnn/stories/028d564ebdabad1214f079ae6c669a760e8e385b.story',\n",
       " 'cnn/stories/028df16d5652f1bcb1bdb4e39d4eae8fcf8666b9.story',\n",
       " 'cnn/stories/028ead68886640b09d648b8c82f611099b0ed6f7.story',\n",
       " 'cnn/stories/028f626e0ecc21c8a51330325e349398f119a591.story',\n",
       " 'cnn/stories/02904bd4421fa1ea3ee7d647cade3ebde8694391.story',\n",
       " 'cnn/stories/029050376967d7c47a862ade09bf39a64cc97f92.story',\n",
       " 'cnn/stories/02907b8a76079e56ff670091a21a0d08d00ac6db.story',\n",
       " 'cnn/stories/0290feb13240ea14c3401f17448905eb60a382ae.story',\n",
       " 'cnn/stories/02927c88776bcc84fc883435a5037b4c8ca70e83.story',\n",
       " 'cnn/stories/0292d0189dde3ecd38a3695c5ed9678065a85497.story',\n",
       " 'cnn/stories/0292f175ac31417d56e52c81e20e6e582b1c6213.story',\n",
       " 'cnn/stories/0293107dc735291995305b2b55496637ebb21253.story',\n",
       " 'cnn/stories/029349034f859b64c21daf39b3eaf56a56e069a8.story',\n",
       " 'cnn/stories/0295c4f86921f5ce138643c929e86a06c8d698a8.story',\n",
       " 'cnn/stories/0295c779ae4a8a06d8fd47f370c35aff3746689b.story',\n",
       " 'cnn/stories/02985072d3451ce27557f96b2d1ab28d01796d30.story',\n",
       " 'cnn/stories/0298ece54b634a1ea494c4cda530c36e2e7c7652.story',\n",
       " 'cnn/stories/029a53fbac29898775420964629562063013db9b.story',\n",
       " 'cnn/stories/029ae7053b983a009e81af56a95ceee1c68cec08.story',\n",
       " 'cnn/stories/029af794302766dfb3f5dd1e33f499d3aa187a2e.story',\n",
       " 'cnn/stories/029b2d379c90df7f7d7e64a54fb9f0e3e882bfda.story',\n",
       " 'cnn/stories/029bc476dd93f81bf355f8e8717b7943704e0162.story',\n",
       " 'cnn/stories/029bcd7faefcaed0f597642b7340d83cb4509756.story',\n",
       " 'cnn/stories/029cb353e9cc8ddfda29f13217a20e01d38a1d95.story',\n",
       " 'cnn/stories/029d493db1c6f4d2a3b29d5395b9f27a2983c76d.story',\n",
       " 'cnn/stories/029e07f312857dea43264baf0baf7bcf03bdb227.story',\n",
       " 'cnn/stories/029e143f914b3eeefa43e787c8a72328b8d12b54.story',\n",
       " 'cnn/stories/029e659b2cbb8a7a46d83f1984efec378aec7c54.story',\n",
       " 'cnn/stories/029ef16ef7a3288cb3ee71d01a17d3764753c243.story',\n",
       " 'cnn/stories/029f5f6775ecd8b5fad0dc0e55ab4a9754fcad8f.story',\n",
       " 'cnn/stories/02a10e3adde1d51286218cfd3e3b8d3a7697b5a1.story',\n",
       " 'cnn/stories/02a27a18b0eed52ab39670f007b6b77da518b4ce.story',\n",
       " 'cnn/stories/02a2af5abc2c9b140b8ded1f73daf85fb75ed1e0.story',\n",
       " 'cnn/stories/02a300f080fbcc72542d250b40220919af9d0c27.story',\n",
       " 'cnn/stories/02a3ebc55777174ee7db6c5bcd0a5b2fb993444b.story',\n",
       " 'cnn/stories/02a3eeff8a17721bd8ec8ffa0a6c9bfd98a48ce7.story',\n",
       " 'cnn/stories/02a44826ef0b55c6f772774890603178fcf8df3c.story',\n",
       " 'cnn/stories/02a45f544dfdf4503dbb1c3b1fb374f978eb7237.story',\n",
       " 'cnn/stories/02a57307f50d595b1c01fa18688f51c6161de142.story',\n",
       " 'cnn/stories/02a5f1a831b5daeb6b8ec60c083c99f6249b97ac.story',\n",
       " 'cnn/stories/02a60291ec2f525279bb4e6bbb8f4be7fde40974.story',\n",
       " 'cnn/stories/02a609b00c6d7ca522b5ae7297eb3f673c16ab50.story',\n",
       " 'cnn/stories/02a62f4045100d558801373211f5ad410f963340.story',\n",
       " 'cnn/stories/02a6460c4793528eba2bb29aa5eac995edfad8e2.story',\n",
       " 'cnn/stories/02a6c0932681500df92b1e49cac472ec671b0ea0.story',\n",
       " 'cnn/stories/02a72ce1e1ddcf0b58d7c12c3dd4dcdab9f970ea.story',\n",
       " 'cnn/stories/02a73fa6165efae4df3711a37c6beb0e328d8f86.story',\n",
       " 'cnn/stories/02a7dafda52e2cf23e727a32bfb9752894858388.story',\n",
       " 'cnn/stories/02a801fd7eaf84885445d70e9b865abfbad74291.story',\n",
       " 'cnn/stories/02a84f2af1daf4fae7e463adb186cfeb1877fc07.story',\n",
       " 'cnn/stories/02a969f66b83ef0da3a960abde2f02e7bae01109.story',\n",
       " 'cnn/stories/02aa74a714235c99df4cd71b02065625d259b7c8.story',\n",
       " 'cnn/stories/02aafe38ea456f761cc0fd92e428a1d7b25a49ae.story',\n",
       " 'cnn/stories/02ab1348b5217cc3df52a1c22430c97a34f38b8a.story',\n",
       " 'cnn/stories/02abcd3931c8bf78a0531f6ce0d7f21f72bee963.story',\n",
       " 'cnn/stories/02abf8300c2187cc1b810e501bbe5258e908c62b.story',\n",
       " 'cnn/stories/02ac690764cdfa11cb24be085f3337552471f612.story',\n",
       " 'cnn/stories/02adb281396578efbc76766bb3c81143b07c196b.story',\n",
       " 'cnn/stories/02adb7a44a6ebbeccc564796c1dfa4c441ea0272.story',\n",
       " 'cnn/stories/02aec4b306fa659c7bb5f91b8c089002eb1a3b3c.story',\n",
       " 'cnn/stories/02aef53a8ae894b86d408fa90194ceaa1332ba5b.story',\n",
       " 'cnn/stories/02af8023a7ad56a5d1796263e5fb414554cd5e5b.story',\n",
       " 'cnn/stories/02b08807ee1c317289075a00c15a39b52c560b20.story',\n",
       " 'cnn/stories/02b2f9523b5d2e9bf07d36ed725aff453ebc9224.story',\n",
       " 'cnn/stories/02b3f706a85cdf9b71a0f85c5ee8534d85e4a06f.story',\n",
       " 'cnn/stories/02b49af7785ec70782772c0d921e5ab286772f69.story',\n",
       " 'cnn/stories/02b501f3302ea6a83449667e944eadf4e7c56992.story',\n",
       " 'cnn/stories/02b52d880f20f464307855fa1171579f8470c78c.story',\n",
       " 'cnn/stories/02b56fb7ef2386c54451fd7db6f3ab386e20b897.story',\n",
       " 'cnn/stories/02b6a9e383f0b5831f176a2a2045de9358082ca5.story',\n",
       " 'cnn/stories/02b75ca4e19495e413a88a3ee12741ce5fe4f784.story',\n",
       " 'cnn/stories/02b7cd5b0d1e192f1dcb9da18d21c1a2bec00562.story',\n",
       " 'cnn/stories/02b7e1b232341e46adce5287877f0f1c8fe0fbb1.story',\n",
       " 'cnn/stories/02b80df4f96c33f13f97c762781f814052104301.story',\n",
       " 'cnn/stories/02b81fb9f6c2cdcc0e7cb7745f45bba275b950e2.story',\n",
       " 'cnn/stories/02b840e0d53d83fbf885632ded4424b1683d32f8.story',\n",
       " 'cnn/stories/02bb059892fba309ca11ad374b18ef0d8a7623ec.story',\n",
       " 'cnn/stories/02bb6879a7a844062794e0244ad6ebaa35a4b714.story',\n",
       " 'cnn/stories/02bb8bf46b2ba5aec79ba570ea12bf502250dc07.story',\n",
       " ...]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILES = glob.glob('%s/*%s' % (TRAINING_DIRECTORY, EXTENSION))\n",
    "print(len(FILES))\n",
    "FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES = FILES[:MAX_FILES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define method for generating text from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    # replace all occurences of multiple newlines and replace them\n",
    "    # with a single newline padded with spaces so it is treated as a\n",
    "    # token\n",
    "    text = ' \\n '.join(t for t in text.split('\\n') if t)\n",
    "    table = {ord(c): None for c in '<>'}\n",
    "    text = text.translate(table)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generator(files, preprocessor=None):\n",
    "    for f in files:\n",
    "        text = open(f).read()\n",
    "        if preprocessor is not None:\n",
    "            text = preprocessor(text)\n",
    "        # remove highlights\n",
    "        body, highlight1, *_ = text.split('@highlight')\n",
    "        yield body, highlight1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, tokenizer, maxlen, begin_token, end_token):\n",
    "    tokens = tokenizer([text])[0]\n",
    "    tokens = tokens[:maxlen-2]\n",
    "    return [[begin_token] + tokens + [end_token]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('It\\'s official: U.S. President Barack Obama wants lawmakers to weigh in on whether to use military force in Syria. \\n Obama sent a letter to the heads of the House and Senate on Saturday night, hours after announcing that he believes military action against Syrian targets is the right step to take over the alleged use of chemical weapons. \\n The proposed legislation from Obama asks Congress to approve the use of military force \"to deter, disrupt, prevent and degrade the potential for future uses of chemical weapons or other weapons of mass destruction.\" \\n It\\'s a step that is set to turn an international crisis into a fierce domestic political battle. \\n There are key questions looming over the debate: What did U.N. weapons inspectors find in Syria? What happens if Congress votes no? And how will the Syrian government react? \\n In a televised address from the White House Rose Garden earlier Saturday, the president said he would take his case to Congress, not because he has to -- but because he wants to. \\n \"While I believe I have the authority to carry out this military action without specific congressional authorization, I know that the country will be stronger if we take this course, and our actions will be even more effective,\" he said. \"We should have this debate, because the issues are too big for business as usual.\" \\n Obama said top congressional leaders had agreed to schedule a debate when the body returns to Washington on September 9. The Senate Foreign Relations Committee will hold a hearing over the matter on Tuesday, Sen. Robert Menendez said. \\n Transcript: Read Obama\\'s full remarks \\n Syrian crisis: Latest developments \\n U.N. inspectors leave Syria \\n Obama\\'s remarks came shortly after U.N. inspectors left Syria, carrying evidence that will determine whether chemical weapons were used in an attack early last week in a Damascus suburb. \\n \"The aim of the game here, the mandate, is very clear -- and that is to ascertain whether chemical weapons were used -- and not by whom,\" U.N. spokesman Martin Nesirky told reporters on Saturday. \\n But who used the weapons in the reported toxic gas attack in a Damascus suburb on August 21 has been a key point of global debate over the Syrian crisis. \\n Top U.S. officials have said there\\'s no doubt that the Syrian government was behind it, while Syrian officials have denied responsibility and blamed jihadists fighting with the rebels. \\n British and U.S. intelligence reports say the attack involved chemical weapons, but U.N. officials have stressed the importance of waiting for an official report from inspectors. \\n The inspectors will share their findings with U.N. Secretary-General Ban Ki-moon Ban, who has said he wants to wait until the U.N. team\\'s final report is completed before presenting it to the U.N. Security Council. \\n The Organization for the Prohibition of Chemical Weapons, which nine of the inspectors belong to, said Saturday that it could take up to three weeks to analyze the evidence they collected. \\n \"It needs time to be able to analyze the information and the samples,\" Nesirky said. \\n He noted that Ban has repeatedly said there is no alternative to a political solution to the crisis in Syria, and that \"a military solution is not an option.\" \\n Bergen:  Syria is a problem from hell for the U.S. \\n Obama: \\'This menace must be confronted\\' \\n Obama\\'s senior advisers have debated the next steps to take, and the president\\'s comments Saturday came amid mounting political pressure over the situation in Syria. Some U.S. lawmakers have called for immediate action while others warn of stepping into what could become a quagmire. \\n Some global leaders have expressed support, but the British Parliament\\'s vote against military action earlier this week was a blow to Obama\\'s hopes of getting strong backing from key NATO allies. \\n On Saturday, Obama proposed what he said would be a limited military action against Syrian President Bashar al-Assad. Any military attack would not be open-ended or include U.S. ground forces, he said. \\n Syria\\'s alleged use of chemical weapons earlier this month \"is an assault on human dignity,\" the president said. \\n A failure to respond with force, Obama argued,  \"could lead to escalating use of chemical weapons or their proliferation to terrorist groups who would do our people harm. In a world with many dangers, this menace must be confronted.\" \\n Syria missile strike: What would happen next? \\n Map: U.S. and allied assets around Syria \\n Obama decision came Friday night \\n On Friday night, the president made a last-minute decision to consult lawmakers. \\n What will happen if they vote no? \\n It\\'s unclear. A senior administration official told CNN that Obama has the authority to act without Congress -- even if Congress rejects his request for authorization to use force. \\n Obama on Saturday continued to shore up support for a strike on the al-Assad government. \\n He spoke by phone with French President Francois Hollande before his Rose Garden speech. \\n \"The two leaders agreed that the international community must deliver a resolute message to the Assad regime -- and others who would consider using chemical weapons -- that these crimes are unacceptable and those who violate this international norm will be held accountable by the world,\" the White House said. \\n Meanwhile, as uncertainty loomed over how Congress would weigh in, U.S. military officials said they remained at the ready. \\n 5 key assertions: U.S. intelligence report on Syria \\n Syria: Who wants what after chemical weapons horror \\n Reactions mixed to Obama\\'s speech \\n A spokesman for the Syrian National Coalition said that the opposition group was disappointed by Obama\\'s announcement. \\n \"Our fear now is that the lack of action could embolden the regime and they repeat his attacks in a more serious way,\" said spokesman Louay Safi. \"So we are quite concerned.\" \\n Some members of Congress applauded Obama\\'s decision. \\n House Speaker John Boehner, Majority Leader Eric Cantor, Majority Whip Kevin McCarthy and Conference Chair Cathy McMorris Rodgers issued a statement Saturday praising the president. \\n \"Under the Constitution, the responsibility to declare war lies with Congress,\" the Republican lawmakers said. \"We are glad the president is seeking authorization for any military action in Syria in response to serious, substantive questions being raised.\" \\n More than 160 legislators, including 63 of Obama\\'s fellow Democrats, had signed letters calling for either a vote or at least a \"full debate\" before any U.S. action. \\n British Prime Minister David Cameron, whose own attempt to get lawmakers in his country to support military action in Syria failed earlier this week, responded to Obama\\'s speech in a Twitter post Saturday. \\n \"I understand and support Barack Obama\\'s position on Syria,\" Cameron said. \\n An influential lawmaker in Russia -- which has stood by Syria and criticized the United States -- had his own theory. \\n \"The main reason Obama is turning to the Congress:  the military operation did not get enough support either in the world, among allies of the US or in the United States itself,\" Alexei Pushkov, chairman of the international-affairs committee of the Russian State Duma, said in a Twitter post. \\n In the United States, scattered groups of anti-war protesters around the country took to the streets Saturday. \\n \"Like many other Americans...we\\'re just tired of the United States getting involved and invading and bombing other countries,\" said Robin Rosecrans, who was among hundreds at a Los Angeles demonstration. \\n What do Syria\\'s neighbors think? \\n Why Russia, China, Iran stand by Assad \\n Syria\\'s government unfazed \\n After Obama\\'s speech, a military and political analyst on Syrian state TV said Obama is \"embarrassed\" that Russia opposes military action against Syria, is \"crying for help\" for someone to come to his rescue and is facing two defeats -- on the political and military levels. \\n Syria\\'s prime minister appeared unfazed by the saber-rattling. \\n \"The Syrian Army\\'s status is on maximum readiness and fingers are on the trigger to confront all challenges,\" Wael Nader al-Halqi said during a meeting with a delegation of Syrian expatriates from Italy, according to a banner on Syria State TV that was broadcast prior to Obama\\'s address. \\n An anchor on Syrian state television said Obama \"appeared to be preparing for an aggression on Syria based on repeated lies.\" \\n A top Syrian diplomat told the state television network that Obama was facing pressure to take military action from Israel, Turkey, some Arabs and right-wing extremists in the United States. \\n \"I think he has done well by doing what Cameron did in terms of taking the issue to Parliament,\" said Bashar Jaafari, Syria\\'s ambassador to the United Nations. \\n Both Obama and Cameron, he said, \"climbed to the top of the tree and don\\'t know how to get down.\" \\n The Syrian government has denied that it used chemical weapons in the August 21 attack, saying that jihadists fighting with the rebels used them in an effort to turn global sentiments against it. \\n British intelligence had put the number of people killed in the attack at more than 350. \\n On Saturday, Obama said \"all told, well over 1,000 people were murdered.\" U.S. Secretary of State John Kerry on Friday cited a death toll of 1,429, more than 400 of them children. No explanation was offered for the discrepancy. \\n Iran: U.S. military action in Syria would spark \\'disaster\\' \\n Opinion: Why strikes in Syria are a bad idea \\n ',\n",
       " ' \\n Syrian official: Obama climbed to the top of the tree, \"doesn\\'t know how to get down\" \\n ')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(text_generator(FILES, preprocessor=preprocessor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer as _Tokenizer\n",
    "\n",
    "class Tokenizer(_Tokenizer):\n",
    "    def fit_on_texts(self, texts):\n",
    "        \"\"\"Updates internal vocabulary based on a list of texts.\n",
    "        In the case where texts contains lists, we assume each entry of the lists\n",
    "        to be a token.\n",
    "        Required before using `texts_to_sequences` or `texts_to_matrix`.\n",
    "        # Arguments\n",
    "            texts: can be a list of strings,\n",
    "                a generator of strings (for memory-efficiency),\n",
    "                or a list of list of strings.\n",
    "        \"\"\"\n",
    "        for text in texts:\n",
    "            self.document_count += 1\n",
    "            if self.char_level or isinstance(text, list):\n",
    "                seq = text\n",
    "            else:\n",
    "                seq = text_to_word_sequence(text,\n",
    "                                            self.filters,\n",
    "                                            self.lower,\n",
    "                                            self.split)\n",
    "            for w in seq:\n",
    "                if w in self.word_counts:\n",
    "                    self.word_counts[w] += 1\n",
    "                else:\n",
    "                    self.word_counts[w] = 1\n",
    "            for w in set(seq):\n",
    "                if w in self.word_docs:\n",
    "                    self.word_docs[w] += 1\n",
    "                else:\n",
    "                    self.word_docs[w] = 1\n",
    "\n",
    "        wcounts = list(self.word_counts.items())\n",
    "        wcounts.sort(key=lambda x: x[1], reverse=True)\n",
    "        sorted_voc = [wc[0] for wc in wcounts]\n",
    "        # note that index 0, 1, 2 is reserved, never assigned to an existing word\n",
    "        self.word_index = dict(list(zip(sorted_voc, list(range(4, len(sorted_voc) + 4)))))\n",
    "        self.word_index[self.oov_token] = 3\n",
    "\n",
    "        for w, c in list(self.word_docs.items()):\n",
    "            self.index_docs[self.word_index[w]] = c\n",
    "\n",
    "    def texts_to_sequences_generator(self, texts):\n",
    "        \"\"\"Transforms each text in `texts` in a sequence of integers.\n",
    "        Each item in texts can also be a list, in which case we assume each item of that list\n",
    "        to be a token.\n",
    "        Only top \"num_words\" most frequent words will be taken into account.\n",
    "        Only words known by the tokenizer will be taken into account.\n",
    "        # Arguments\n",
    "            texts: A list of texts (strings).\n",
    "        # Yields\n",
    "            Yields individual sequences.\n",
    "        \"\"\"\n",
    "        num_words = self.num_words\n",
    "        for text in texts:\n",
    "            if self.char_level or isinstance(text, list):\n",
    "                seq = text\n",
    "            else:\n",
    "                seq = text_to_word_sequence(text,\n",
    "                                            self.filters,\n",
    "                                            self.lower,\n",
    "                                            self.split)\n",
    "            vect = []\n",
    "            for w in seq:\n",
    "                i = self.word_index.get(w)\n",
    "                if i is not None and (self.num_words and i < self.num_words):\n",
    "                    vect.append(i)\n",
    "                elif self.oov_token is not None:\n",
    "                    i = self.word_index.get(self.oov_token)\n",
    "                    if i is not None:\n",
    "                        vect.append(i)\n",
    "            yield vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = Tokenizer(\n",
    "    num_words=NUM_WORDS,\n",
    "    filters=FILTERS,  # no newline\n",
    "    oov_token=OOV_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = text_generator(FILES, preprocessor=preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.6 ms, sys: 8.5 ms, total: 56.1 ms\n",
      "Wall time: 60 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TOKENIZER.fit_on_texts(text for train_pair in gen for text in train_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENIZER.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENIZER.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6590,\n",
       " {'the': 4,\n",
       "  'to': 5,\n",
       "  'of': 6,\n",
       "  'a': 7,\n",
       "  'in': 8,\n",
       "  'and': 9,\n",
       "  'that': 10,\n",
       "  'is': 11,\n",
       "  'for': 12,\n",
       "  'on': 13,\n",
       "  'said': 14,\n",
       "  'it': 15,\n",
       "  'with': 16,\n",
       "  'was': 17,\n",
       "  'he': 18,\n",
       "  'as': 19,\n",
       "  'has': 20,\n",
       "  'be': 21,\n",
       "  'his': 22,\n",
       "  'have': 23,\n",
       "  'from': 24,\n",
       "  'by': 25,\n",
       "  'at': 26,\n",
       "  'this': 27,\n",
       "  'are': 28,\n",
       "  'not': 29,\n",
       "  'but': 30,\n",
       "  'who': 31,\n",
       "  'an': 32,\n",
       "  'they': 33,\n",
       "  'will': 34,\n",
       "  'u': 35,\n",
       "  'their': 36,\n",
       "  'more': 37,\n",
       "  'been': 38,\n",
       "  'or': 39,\n",
       "  'i': 40,\n",
       "  's': 41,\n",
       "  'we': 42,\n",
       "  'year': 43,\n",
       "  'had': 44,\n",
       "  'were': 45,\n",
       "  'she': 46,\n",
       "  'people': 47,\n",
       "  'than': 48,\n",
       "  'cnn': 49,\n",
       "  'also': 50,\n",
       "  'one': 51,\n",
       "  'about': 52,\n",
       "  'all': 53,\n",
       "  'which': 54,\n",
       "  'her': 55,\n",
       "  'united': 56,\n",
       "  'new': 57,\n",
       "  'after': 58,\n",
       "  'states': 59,\n",
       "  'government': 60,\n",
       "  'if': 61,\n",
       "  'state': 62,\n",
       "  'other': 63,\n",
       "  'can': 64,\n",
       "  'what': 65,\n",
       "  'isis': 66,\n",
       "  'when': 67,\n",
       "  'obama': 68,\n",
       "  'there': 69,\n",
       "  'would': 70,\n",
       "  'no': 71,\n",
       "  'its': 72,\n",
       "  'so': 73,\n",
       "  'into': 74,\n",
       "  'court': 75,\n",
       "  \"it's\": 76,\n",
       "  'two': 77,\n",
       "  'him': 78,\n",
       "  'last': 79,\n",
       "  'syria': 80,\n",
       "  'according': 81,\n",
       "  'says': 82,\n",
       "  'against': 83,\n",
       "  'white': 84,\n",
       "  'up': 85,\n",
       "  'law': 86,\n",
       "  'over': 87,\n",
       "  'syrian': 88,\n",
       "  'could': 89,\n",
       "  'out': 90,\n",
       "  'told': 91,\n",
       "  'time': 92,\n",
       "  'our': 93,\n",
       "  'years': 94,\n",
       "  'military': 95,\n",
       "  'president': 96,\n",
       "  'because': 97,\n",
       "  'you': 98,\n",
       "  'most': 99,\n",
       "  'now': 100,\n",
       "  'police': 101,\n",
       "  'some': 102,\n",
       "  'any': 103,\n",
       "  'before': 104,\n",
       "  'those': 105,\n",
       "  'like': 106,\n",
       "  'rights': 107,\n",
       "  'do': 108,\n",
       "  'first': 109,\n",
       "  'day': 110,\n",
       "  'may': 111,\n",
       "  'old': 112,\n",
       "  'where': 113,\n",
       "  'take': 114,\n",
       "  'case': 115,\n",
       "  'while': 116,\n",
       "  'even': 117,\n",
       "  'world': 118,\n",
       "  'many': 119,\n",
       "  'them': 120,\n",
       "  'get': 121,\n",
       "  '000': 122,\n",
       "  'country': 123,\n",
       "  'attack': 124,\n",
       "  'including': 125,\n",
       "  'just': 126,\n",
       "  '1': 127,\n",
       "  'american': 128,\n",
       "  'city': 129,\n",
       "  'federal': 130,\n",
       "  'million': 131,\n",
       "  'group': 132,\n",
       "  'back': 133,\n",
       "  'another': 134,\n",
       "  'department': 135,\n",
       "  'iraq': 136,\n",
       "  'week': 137,\n",
       "  'statement': 138,\n",
       "  'down': 139,\n",
       "  'between': 140,\n",
       "  'still': 141,\n",
       "  'my': 142,\n",
       "  'only': 143,\n",
       "  'official': 144,\n",
       "  'very': 145,\n",
       "  'report': 146,\n",
       "  'team': 147,\n",
       "  'part': 148,\n",
       "  'since': 149,\n",
       "  'whether': 150,\n",
       "  'did': 151,\n",
       "  'called': 152,\n",
       "  'these': 153,\n",
       "  'way': 154,\n",
       "  'make': 155,\n",
       "  'found': 156,\n",
       "  'then': 157,\n",
       "  'thursday': 158,\n",
       "  'house': 159,\n",
       "  'how': 160,\n",
       "  'al': 161,\n",
       "  'human': 162,\n",
       "  'former': 163,\n",
       "  'ebola': 164,\n",
       "  'action': 165,\n",
       "  'n': 166,\n",
       "  'clear': 167,\n",
       "  'say': 168,\n",
       "  'think': 169,\n",
       "  'irs': 170,\n",
       "  'right': 171,\n",
       "  'man': 172,\n",
       "  'video': 173,\n",
       "  'see': 174,\n",
       "  'tax': 175,\n",
       "  'such': 176,\n",
       "  'weapons': 177,\n",
       "  'without': 178,\n",
       "  'returns': 179,\n",
       "  'martin': 180,\n",
       "  'officials': 181,\n",
       "  'three': 182,\n",
       "  'support': 183,\n",
       "  'national': 184,\n",
       "  'meeting': 185,\n",
       "  'well': 186,\n",
       "  'work': 187,\n",
       "  'best': 188,\n",
       "  'go': 189,\n",
       "  'does': 190,\n",
       "  'identity': 191,\n",
       "  'saturday': 192,\n",
       "  'set': 193,\n",
       "  'international': 194,\n",
       "  'know': 195,\n",
       "  'should': 196,\n",
       "  'washington': 197,\n",
       "  'foreign': 198,\n",
       "  'administration': 199,\n",
       "  'being': 200,\n",
       "  'killed': 201,\n",
       "  'sunday': 202,\n",
       "  'gsa': 203,\n",
       "  \"cnn's\": 204,\n",
       "  'campaign': 205,\n",
       "  'district': 206,\n",
       "  'top': 207,\n",
       "  'came': 208,\n",
       "  'left': 209,\n",
       "  'groups': 210,\n",
       "  'friday': 211,\n",
       "  'place': 212,\n",
       "  'four': 213,\n",
       "  'through': 214,\n",
       "  'fire': 215,\n",
       "  'arizona': 216,\n",
       "  'much': 217,\n",
       "  'health': 218,\n",
       "  'cheney': 219,\n",
       "  'oil': 220,\n",
       "  'zimmerman': 221,\n",
       "  'force': 222,\n",
       "  'big': 223,\n",
       "  'security': 224,\n",
       "  'must': 225,\n",
       "  'others': 226,\n",
       "  'made': 227,\n",
       "  'members': 228,\n",
       "  'issued': 229,\n",
       "  'under': 230,\n",
       "  'help': 231,\n",
       "  'during': 232,\n",
       "  'show': 233,\n",
       "  'judge': 234,\n",
       "  'life': 235,\n",
       "  'race': 236,\n",
       "  'future': 237,\n",
       "  'around': 238,\n",
       "  'leader': 239,\n",
       "  'war': 240,\n",
       "  'own': 241,\n",
       "  'someone': 242,\n",
       "  'based': 243,\n",
       "  'number': 244,\n",
       "  'death': 245,\n",
       "  'home': 246,\n",
       "  \"that's\": 247,\n",
       "  'working': 248,\n",
       "  'nearly': 249,\n",
       "  '3': 250,\n",
       "  'money': 251,\n",
       "  'burkhart': 252,\n",
       "  'west': 253,\n",
       "  'attorney': 254,\n",
       "  '2': 255,\n",
       "  'school': 256,\n",
       "  '10': 257,\n",
       "  'far': 258,\n",
       "  '4': 259,\n",
       "  'died': 260,\n",
       "  'kasem': 261,\n",
       "  'de': 262,\n",
       "  'use': 263,\n",
       "  'senate': 264,\n",
       "  'body': 265,\n",
       "  \"obama's\": 266,\n",
       "  'used': 267,\n",
       "  'here': 268,\n",
       "  'next': 269,\n",
       "  'vote': 270,\n",
       "  'held': 271,\n",
       "  \"doesn't\": 272,\n",
       "  'second': 273,\n",
       "  'program': 274,\n",
       "  'going': 275,\n",
       "  'months': 276,\n",
       "  'car': 277,\n",
       "  'taken': 278,\n",
       "  'legal': 279,\n",
       "  'middle': 280,\n",
       "  'something': 281,\n",
       "  'ces': 282,\n",
       "  'political': 283,\n",
       "  'reported': 284,\n",
       "  'able': 285,\n",
       "  'become': 286,\n",
       "  'took': 287,\n",
       "  'both': 288,\n",
       "  'put': 289,\n",
       "  'long': 290,\n",
       "  'got': 291,\n",
       "  '2011': 292,\n",
       "  'area': 293,\n",
       "  'california': 294,\n",
       "  'fires': 295,\n",
       "  'head': 296,\n",
       "  'european': 297,\n",
       "  'cardinals': 298,\n",
       "  '17': 299,\n",
       "  'cases': 300,\n",
       "  'fraudulent': 301,\n",
       "  'george': 302,\n",
       "  'congress': 303,\n",
       "  'prevent': 304,\n",
       "  'too': 305,\n",
       "  'leaders': 306,\n",
       "  'general': 307,\n",
       "  'until': 308,\n",
       "  'information': 309,\n",
       "  'response': 310,\n",
       "  'post': 311,\n",
       "  'among': 312,\n",
       "  'countries': 313,\n",
       "  'issue': 314,\n",
       "  \"don't\": 315,\n",
       "  'saying': 316,\n",
       "  'though': 317,\n",
       "  'office': 318,\n",
       "  'family': 319,\n",
       "  'authorities': 320,\n",
       "  'local': 321,\n",
       "  'men': 322,\n",
       "  'great': 323,\n",
       "  'across': 324,\n",
       "  'service': 325,\n",
       "  'bush': 326,\n",
       "  'theft': 327,\n",
       "  'space': 328,\n",
       "  \"there's\": 329,\n",
       "  'final': 330,\n",
       "  'senior': 331,\n",
       "  'community': 332,\n",
       "  'republican': 333,\n",
       "  'anti': 334,\n",
       "  'come': 335,\n",
       "  'done': 336,\n",
       "  'possible': 337,\n",
       "  'calls': 338,\n",
       "  'travel': 339,\n",
       "  'different': 340,\n",
       "  'released': 341,\n",
       "  '20': 342,\n",
       "  'contributed': 343,\n",
       "  'charges': 344,\n",
       "  'person': 345,\n",
       "  'election': 346,\n",
       "  'question': 347,\n",
       "  'stop': 348,\n",
       "  'end': 349,\n",
       "  'your': 350,\n",
       "  'making': 351,\n",
       "  'again': 352,\n",
       "  'east': 353,\n",
       "  'marriage': 354,\n",
       "  'same': 355,\n",
       "  'filin': 356,\n",
       "  'grand': 357,\n",
       "  'garcia': 358,\n",
       "  'middel': 359,\n",
       "  'earlier': 360,\n",
       "  'full': 361,\n",
       "  'spokesman': 362,\n",
       "  'british': 363,\n",
       "  'problem': 364,\n",
       "  'expressed': 365,\n",
       "  \"syria's\": 366,\n",
       "  'decision': 367,\n",
       "  'act': 368,\n",
       "  '5': 369,\n",
       "  'russian': 370,\n",
       "  \"we're\": 371,\n",
       "  'los': 372,\n",
       "  'angeles': 373,\n",
       "  'nations': 374,\n",
       "  'small': 375,\n",
       "  'never': 376,\n",
       "  'medal': 377,\n",
       "  'investigation': 378,\n",
       "  'cost': 379,\n",
       "  'several': 380,\n",
       "  'agency': 381,\n",
       "  'mental': 382,\n",
       "  'asked': 383,\n",
       "  'county': 384,\n",
       "  'fraud': 385,\n",
       "  'began': 386,\n",
       "  'cars': 387,\n",
       "  'high': 388,\n",
       "  'violence': 389,\n",
       "  'immigration': 390,\n",
       "  'anyone': 391,\n",
       "  'director': 392,\n",
       "  'citizens': 393,\n",
       "  'policy': 394,\n",
       "  'democratic': 395,\n",
       "  'york': 396,\n",
       "  'toward': 397,\n",
       "  'victims': 398,\n",
       "  'every': 399,\n",
       "  'really': 400,\n",
       "  'each': 401,\n",
       "  'gonzalez': 402,\n",
       "  'students': 403,\n",
       "  'night': 404,\n",
       "  'chemical': 405,\n",
       "  'key': 406,\n",
       "  'questions': 407,\n",
       "  'believe': 408,\n",
       "  'issues': 409,\n",
       "  'hold': 410,\n",
       "  'hearing': 411,\n",
       "  'tuesday': 412,\n",
       "  'read': 413,\n",
       "  'behind': 414,\n",
       "  'situation': 415,\n",
       "  'phone': 416,\n",
       "  'minister': 417,\n",
       "  'russia': 418,\n",
       "  'us': 419,\n",
       "  'americans': 420,\n",
       "  'tv': 421,\n",
       "  'ambassador': 422,\n",
       "  'continue': 423,\n",
       "  'five': 424,\n",
       "  'brought': 425,\n",
       "  'kansas': 426,\n",
       "  'need': 427,\n",
       "  'interview': 428,\n",
       "  'german': 429,\n",
       "  'defendant': 430,\n",
       "  'late': 431,\n",
       "  'however': 432,\n",
       "  '15': 433,\n",
       "  'town': 434,\n",
       "  'difficult': 435,\n",
       "  'center': 436,\n",
       "  'america': 437,\n",
       "  'cannot': 438,\n",
       "  'mexico': 439,\n",
       "  'season': 440,\n",
       "  'public': 441,\n",
       "  'running': 442,\n",
       "  'within': 443,\n",
       "  'technology': 444,\n",
       "  'university': 445,\n",
       "  'away': 446,\n",
       "  'afghanistan': 447,\n",
       "  'leopard': 448,\n",
       "  'honor': 449,\n",
       "  'few': 450,\n",
       "  'brand': 451,\n",
       "  \"country's\": 452,\n",
       "  'supreme': 453,\n",
       "  'win': 454,\n",
       "  'statute': 455,\n",
       "  'ivanovic': 456,\n",
       "  'wants': 457,\n",
       "  'global': 458,\n",
       "  'organization': 459,\n",
       "  'open': 460,\n",
       "  'french': 461,\n",
       "  'opposition': 462,\n",
       "  'least': 463,\n",
       "  'why': 464,\n",
       "  'china': 465,\n",
       "  'doing': 466,\n",
       "  'taking': 467,\n",
       "  'children': 468,\n",
       "  'third': 469,\n",
       "  '26': 470,\n",
       "  'illegal': 471,\n",
       "  'later': 472,\n",
       "  'past': 473,\n",
       "  'region': 474,\n",
       "  'times': 475,\n",
       "  'hiring': 476,\n",
       "  'better': 477,\n",
       "  'd': 478,\n",
       "  '2010': 479,\n",
       "  'arson': 480,\n",
       "  'claims': 481,\n",
       "  'seen': 482,\n",
       "  'wednesday': 483,\n",
       "  'likely': 484,\n",
       "  'dead': 485,\n",
       "  'humanitarian': 486,\n",
       "  'challenge': 487,\n",
       "  'defense': 488,\n",
       "  'news': 489,\n",
       "  'result': 490,\n",
       "  'recent': 491,\n",
       "  'little': 492,\n",
       "  '8': 493,\n",
       "  'florida': 494,\n",
       "  'risk': 495,\n",
       "  'fight': 496,\n",
       "  '50': 497,\n",
       "  'decided': 498,\n",
       "  'nation': 499,\n",
       "  'change': 500,\n",
       "  'disease': 501,\n",
       "  'airlines': 502,\n",
       "  'list': 503,\n",
       "  'justice': 504,\n",
       "  'detainees': 505,\n",
       "  'special': 506,\n",
       "  'egypt': 507,\n",
       "  'gone': 508,\n",
       "  'army': 509,\n",
       "  'face': 510,\n",
       "  'system': 511,\n",
       "  'air': 512,\n",
       "  'refund': 513,\n",
       "  'windsor': 514,\n",
       "  'believes': 515,\n",
       "  'step': 516,\n",
       "  'debate': 517,\n",
       "  'committee': 518,\n",
       "  'latest': 519,\n",
       "  'reports': 520,\n",
       "  'secretary': 521,\n",
       "  'forces': 522,\n",
       "  'conference': 523,\n",
       "  'democrats': 524,\n",
       "  'protesters': 525,\n",
       "  'appeared': 526,\n",
       "  'italy': 527,\n",
       "  'off': 528,\n",
       "  'champions': 529,\n",
       "  'woman': 530,\n",
       "  'major': 531,\n",
       "  'already': 532,\n",
       "  'employee': 533,\n",
       "  '24': 534,\n",
       "  'employees': 535,\n",
       "  'value': 536,\n",
       "  'give': 537,\n",
       "  'move': 538,\n",
       "  'days': 539,\n",
       "  '12': 540,\n",
       "  'billion': 541,\n",
       "  'following': 542,\n",
       "  'happened': 543,\n",
       "  'monday': 544,\n",
       "  'allegedly': 545,\n",
       "  'hospital': 546,\n",
       "  'james': 547,\n",
       "  'added': 548,\n",
       "  'efforts': 549,\n",
       "  'defeat': 550,\n",
       "  'expect': 551,\n",
       "  'known': 552,\n",
       "  'close': 553,\n",
       "  'process': 554,\n",
       "  'makes': 555,\n",
       "  'order': 556,\n",
       "  'economic': 557,\n",
       "  'start': 558,\n",
       "  'party': 559,\n",
       "  'run': 560,\n",
       "  'might': 561,\n",
       "  'expected': 562,\n",
       "  'radio': 563,\n",
       "  'blood': 564,\n",
       "  \"alzheimer's\": 565,\n",
       "  'plane': 566,\n",
       "  'announced': 567,\n",
       "  'keep': 568,\n",
       "  'despite': 569,\n",
       "  'europe': 570,\n",
       "  \"didn't\": 571,\n",
       "  'initial': 572,\n",
       "  'numbers': 573,\n",
       "  'trying': 574,\n",
       "  'acid': 575,\n",
       "  'dha': 576,\n",
       "  '2008': 577,\n",
       "  'corey': 578,\n",
       "  'education': 579,\n",
       "  'romney': 580,\n",
       "  'barack': 581,\n",
       "  'hours': 582,\n",
       "  'september': 583,\n",
       "  '9': 584,\n",
       "  'matter': 585,\n",
       "  'fighting': 586,\n",
       "  'assad': 587,\n",
       "  'ground': 588,\n",
       "  'month': 589,\n",
       "  'harm': 590,\n",
       "  'message': 591,\n",
       "  'attacks': 592,\n",
       "  'quite': 593,\n",
       "  'john': 594,\n",
       "  'failed': 595,\n",
       "  'understand': 596,\n",
       "  'criticized': 597,\n",
       "  'stand': 598,\n",
       "  'idea': 599,\n",
       "  'bolt': 600,\n",
       "  'charged': 601,\n",
       "  'eight': 602,\n",
       "  'record': 603,\n",
       "  'hand': 604,\n",
       "  'championship': 605,\n",
       "  'further': 606,\n",
       "  'allowed': 607,\n",
       "  'live': 608,\n",
       "  'started': 609,\n",
       "  'takes': 610,\n",
       "  'private': 611,\n",
       "  'spent': 612,\n",
       "  'taxpayer': 613,\n",
       "  'real': 614,\n",
       "  'practices': 615,\n",
       "  'role': 616,\n",
       "  'hollywood': 617,\n",
       "  'prosecutors': 618,\n",
       "  'near': 619,\n",
       "  'arrested': 620,\n",
       "  'rape': 621,\n",
       "  'northern': 622,\n",
       "  'ivory': 623,\n",
       "  'peacekeepers': 624,\n",
       "  'central': 625,\n",
       "  'presidential': 626,\n",
       "  '30': 627,\n",
       "  'civil': 628,\n",
       "  'immigrants': 629,\n",
       "  'try': 630,\n",
       "  'member': 631,\n",
       "  'deal': 632,\n",
       "  'policies': 633,\n",
       "  'always': 634,\n",
       "  'performance': 635,\n",
       "  'father': 636,\n",
       "  'social': 637,\n",
       "  'helping': 638,\n",
       "  'story': 639,\n",
       "  'host': 640,\n",
       "  'rate': 641,\n",
       "  'per': 642,\n",
       "  '25': 643,\n",
       "  'francis': 644,\n",
       "  'africa': 645,\n",
       "  'uyghur': 646,\n",
       "  'pro': 647,\n",
       "  'focus': 648,\n",
       "  'uyghurs': 649,\n",
       "  'spain': 650,\n",
       "  'chief': 651,\n",
       "  'killing': 652,\n",
       "  'alone': 653,\n",
       "  'north': 654,\n",
       "  'electronics': 655,\n",
       "  'recently': 656,\n",
       "  'internet': 657,\n",
       "  'want': 658,\n",
       "  'league': 659,\n",
       "  'football': 660,\n",
       "  'title': 661,\n",
       "  \"administration's\": 662,\n",
       "  'led': 663,\n",
       "  'things': 664,\n",
       "  'ordered': 665,\n",
       "  'large': 666,\n",
       "  'income': 667,\n",
       "  'return': 668,\n",
       "  'fish': 669,\n",
       "  'bolshoi': 670,\n",
       "  'century': 671,\n",
       "  'dream': 672,\n",
       "  'square': 673,\n",
       "  'moschetto': 674,\n",
       "  'ryan': 675,\n",
       "  'alleged': 676,\n",
       "  'crisis': 677,\n",
       "  'fierce': 678,\n",
       "  'battle': 679,\n",
       "  'agreed': 680,\n",
       "  'evidence': 681,\n",
       "  'rebels': 682,\n",
       "  'involved': 683,\n",
       "  'steps': 684,\n",
       "  'pressure': 685,\n",
       "  'include': 686,\n",
       "  'lead': 687,\n",
       "  'meanwhile': 688,\n",
       "  'constitution': 689,\n",
       "  'calling': 690,\n",
       "  'either': 691,\n",
       "  'whose': 692,\n",
       "  'hundreds': 693,\n",
       "  'status': 694,\n",
       "  'gold': 695,\n",
       "  'moscow': 696,\n",
       "  'victory': 697,\n",
       "  'won': 698,\n",
       "  'six': 699,\n",
       "  '100': 700,\n",
       "  \"i'm\": 701,\n",
       "  'problems': 702,\n",
       "  'control': 703,\n",
       "  'women': 704,\n",
       "  'france': 705,\n",
       "  'initially': 706,\n",
       "  'fourth': 707,\n",
       "  'previous': 708,\n",
       "  'laws': 709,\n",
       "  'criminal': 710,\n",
       "  'event': 711,\n",
       "  'millions': 712,\n",
       "  'job': 713,\n",
       "  'side': 714,\n",
       "  'successful': 715,\n",
       "  'defend': 716,\n",
       "  'everything': 717,\n",
       "  'look': 718,\n",
       "  'lot': 719,\n",
       "  'appointed': 720,\n",
       "  'important': 721,\n",
       "  'sure': 722,\n",
       "  'companies': 723,\n",
       "  'company': 724,\n",
       "  'october': 725,\n",
       "  'responsible': 726,\n",
       "  'ago': 727,\n",
       "  'staff': 728,\n",
       "  'western': 729,\n",
       "  'july': 730,\n",
       "  'doctor': 731,\n",
       "  'dr': 732,\n",
       "  'wrote': 733,\n",
       "  'deputy': 734,\n",
       "  'germany': 735,\n",
       "  'worst': 736,\n",
       "  'caused': 737,\n",
       "  'custody': 738,\n",
       "  'pay': 739,\n",
       "  'deals': 740,\n",
       "  'accused': 741,\n",
       "  'victim': 742,\n",
       "  'critical': 743,\n",
       "  '14': 744,\n",
       "  'age': 745,\n",
       "  'outside': 746,\n",
       "  'coast': 747,\n",
       "  'village': 748,\n",
       "  'african': 749,\n",
       "  'border': 750,\n",
       "  'movement': 751,\n",
       "  'heard': 752,\n",
       "  'april': 753,\n",
       "  '11': 754,\n",
       "  'alien': 755,\n",
       "  'present': 756,\n",
       "  'suffering': 757,\n",
       "  'allow': 758,\n",
       "  'chance': 759,\n",
       "  \"we've\": 760,\n",
       "  'tortured': 761,\n",
       "  'piece': 762,\n",
       "  'voters': 763,\n",
       "  'beginning': 764,\n",
       "  'popular': 765,\n",
       "  'voice': 766,\n",
       "  '6': 767,\n",
       "  'stopped': 768,\n",
       "  'republicans': 769,\n",
       "  'conservative': 770,\n",
       "  'name': 771,\n",
       "  'journalist': 772,\n",
       "  'ministry': 773,\n",
       "  'arrigoni': 774,\n",
       "  'black': 775,\n",
       "  'co': 776,\n",
       "  'saw': 777,\n",
       "  '2012': 778,\n",
       "  'needed': 779,\n",
       "  'study': 780,\n",
       "  'consumer': 781,\n",
       "  'february': 782,\n",
       "  'rather': 783,\n",
       "  'bermuda': 784,\n",
       "  'islamic': 785,\n",
       "  'relationship': 786,\n",
       "  'received': 787,\n",
       "  'teams': 788,\n",
       "  'single': 789,\n",
       "  'eating': 790,\n",
       "  'animals': 791,\n",
       "  'lost': 792,\n",
       "  'gave': 793,\n",
       "  \"he's\": 794,\n",
       "  'trained': 795,\n",
       "  'good': 796,\n",
       "  'hard': 797,\n",
       "  'tablets': 798,\n",
       "  'microsoft': 799,\n",
       "  'whole': 800,\n",
       "  'plans': 801,\n",
       "  'clubs': 802,\n",
       "  'england': 803,\n",
       "  'quickly': 804,\n",
       "  'libyan': 805,\n",
       "  'believed': 806,\n",
       "  'suspected': 807,\n",
       "  'decide': 808,\n",
       "  'resources': 809,\n",
       "  'shark': 810,\n",
       "  'dreams': 811,\n",
       "  'foley': 812,\n",
       "  'seems': 813,\n",
       "  'com': 814,\n",
       "  'ballet': 815,\n",
       "  'career': 816,\n",
       "  \"she's\": 817,\n",
       "  'virus': 818,\n",
       "  'ukraine': 819,\n",
       "  \"zimmerman's\": 820,\n",
       "  'harvard': 821,\n",
       "  'festival': 822,\n",
       "  'slam': 823,\n",
       "  'egyptian': 824,\n",
       "  'embassy': 825,\n",
       "  'nkoloso': 826,\n",
       "  'turn': 827,\n",
       "  'inspectors': 828,\n",
       "  'find': 829,\n",
       "  'carry': 830,\n",
       "  'actions': 831,\n",
       "  'business': 832,\n",
       "  'sen': 833,\n",
       "  'early': 834,\n",
       "  'whom': 835,\n",
       "  '21': 836,\n",
       "  'point': 837,\n",
       "  'denied': 838,\n",
       "  'waiting': 839,\n",
       "  'nine': 840,\n",
       "  'needs': 841,\n",
       "  'speech': 842,\n",
       "  'regime': 843,\n",
       "  'ready': 844,\n",
       "  'fellow': 845,\n",
       "  'reason': 846,\n",
       "  'itself': 847,\n",
       "  'affairs': 848,\n",
       "  'iran': 849,\n",
       "  'analyst': 850,\n",
       "  'israel': 851,\n",
       "  'tree': 852,\n",
       "  'strikes': 853,\n",
       "  'seconds': 854,\n",
       "  'britain': 855,\n",
       "  'having': 856,\n",
       "  'winning': 857,\n",
       "  'english': 858,\n",
       "  \"gsa's\": 859,\n",
       "  'virtual': 860,\n",
       "  'workers': 861,\n",
       "  \"they're\": 862,\n",
       "  'actually': 863,\n",
       "  'january': 864,\n",
       "  'st': 865,\n",
       "  'taxpayers': 866,\n",
       "  'regional': 867,\n",
       "  'candidate': 868,\n",
       "  'spokeswoman': 869,\n",
       "  'requested': 870,\n",
       "  'me': 871,\n",
       "  'reform': 872,\n",
       "  'details': 873,\n",
       "  'june': 874,\n",
       "  'budget': 875,\n",
       "  'amount': 876,\n",
       "  'free': 877,\n",
       "  'm': 878,\n",
       "  'medical': 879,\n",
       "  'stancheva': 880,\n",
       "  'mother': 881,\n",
       "  'arrest': 882,\n",
       "  'terror': 883,\n",
       "  'kept': 884,\n",
       "  'jail': 885,\n",
       "  \"city's\": 886,\n",
       "  'history': 887,\n",
       "  'reach': 888,\n",
       "  'almost': 889,\n",
       "  'condition': 890,\n",
       "  'enter': 891,\n",
       "  '19': 892,\n",
       "  'thousands': 893,\n",
       "  'seven': 894,\n",
       "  'liberia': 895,\n",
       "  '40': 896,\n",
       "  'dialogue': 897,\n",
       "  'incident': 898,\n",
       "  \"can't\": 899,\n",
       "  'immigrant': 900,\n",
       "  'mexican': 901,\n",
       "  'union': 902,\n",
       "  'ever': 903,\n",
       "  'executive': 904,\n",
       "  'requires': 905,\n",
       "  'street': 906,\n",
       "  'bill': 907,\n",
       "  'threat': 908,\n",
       "  'officers': 909,\n",
       "  'minority': 910,\n",
       "  'anything': 911,\n",
       "  'especially': 912,\n",
       "  'care': 913,\n",
       "  'concerns': 914,\n",
       "  'vice': 915,\n",
       "  'walker': 916,\n",
       "  'landrieu': 917,\n",
       "  'december': 918,\n",
       "  'hit': 919,\n",
       "  'facility': 920,\n",
       "  'deraney': 921,\n",
       "  'visit': 922,\n",
       "  'let': 923,\n",
       "  'coming': 924,\n",
       "  '13': 925,\n",
       "  'church': 926,\n",
       "  'sometimes': 927,\n",
       "  'power': 928,\n",
       "  'chinese': 929,\n",
       "  'guantanamo': 930,\n",
       "  'attorneys': 931,\n",
       "  'offer': 932,\n",
       "  'london': 933,\n",
       "  '2001': 934,\n",
       "  'palau': 935,\n",
       "  'average': 936,\n",
       "  'nepal': 937,\n",
       "  'practice': 938,\n",
       "  'permission': 939,\n",
       "  \"america's\": 940,\n",
       "  'today': 941,\n",
       "  'dressed': 942,\n",
       "  'less': 943,\n",
       "  'looking': 944,\n",
       "  \"couldn't\": 945,\n",
       "  'images': 946,\n",
       "  'thought': 947,\n",
       "  \"he'd\": 948,\n",
       "  'makers': 949,\n",
       "  'players': 950,\n",
       "  'software': 951,\n",
       "  'tablet': 952,\n",
       "  'push': 953,\n",
       "  'independent': 954,\n",
       "  'bayern': 955,\n",
       "  \"football's\": 956,\n",
       "  'brands': 957,\n",
       "  'chattaway': 958,\n",
       "  'slow': 959,\n",
       "  'terrorism': 960,\n",
       "  'essential': 961,\n",
       "  'rules': 962,\n",
       "  'examination': 963,\n",
       "  'client': 964,\n",
       "  'series': 965,\n",
       "  'cdc': 966,\n",
       "  'cantaloupes': 967,\n",
       "  'yet': 968,\n",
       "  'identities': 969,\n",
       "  'although': 970,\n",
       "  'beat': 971,\n",
       "  'young': 972,\n",
       "  'ruling': 973,\n",
       "  'drama': 974,\n",
       "  'played': 975,\n",
       "  'lawyer': 976,\n",
       "  'doma': 977,\n",
       "  'tells': 978,\n",
       "  'front': 979,\n",
       "  'track': 980,\n",
       "  'uefa': 981,\n",
       "  'half': 982,\n",
       "  'dee': 983,\n",
       "  'horse': 984,\n",
       "  'greek': 985,\n",
       "  'hazzard': 986,\n",
       "  'muslims': 987,\n",
       "  'youth': 988,\n",
       "  \"egypt's\": 989,\n",
       "  'protest': 990,\n",
       "  'swedish': 991,\n",
       "  'protests': 992,\n",
       "  'base': 993,\n",
       "  'tort': 994,\n",
       "  'hong': 995,\n",
       "  'lawmakers': 996,\n",
       "  'sent': 997,\n",
       "  'targets': 998,\n",
       "  'legislation': 999,\n",
       "  'potential': 1000,\n",
       "  'congressional': 1001,\n",
       "  'leave': 1002,\n",
       "  'carrying': 1003,\n",
       "  ...})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TOKENIZER.word_index), TOKENIZER.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {v: k for k, v in TOKENIZER.word_index.items()}\n",
    "index_to_word[0] = '<pad>'\n",
    "index_to_word[1] = '<begin>'\n",
    "index_to_word[2] = '<end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '<pad>'),\n",
       " (1, '<begin>'),\n",
       " (2, '<end>'),\n",
       " (3, '<unk>'),\n",
       " (4, 'the'),\n",
       " (5, 'to'),\n",
       " (6, 'of'),\n",
       " (7, 'a'),\n",
       " (8, 'in'),\n",
       " (9, 'and'),\n",
       " (10, 'that'),\n",
       " (11, 'is'),\n",
       " (12, 'for'),\n",
       " (13, 'on'),\n",
       " (14, 'said'),\n",
       " (15, 'it'),\n",
       " (16, 'with'),\n",
       " (17, 'was'),\n",
       " (18, 'he'),\n",
       " (19, 'as'),\n",
       " (20, 'has'),\n",
       " (21, 'be'),\n",
       " (22, 'his'),\n",
       " (23, 'have'),\n",
       " (24, 'from'),\n",
       " (25, 'by'),\n",
       " (26, 'at'),\n",
       " (27, 'this'),\n",
       " (28, 'are'),\n",
       " (29, 'not'),\n",
       " (30, 'but'),\n",
       " (31, 'who'),\n",
       " (32, 'an'),\n",
       " (33, 'they'),\n",
       " (34, 'will'),\n",
       " (35, 'u'),\n",
       " (36, 'their'),\n",
       " (37, 'more'),\n",
       " (38, 'been'),\n",
       " (39, 'or'),\n",
       " (40, 'i'),\n",
       " (41, 's'),\n",
       " (42, 'we'),\n",
       " (43, 'year'),\n",
       " (44, 'had'),\n",
       " (45, 'were'),\n",
       " (46, 'she'),\n",
       " (47, 'people'),\n",
       " (48, 'than'),\n",
       " (49, 'cnn'),\n",
       " (50, 'also'),\n",
       " (51, 'one'),\n",
       " (52, 'about'),\n",
       " (53, 'all'),\n",
       " (54, 'which'),\n",
       " (55, 'her'),\n",
       " (56, 'united'),\n",
       " (57, 'new'),\n",
       " (58, 'after'),\n",
       " (59, 'states'),\n",
       " (60, 'government'),\n",
       " (61, 'if'),\n",
       " (62, 'state'),\n",
       " (63, 'other'),\n",
       " (64, 'can'),\n",
       " (65, 'what'),\n",
       " (66, 'isis'),\n",
       " (67, 'when'),\n",
       " (68, 'obama'),\n",
       " (69, 'there'),\n",
       " (70, 'would'),\n",
       " (71, 'no'),\n",
       " (72, 'its'),\n",
       " (73, 'so'),\n",
       " (74, 'into'),\n",
       " (75, 'court'),\n",
       " (76, \"it's\"),\n",
       " (77, 'two'),\n",
       " (78, 'him'),\n",
       " (79, 'last'),\n",
       " (80, 'syria'),\n",
       " (81, 'according'),\n",
       " (82, 'says'),\n",
       " (83, 'against'),\n",
       " (84, 'white'),\n",
       " (85, 'up'),\n",
       " (86, 'law'),\n",
       " (87, 'over'),\n",
       " (88, 'syrian'),\n",
       " (89, 'could'),\n",
       " (90, 'out'),\n",
       " (91, 'told'),\n",
       " (92, 'time'),\n",
       " (93, 'our'),\n",
       " (94, 'years'),\n",
       " (95, 'military'),\n",
       " (96, 'president'),\n",
       " (97, 'because'),\n",
       " (98, 'you'),\n",
       " (99, 'most'),\n",
       " (100, 'now'),\n",
       " (101, 'police'),\n",
       " (102, 'some'),\n",
       " (103, 'any'),\n",
       " (104, 'before'),\n",
       " (105, 'those'),\n",
       " (106, 'like'),\n",
       " (107, 'rights'),\n",
       " (108, 'do'),\n",
       " (109, 'first'),\n",
       " (110, 'day'),\n",
       " (111, 'may'),\n",
       " (112, 'old'),\n",
       " (113, 'where'),\n",
       " (114, 'take'),\n",
       " (115, 'case'),\n",
       " (116, 'while'),\n",
       " (117, 'even'),\n",
       " (118, 'world'),\n",
       " (119, 'many'),\n",
       " (120, 'them'),\n",
       " (121, 'get'),\n",
       " (122, '000'),\n",
       " (123, 'country'),\n",
       " (124, 'attack'),\n",
       " (125, 'including'),\n",
       " (126, 'just'),\n",
       " (127, '1'),\n",
       " (128, 'american'),\n",
       " (129, 'city'),\n",
       " (130, 'federal'),\n",
       " (131, 'million'),\n",
       " (132, 'group'),\n",
       " (133, 'back'),\n",
       " (134, 'another'),\n",
       " (135, 'department'),\n",
       " (136, 'iraq'),\n",
       " (137, 'week'),\n",
       " (138, 'statement'),\n",
       " (139, 'down'),\n",
       " (140, 'between'),\n",
       " (141, 'still'),\n",
       " (142, 'my'),\n",
       " (143, 'only'),\n",
       " (144, 'official'),\n",
       " (145, 'very'),\n",
       " (146, 'report'),\n",
       " (147, 'team'),\n",
       " (148, 'part'),\n",
       " (149, 'since'),\n",
       " (150, 'whether'),\n",
       " (151, 'did'),\n",
       " (152, 'called'),\n",
       " (153, 'these'),\n",
       " (154, 'way'),\n",
       " (155, 'make'),\n",
       " (156, 'found'),\n",
       " (157, 'then'),\n",
       " (158, 'thursday'),\n",
       " (159, 'house'),\n",
       " (160, 'how'),\n",
       " (161, 'al'),\n",
       " (162, 'human'),\n",
       " (163, 'former'),\n",
       " (164, 'ebola'),\n",
       " (165, 'action'),\n",
       " (166, 'n'),\n",
       " (167, 'clear'),\n",
       " (168, 'say'),\n",
       " (169, 'think'),\n",
       " (170, 'irs'),\n",
       " (171, 'right'),\n",
       " (172, 'man'),\n",
       " (173, 'video'),\n",
       " (174, 'see'),\n",
       " (175, 'tax'),\n",
       " (176, 'such'),\n",
       " (177, 'weapons'),\n",
       " (178, 'without'),\n",
       " (179, 'returns'),\n",
       " (180, 'martin'),\n",
       " (181, 'officials'),\n",
       " (182, 'three'),\n",
       " (183, 'support'),\n",
       " (184, 'national'),\n",
       " (185, 'meeting'),\n",
       " (186, 'well'),\n",
       " (187, 'work'),\n",
       " (188, 'best'),\n",
       " (189, 'go'),\n",
       " (190, 'does'),\n",
       " (191, 'identity'),\n",
       " (192, 'saturday'),\n",
       " (193, 'set'),\n",
       " (194, 'international'),\n",
       " (195, 'know'),\n",
       " (196, 'should'),\n",
       " (197, 'washington'),\n",
       " (198, 'foreign'),\n",
       " (199, 'administration'),\n",
       " (200, 'being'),\n",
       " (201, 'killed'),\n",
       " (202, 'sunday'),\n",
       " (203, 'gsa'),\n",
       " (204, \"cnn's\"),\n",
       " (205, 'campaign'),\n",
       " (206, 'district'),\n",
       " (207, 'top'),\n",
       " (208, 'came'),\n",
       " (209, 'left'),\n",
       " (210, 'groups'),\n",
       " (211, 'friday'),\n",
       " (212, 'place'),\n",
       " (213, 'four'),\n",
       " (214, 'through'),\n",
       " (215, 'fire'),\n",
       " (216, 'arizona'),\n",
       " (217, 'much'),\n",
       " (218, 'health'),\n",
       " (219, 'cheney'),\n",
       " (220, 'oil'),\n",
       " (221, 'zimmerman'),\n",
       " (222, 'force'),\n",
       " (223, 'big'),\n",
       " (224, 'security'),\n",
       " (225, 'must'),\n",
       " (226, 'others'),\n",
       " (227, 'made'),\n",
       " (228, 'members'),\n",
       " (229, 'issued'),\n",
       " (230, 'under'),\n",
       " (231, 'help'),\n",
       " (232, 'during'),\n",
       " (233, 'show'),\n",
       " (234, 'judge'),\n",
       " (235, 'life'),\n",
       " (236, 'race'),\n",
       " (237, 'future'),\n",
       " (238, 'around'),\n",
       " (239, 'leader'),\n",
       " (240, 'war'),\n",
       " (241, 'own'),\n",
       " (242, 'someone'),\n",
       " (243, 'based'),\n",
       " (244, 'number'),\n",
       " (245, 'death'),\n",
       " (246, 'home'),\n",
       " (247, \"that's\"),\n",
       " (248, 'working'),\n",
       " (249, 'nearly'),\n",
       " (250, '3'),\n",
       " (251, 'money'),\n",
       " (252, 'burkhart'),\n",
       " (253, 'west'),\n",
       " (254, 'attorney'),\n",
       " (255, '2'),\n",
       " (256, 'school'),\n",
       " (257, '10'),\n",
       " (258, 'far'),\n",
       " (259, '4'),\n",
       " (260, 'died'),\n",
       " (261, 'kasem'),\n",
       " (262, 'de'),\n",
       " (263, 'use'),\n",
       " (264, 'senate'),\n",
       " (265, 'body'),\n",
       " (266, \"obama's\"),\n",
       " (267, 'used'),\n",
       " (268, 'here'),\n",
       " (269, 'next'),\n",
       " (270, 'vote'),\n",
       " (271, 'held'),\n",
       " (272, \"doesn't\"),\n",
       " (273, 'second'),\n",
       " (274, 'program'),\n",
       " (275, 'going'),\n",
       " (276, 'months'),\n",
       " (277, 'car'),\n",
       " (278, 'taken'),\n",
       " (279, 'legal'),\n",
       " (280, 'middle'),\n",
       " (281, 'something'),\n",
       " (282, 'ces'),\n",
       " (283, 'political'),\n",
       " (284, 'reported'),\n",
       " (285, 'able'),\n",
       " (286, 'become'),\n",
       " (287, 'took'),\n",
       " (288, 'both'),\n",
       " (289, 'put'),\n",
       " (290, 'long'),\n",
       " (291, 'got'),\n",
       " (292, '2011'),\n",
       " (293, 'area'),\n",
       " (294, 'california'),\n",
       " (295, 'fires'),\n",
       " (296, 'head'),\n",
       " (297, 'european'),\n",
       " (298, 'cardinals'),\n",
       " (299, '17'),\n",
       " (300, 'cases'),\n",
       " (301, 'fraudulent'),\n",
       " (302, 'george'),\n",
       " (303, 'congress'),\n",
       " (304, 'prevent'),\n",
       " (305, 'too'),\n",
       " (306, 'leaders'),\n",
       " (307, 'general'),\n",
       " (308, 'until'),\n",
       " (309, 'information'),\n",
       " (310, 'response'),\n",
       " (311, 'post'),\n",
       " (312, 'among'),\n",
       " (313, 'countries'),\n",
       " (314, 'issue'),\n",
       " (315, \"don't\"),\n",
       " (316, 'saying'),\n",
       " (317, 'though'),\n",
       " (318, 'office'),\n",
       " (319, 'family'),\n",
       " (320, 'authorities'),\n",
       " (321, 'local'),\n",
       " (322, 'men'),\n",
       " (323, 'great'),\n",
       " (324, 'across'),\n",
       " (325, 'service'),\n",
       " (326, 'bush'),\n",
       " (327, 'theft'),\n",
       " (328, 'space'),\n",
       " (329, \"there's\"),\n",
       " (330, 'final'),\n",
       " (331, 'senior'),\n",
       " (332, 'community'),\n",
       " (333, 'republican'),\n",
       " (334, 'anti'),\n",
       " (335, 'come'),\n",
       " (336, 'done'),\n",
       " (337, 'possible'),\n",
       " (338, 'calls'),\n",
       " (339, 'travel'),\n",
       " (340, 'different'),\n",
       " (341, 'released'),\n",
       " (342, '20'),\n",
       " (343, 'contributed'),\n",
       " (344, 'charges'),\n",
       " (345, 'person'),\n",
       " (346, 'election'),\n",
       " (347, 'question'),\n",
       " (348, 'stop'),\n",
       " (349, 'end'),\n",
       " (350, 'your'),\n",
       " (351, 'making'),\n",
       " (352, 'again'),\n",
       " (353, 'east'),\n",
       " (354, 'marriage'),\n",
       " (355, 'same'),\n",
       " (356, 'filin'),\n",
       " (357, 'grand'),\n",
       " (358, 'garcia'),\n",
       " (359, 'middel'),\n",
       " (360, 'earlier'),\n",
       " (361, 'full'),\n",
       " (362, 'spokesman'),\n",
       " (363, 'british'),\n",
       " (364, 'problem'),\n",
       " (365, 'expressed'),\n",
       " (366, \"syria's\"),\n",
       " (367, 'decision'),\n",
       " (368, 'act'),\n",
       " (369, '5'),\n",
       " (370, 'russian'),\n",
       " (371, \"we're\"),\n",
       " (372, 'los'),\n",
       " (373, 'angeles'),\n",
       " (374, 'nations'),\n",
       " (375, 'small'),\n",
       " (376, 'never'),\n",
       " (377, 'medal'),\n",
       " (378, 'investigation'),\n",
       " (379, 'cost'),\n",
       " (380, 'several'),\n",
       " (381, 'agency'),\n",
       " (382, 'mental'),\n",
       " (383, 'asked'),\n",
       " (384, 'county'),\n",
       " (385, 'fraud'),\n",
       " (386, 'began'),\n",
       " (387, 'cars'),\n",
       " (388, 'high'),\n",
       " (389, 'violence'),\n",
       " (390, 'immigration'),\n",
       " (391, 'anyone'),\n",
       " (392, 'director'),\n",
       " (393, 'citizens'),\n",
       " (394, 'policy'),\n",
       " (395, 'democratic'),\n",
       " (396, 'york'),\n",
       " (397, 'toward'),\n",
       " (398, 'victims'),\n",
       " (399, 'every'),\n",
       " (400, 'really'),\n",
       " (401, 'each'),\n",
       " (402, 'gonzalez'),\n",
       " (403, 'students'),\n",
       " (404, 'night'),\n",
       " (405, 'chemical'),\n",
       " (406, 'key'),\n",
       " (407, 'questions'),\n",
       " (408, 'believe'),\n",
       " (409, 'issues'),\n",
       " (410, 'hold'),\n",
       " (411, 'hearing'),\n",
       " (412, 'tuesday'),\n",
       " (413, 'read'),\n",
       " (414, 'behind'),\n",
       " (415, 'situation'),\n",
       " (416, 'phone'),\n",
       " (417, 'minister'),\n",
       " (418, 'russia'),\n",
       " (419, 'us'),\n",
       " (420, 'americans'),\n",
       " (421, 'tv'),\n",
       " (422, 'ambassador'),\n",
       " (423, 'continue'),\n",
       " (424, 'five'),\n",
       " (425, 'brought'),\n",
       " (426, 'kansas'),\n",
       " (427, 'need'),\n",
       " (428, 'interview'),\n",
       " (429, 'german'),\n",
       " (430, 'defendant'),\n",
       " (431, 'late'),\n",
       " (432, 'however'),\n",
       " (433, '15'),\n",
       " (434, 'town'),\n",
       " (435, 'difficult'),\n",
       " (436, 'center'),\n",
       " (437, 'america'),\n",
       " (438, 'cannot'),\n",
       " (439, 'mexico'),\n",
       " (440, 'season'),\n",
       " (441, 'public'),\n",
       " (442, 'running'),\n",
       " (443, 'within'),\n",
       " (444, 'technology'),\n",
       " (445, 'university'),\n",
       " (446, 'away'),\n",
       " (447, 'afghanistan'),\n",
       " (448, 'leopard'),\n",
       " (449, 'honor'),\n",
       " (450, 'few'),\n",
       " (451, 'brand'),\n",
       " (452, \"country's\"),\n",
       " (453, 'supreme'),\n",
       " (454, 'win'),\n",
       " (455, 'statute'),\n",
       " (456, 'ivanovic'),\n",
       " (457, 'wants'),\n",
       " (458, 'global'),\n",
       " (459, 'organization'),\n",
       " (460, 'open'),\n",
       " (461, 'french'),\n",
       " (462, 'opposition'),\n",
       " (463, 'least'),\n",
       " (464, 'why'),\n",
       " (465, 'china'),\n",
       " (466, 'doing'),\n",
       " (467, 'taking'),\n",
       " (468, 'children'),\n",
       " (469, 'third'),\n",
       " (470, '26'),\n",
       " (471, 'illegal'),\n",
       " (472, 'later'),\n",
       " (473, 'past'),\n",
       " (474, 'region'),\n",
       " (475, 'times'),\n",
       " (476, 'hiring'),\n",
       " (477, 'better'),\n",
       " (478, 'd'),\n",
       " (479, '2010'),\n",
       " (480, 'arson'),\n",
       " (481, 'claims'),\n",
       " (482, 'seen'),\n",
       " (483, 'wednesday'),\n",
       " (484, 'likely'),\n",
       " (485, 'dead'),\n",
       " (486, 'humanitarian'),\n",
       " (487, 'challenge'),\n",
       " (488, 'defense'),\n",
       " (489, 'news'),\n",
       " (490, 'result'),\n",
       " (491, 'recent'),\n",
       " (492, 'little'),\n",
       " (493, '8'),\n",
       " (494, 'florida'),\n",
       " (495, 'risk'),\n",
       " (496, 'fight'),\n",
       " (497, '50'),\n",
       " (498, 'decided'),\n",
       " (499, 'nation'),\n",
       " (500, 'change'),\n",
       " (501, 'disease'),\n",
       " (502, 'airlines'),\n",
       " (503, 'list'),\n",
       " (504, 'justice'),\n",
       " (505, 'detainees'),\n",
       " (506, 'special'),\n",
       " (507, 'egypt'),\n",
       " (508, 'gone'),\n",
       " (509, 'army'),\n",
       " (510, 'face'),\n",
       " (511, 'system'),\n",
       " (512, 'air'),\n",
       " (513, 'refund'),\n",
       " (514, 'windsor'),\n",
       " (515, 'believes'),\n",
       " (516, 'step'),\n",
       " (517, 'debate'),\n",
       " (518, 'committee'),\n",
       " (519, 'latest'),\n",
       " (520, 'reports'),\n",
       " (521, 'secretary'),\n",
       " (522, 'forces'),\n",
       " (523, 'conference'),\n",
       " (524, 'democrats'),\n",
       " (525, 'protesters'),\n",
       " (526, 'appeared'),\n",
       " (527, 'italy'),\n",
       " (528, 'off'),\n",
       " (529, 'champions'),\n",
       " (530, 'woman'),\n",
       " (531, 'major'),\n",
       " (532, 'already'),\n",
       " (533, 'employee'),\n",
       " (534, '24'),\n",
       " (535, 'employees'),\n",
       " (536, 'value'),\n",
       " (537, 'give'),\n",
       " (538, 'move'),\n",
       " (539, 'days'),\n",
       " (540, '12'),\n",
       " (541, 'billion'),\n",
       " (542, 'following'),\n",
       " (543, 'happened'),\n",
       " (544, 'monday'),\n",
       " (545, 'allegedly'),\n",
       " (546, 'hospital'),\n",
       " (547, 'james'),\n",
       " (548, 'added'),\n",
       " (549, 'efforts'),\n",
       " (550, 'defeat'),\n",
       " (551, 'expect'),\n",
       " (552, 'known'),\n",
       " (553, 'close'),\n",
       " (554, 'process'),\n",
       " (555, 'makes'),\n",
       " (556, 'order'),\n",
       " (557, 'economic'),\n",
       " (558, 'start'),\n",
       " (559, 'party'),\n",
       " (560, 'run'),\n",
       " (561, 'might'),\n",
       " (562, 'expected'),\n",
       " (563, 'radio'),\n",
       " (564, 'blood'),\n",
       " (565, \"alzheimer's\"),\n",
       " (566, 'plane'),\n",
       " (567, 'announced'),\n",
       " (568, 'keep'),\n",
       " (569, 'despite'),\n",
       " (570, 'europe'),\n",
       " (571, \"didn't\"),\n",
       " (572, 'initial'),\n",
       " (573, 'numbers'),\n",
       " (574, 'trying'),\n",
       " (575, 'acid'),\n",
       " (576, 'dha'),\n",
       " (577, '2008'),\n",
       " (578, 'corey'),\n",
       " (579, 'education'),\n",
       " (580, 'romney'),\n",
       " (581, 'barack'),\n",
       " (582, 'hours'),\n",
       " (583, 'september'),\n",
       " (584, '9'),\n",
       " (585, 'matter'),\n",
       " (586, 'fighting'),\n",
       " (587, 'assad'),\n",
       " (588, 'ground'),\n",
       " (589, 'month'),\n",
       " (590, 'harm'),\n",
       " (591, 'message'),\n",
       " (592, 'attacks'),\n",
       " (593, 'quite'),\n",
       " (594, 'john'),\n",
       " (595, 'failed'),\n",
       " (596, 'understand'),\n",
       " (597, 'criticized'),\n",
       " (598, 'stand'),\n",
       " (599, 'idea'),\n",
       " (600, 'bolt'),\n",
       " (601, 'charged'),\n",
       " (602, 'eight'),\n",
       " (603, 'record'),\n",
       " (604, 'hand'),\n",
       " (605, 'championship'),\n",
       " (606, 'further'),\n",
       " (607, 'allowed'),\n",
       " (608, 'live'),\n",
       " (609, 'started'),\n",
       " (610, 'takes'),\n",
       " (611, 'private'),\n",
       " (612, 'spent'),\n",
       " (613, 'taxpayer'),\n",
       " (614, 'real'),\n",
       " (615, 'practices'),\n",
       " (616, 'role'),\n",
       " (617, 'hollywood'),\n",
       " (618, 'prosecutors'),\n",
       " (619, 'near'),\n",
       " (620, 'arrested'),\n",
       " (621, 'rape'),\n",
       " (622, 'northern'),\n",
       " (623, 'ivory'),\n",
       " (624, 'peacekeepers'),\n",
       " (625, 'central'),\n",
       " (626, 'presidential'),\n",
       " (627, '30'),\n",
       " (628, 'civil'),\n",
       " (629, 'immigrants'),\n",
       " (630, 'try'),\n",
       " (631, 'member'),\n",
       " (632, 'deal'),\n",
       " (633, 'policies'),\n",
       " (634, 'always'),\n",
       " (635, 'performance'),\n",
       " (636, 'father'),\n",
       " (637, 'social'),\n",
       " (638, 'helping'),\n",
       " (639, 'story'),\n",
       " (640, 'host'),\n",
       " (641, 'rate'),\n",
       " (642, 'per'),\n",
       " (643, '25'),\n",
       " (644, 'francis'),\n",
       " (645, 'africa'),\n",
       " (646, 'uyghur'),\n",
       " (647, 'pro'),\n",
       " (648, 'focus'),\n",
       " (649, 'uyghurs'),\n",
       " (650, 'spain'),\n",
       " (651, 'chief'),\n",
       " (652, 'killing'),\n",
       " (653, 'alone'),\n",
       " (654, 'north'),\n",
       " (655, 'electronics'),\n",
       " (656, 'recently'),\n",
       " (657, 'internet'),\n",
       " (658, 'want'),\n",
       " (659, 'league'),\n",
       " (660, 'football'),\n",
       " (661, 'title'),\n",
       " (662, \"administration's\"),\n",
       " (663, 'led'),\n",
       " (664, 'things'),\n",
       " (665, 'ordered'),\n",
       " (666, 'large'),\n",
       " (667, 'income'),\n",
       " (668, 'return'),\n",
       " (669, 'fish'),\n",
       " (670, 'bolshoi'),\n",
       " (671, 'century'),\n",
       " (672, 'dream'),\n",
       " (673, 'square'),\n",
       " (674, 'moschetto'),\n",
       " (675, 'ryan'),\n",
       " (676, 'alleged'),\n",
       " (677, 'crisis'),\n",
       " (678, 'fierce'),\n",
       " (679, 'battle'),\n",
       " (680, 'agreed'),\n",
       " (681, 'evidence'),\n",
       " (682, 'rebels'),\n",
       " (683, 'involved'),\n",
       " (684, 'steps'),\n",
       " (685, 'pressure'),\n",
       " (686, 'include'),\n",
       " (687, 'lead'),\n",
       " (688, 'meanwhile'),\n",
       " (689, 'constitution'),\n",
       " (690, 'calling'),\n",
       " (691, 'either'),\n",
       " (692, 'whose'),\n",
       " (693, 'hundreds'),\n",
       " (694, 'status'),\n",
       " (695, 'gold'),\n",
       " (696, 'moscow'),\n",
       " (697, 'victory'),\n",
       " (698, 'won'),\n",
       " (699, 'six'),\n",
       " (700, '100'),\n",
       " (701, \"i'm\"),\n",
       " (702, 'problems'),\n",
       " (703, 'control'),\n",
       " (704, 'women'),\n",
       " (705, 'france'),\n",
       " (706, 'initially'),\n",
       " (707, 'fourth'),\n",
       " (708, 'previous'),\n",
       " (709, 'laws'),\n",
       " (710, 'criminal'),\n",
       " (711, 'event'),\n",
       " (712, 'millions'),\n",
       " (713, 'job'),\n",
       " (714, 'side'),\n",
       " (715, 'successful'),\n",
       " (716, 'defend'),\n",
       " (717, 'everything'),\n",
       " (718, 'look'),\n",
       " (719, 'lot'),\n",
       " (720, 'appointed'),\n",
       " (721, 'important'),\n",
       " (722, 'sure'),\n",
       " (723, 'companies'),\n",
       " (724, 'company'),\n",
       " (725, 'october'),\n",
       " (726, 'responsible'),\n",
       " (727, 'ago'),\n",
       " (728, 'staff'),\n",
       " (729, 'western'),\n",
       " (730, 'july'),\n",
       " (731, 'doctor'),\n",
       " (732, 'dr'),\n",
       " (733, 'wrote'),\n",
       " (734, 'deputy'),\n",
       " (735, 'germany'),\n",
       " (736, 'worst'),\n",
       " (737, 'caused'),\n",
       " (738, 'custody'),\n",
       " (739, 'pay'),\n",
       " (740, 'deals'),\n",
       " (741, 'accused'),\n",
       " (742, 'victim'),\n",
       " (743, 'critical'),\n",
       " (744, '14'),\n",
       " (745, 'age'),\n",
       " (746, 'outside'),\n",
       " (747, 'coast'),\n",
       " (748, 'village'),\n",
       " (749, 'african'),\n",
       " (750, 'border'),\n",
       " (751, 'movement'),\n",
       " (752, 'heard'),\n",
       " (753, 'april'),\n",
       " (754, '11'),\n",
       " (755, 'alien'),\n",
       " (756, 'present'),\n",
       " (757, 'suffering'),\n",
       " (758, 'allow'),\n",
       " (759, 'chance'),\n",
       " (760, \"we've\"),\n",
       " (761, 'tortured'),\n",
       " (762, 'piece'),\n",
       " (763, 'voters'),\n",
       " (764, 'beginning'),\n",
       " (765, 'popular'),\n",
       " (766, 'voice'),\n",
       " (767, '6'),\n",
       " (768, 'stopped'),\n",
       " (769, 'republicans'),\n",
       " (770, 'conservative'),\n",
       " (771, 'name'),\n",
       " (772, 'journalist'),\n",
       " (773, 'ministry'),\n",
       " (774, 'arrigoni'),\n",
       " (775, 'black'),\n",
       " (776, 'co'),\n",
       " (777, 'saw'),\n",
       " (778, '2012'),\n",
       " (779, 'needed'),\n",
       " (780, 'study'),\n",
       " (781, 'consumer'),\n",
       " (782, 'february'),\n",
       " (783, 'rather'),\n",
       " (784, 'bermuda'),\n",
       " (785, 'islamic'),\n",
       " (786, 'relationship'),\n",
       " (787, 'received'),\n",
       " (788, 'teams'),\n",
       " (789, 'single'),\n",
       " (790, 'eating'),\n",
       " (791, 'animals'),\n",
       " (792, 'lost'),\n",
       " (793, 'gave'),\n",
       " (794, \"he's\"),\n",
       " (795, 'trained'),\n",
       " (796, 'good'),\n",
       " (797, 'hard'),\n",
       " (798, 'tablets'),\n",
       " (799, 'microsoft'),\n",
       " (800, 'whole'),\n",
       " (801, 'plans'),\n",
       " (802, 'clubs'),\n",
       " (803, 'england'),\n",
       " (804, 'quickly'),\n",
       " (805, 'libyan'),\n",
       " (806, 'believed'),\n",
       " (807, 'suspected'),\n",
       " (808, 'decide'),\n",
       " (809, 'resources'),\n",
       " (810, 'shark'),\n",
       " (811, 'dreams'),\n",
       " (812, 'foley'),\n",
       " (813, 'seems'),\n",
       " (814, 'com'),\n",
       " (815, 'ballet'),\n",
       " (816, 'career'),\n",
       " (817, \"she's\"),\n",
       " (818, 'virus'),\n",
       " (819, 'ukraine'),\n",
       " (820, \"zimmerman's\"),\n",
       " (821, 'harvard'),\n",
       " (822, 'festival'),\n",
       " (823, 'slam'),\n",
       " (824, 'egyptian'),\n",
       " (825, 'embassy'),\n",
       " (826, 'nkoloso'),\n",
       " (827, 'turn'),\n",
       " (828, 'inspectors'),\n",
       " (829, 'find'),\n",
       " (830, 'carry'),\n",
       " (831, 'actions'),\n",
       " (832, 'business'),\n",
       " (833, 'sen'),\n",
       " (834, 'early'),\n",
       " (835, 'whom'),\n",
       " (836, '21'),\n",
       " (837, 'point'),\n",
       " (838, 'denied'),\n",
       " (839, 'waiting'),\n",
       " (840, 'nine'),\n",
       " (841, 'needs'),\n",
       " (842, 'speech'),\n",
       " (843, 'regime'),\n",
       " (844, 'ready'),\n",
       " (845, 'fellow'),\n",
       " (846, 'reason'),\n",
       " (847, 'itself'),\n",
       " (848, 'affairs'),\n",
       " (849, 'iran'),\n",
       " (850, 'analyst'),\n",
       " (851, 'israel'),\n",
       " (852, 'tree'),\n",
       " (853, 'strikes'),\n",
       " (854, 'seconds'),\n",
       " (855, 'britain'),\n",
       " (856, 'having'),\n",
       " (857, 'winning'),\n",
       " (858, 'english'),\n",
       " (859, \"gsa's\"),\n",
       " (860, 'virtual'),\n",
       " (861, 'workers'),\n",
       " (862, \"they're\"),\n",
       " (863, 'actually'),\n",
       " (864, 'january'),\n",
       " (865, 'st'),\n",
       " (866, 'taxpayers'),\n",
       " (867, 'regional'),\n",
       " (868, 'candidate'),\n",
       " (869, 'spokeswoman'),\n",
       " (870, 'requested'),\n",
       " (871, 'me'),\n",
       " (872, 'reform'),\n",
       " (873, 'details'),\n",
       " (874, 'june'),\n",
       " (875, 'budget'),\n",
       " (876, 'amount'),\n",
       " (877, 'free'),\n",
       " (878, 'm'),\n",
       " (879, 'medical'),\n",
       " (880, 'stancheva'),\n",
       " (881, 'mother'),\n",
       " (882, 'arrest'),\n",
       " (883, 'terror'),\n",
       " (884, 'kept'),\n",
       " (885, 'jail'),\n",
       " (886, \"city's\"),\n",
       " (887, 'history'),\n",
       " (888, 'reach'),\n",
       " (889, 'almost'),\n",
       " (890, 'condition'),\n",
       " (891, 'enter'),\n",
       " (892, '19'),\n",
       " (893, 'thousands'),\n",
       " (894, 'seven'),\n",
       " (895, 'liberia'),\n",
       " (896, '40'),\n",
       " (897, 'dialogue'),\n",
       " (898, 'incident'),\n",
       " (899, \"can't\"),\n",
       " (900, 'immigrant'),\n",
       " (901, 'mexican'),\n",
       " (902, 'union'),\n",
       " (903, 'ever'),\n",
       " (904, 'executive'),\n",
       " (905, 'requires'),\n",
       " (906, 'street'),\n",
       " (907, 'bill'),\n",
       " (908, 'threat'),\n",
       " (909, 'officers'),\n",
       " (910, 'minority'),\n",
       " (911, 'anything'),\n",
       " (912, 'especially'),\n",
       " (913, 'care'),\n",
       " (914, 'concerns'),\n",
       " (915, 'vice'),\n",
       " (916, 'walker'),\n",
       " (917, 'landrieu'),\n",
       " (918, 'december'),\n",
       " (919, 'hit'),\n",
       " (920, 'facility'),\n",
       " (921, 'deraney'),\n",
       " (922, 'visit'),\n",
       " (923, 'let'),\n",
       " (924, 'coming'),\n",
       " (925, '13'),\n",
       " (926, 'church'),\n",
       " (927, 'sometimes'),\n",
       " (928, 'power'),\n",
       " (929, 'chinese'),\n",
       " (930, 'guantanamo'),\n",
       " (931, 'attorneys'),\n",
       " (932, 'offer'),\n",
       " (933, 'london'),\n",
       " (934, '2001'),\n",
       " (935, 'palau'),\n",
       " (936, 'average'),\n",
       " (937, 'nepal'),\n",
       " (938, 'practice'),\n",
       " (939, 'permission'),\n",
       " (940, \"america's\"),\n",
       " (941, 'today'),\n",
       " (942, 'dressed'),\n",
       " (943, 'less'),\n",
       " (944, 'looking'),\n",
       " (945, \"couldn't\"),\n",
       " (946, 'images'),\n",
       " (947, 'thought'),\n",
       " (948, \"he'd\"),\n",
       " (949, 'makers'),\n",
       " (950, 'players'),\n",
       " (951, 'software'),\n",
       " (952, 'tablet'),\n",
       " (953, 'push'),\n",
       " (954, 'independent'),\n",
       " (955, 'bayern'),\n",
       " (956, \"football's\"),\n",
       " (957, 'brands'),\n",
       " (958, 'chattaway'),\n",
       " (959, 'slow'),\n",
       " (960, 'terrorism'),\n",
       " (961, 'essential'),\n",
       " (962, 'rules'),\n",
       " (963, 'examination'),\n",
       " (964, 'client'),\n",
       " (965, 'series'),\n",
       " (966, 'cdc'),\n",
       " (967, 'cantaloupes'),\n",
       " (968, 'yet'),\n",
       " (969, 'identities'),\n",
       " (970, 'although'),\n",
       " (971, 'beat'),\n",
       " (972, 'young'),\n",
       " (973, 'ruling'),\n",
       " (974, 'drama'),\n",
       " (975, 'played'),\n",
       " (976, 'lawyer'),\n",
       " (977, 'doma'),\n",
       " (978, 'tells'),\n",
       " (979, 'front'),\n",
       " (980, 'track'),\n",
       " (981, 'uefa'),\n",
       " (982, 'half'),\n",
       " (983, 'dee'),\n",
       " (984, 'horse'),\n",
       " (985, 'greek'),\n",
       " (986, 'hazzard'),\n",
       " (987, 'muslims'),\n",
       " (988, 'youth'),\n",
       " (989, \"egypt's\"),\n",
       " (990, 'protest'),\n",
       " (991, 'swedish'),\n",
       " (992, 'protests'),\n",
       " (993, 'base'),\n",
       " (994, 'tort'),\n",
       " (995, 'hong'),\n",
       " (996, 'lawmakers'),\n",
       " (997, 'sent'),\n",
       " (998, 'targets'),\n",
       " (999, 'legislation'),\n",
       " ...]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(index_to_word.items(), key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER.num_words = min(len(TOKENIZER.word_index)+1, TOKENIZER.num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = text_generator(FILES)\n",
    "x, y = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9442"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It\\'s official: U.S. President Barack Obama wants lawmakers to weigh in on whether to use military force in Syria.\\n\\nObama sent a letter to the heads of the House and Senate on Saturday night, hours after announcing that he believes military action against Syrian targets is the right step to take over the alleged use of chemical weapons.\\n\\nThe proposed legislation from Obama asks Congress to approve the use of military force \"to deter, disrupt, prevent and degrade the potential for future uses of chemical weapons or other weapons of mass destruction.\"\\n\\nIt\\'s a step that is set to turn an international crisis into a fierce domestic political battle.\\n\\nThere are key questions looming over the debate: What did U.N. weapons inspectors find in Syria? What happens if Congress votes no? And how will the Syrian government react?\\n\\nIn a televised address from the White House Rose Garden earlier Saturday, the president said he would take his case to Congress, not because he has to -- but because he wants to.\\n\\n\"While I believe I have the authority to carry out this military action without specific congressional authorization, I know that the country will be stronger if we take this course, and our actions will be even more effective,\" he said. \"We should have this debate, because the issues are too big for business as usual.\"\\n\\nObama said top congressional leaders had agreed to schedule a debate when the body returns to Washington on September 9. The Senate Foreign Relations Committee will hold a hearing over the matter on Tuesday, Sen. Robert Menendez said.\\n\\nTranscript: Read Obama\\'s full remarks\\n\\nSyrian crisis: Latest developments\\n\\nU.N. inspectors leave Syria\\n\\nObama\\'s remarks came shortly after U.N. inspectors left Syria, carrying evidence that will determine whether chemical weapons were used in an attack early last week in a Damascus suburb.\\n\\n\"The aim of the game here, the mandate, is very clear -- and that is to ascertain whether chemical weapons were used -- and not by whom,\" U.N. spokesman Martin Nesirky told reporters on Saturday.\\n\\nBut who used the weapons in the reported toxic gas attack in a Damascus suburb on August 21 has been a key point of global debate over the Syrian crisis.\\n\\nTop U.S. officials have said there\\'s no doubt that the Syrian government was behind it, while Syrian officials have denied responsibility and blamed jihadists fighting with the rebels.\\n\\nBritish and U.S. intelligence reports say the attack involved chemical weapons, but U.N. officials have stressed the importance of waiting for an official report from inspectors.\\n\\nThe inspectors will share their findings with U.N. Secretary-General Ban Ki-moon Ban, who has said he wants to wait until the U.N. team\\'s final report is completed before presenting it to the U.N. Security Council.\\n\\nThe Organization for the Prohibition of Chemical Weapons, which nine of the inspectors belong to, said Saturday that it could take up to three weeks to analyze the evidence they collected.\\n\\n\"It needs time to be able to analyze the information and the samples,\" Nesirky said.\\n\\nHe noted that Ban has repeatedly said there is no alternative to a political solution to the crisis in Syria, and that \"a military solution is not an option.\"\\n\\nBergen:  Syria is a problem from hell for the U.S.\\n\\nObama: \\'This menace must be confronted\\'\\n\\nObama\\'s senior advisers have debated the next steps to take, and the president\\'s comments Saturday came amid mounting political pressure over the situation in Syria. Some U.S. lawmakers have called for immediate action while others warn of stepping into what could become a quagmire.\\n\\nSome global leaders have expressed support, but the British Parliament\\'s vote against military action earlier this week was a blow to Obama\\'s hopes of getting strong backing from key NATO allies.\\n\\nOn Saturday, Obama proposed what he said would be a limited military action against Syrian President Bashar al-Assad. Any military attack would not be open-ended or include U.S. ground forces, he said.\\n\\nSyria\\'s alleged use of chemical weapons earlier this month \"is an assault on human dignity,\" the president said.\\n\\nA failure to respond with force, Obama argued,  \"could lead to escalating use of chemical weapons or their proliferation to terrorist groups who would do our people harm. In a world with many dangers, this menace must be confronted.\"\\n\\nSyria missile strike: What would happen next?\\n\\nMap: U.S. and allied assets around Syria\\n\\nObama decision came Friday night\\n\\nOn Friday night, the president made a last-minute decision to consult lawmakers.\\n\\nWhat will happen if they vote no?\\n\\nIt\\'s unclear. A senior administration official told CNN that Obama has the authority to act without Congress -- even if Congress rejects his request for authorization to use force.\\n\\nObama on Saturday continued to shore up support for a strike on the al-Assad government.\\n\\nHe spoke by phone with French President Francois Hollande before his Rose Garden speech.\\n\\n\"The two leaders agreed that the international community must deliver a resolute message to the Assad regime -- and others who would consider using chemical weapons -- that these crimes are unacceptable and those who violate this international norm will be held accountable by the world,\" the White House said.\\n\\nMeanwhile, as uncertainty loomed over how Congress would weigh in, U.S. military officials said they remained at the ready.\\n\\n5 key assertions: U.S. intelligence report on Syria\\n\\nSyria: Who wants what after chemical weapons horror\\n\\nReactions mixed to Obama\\'s speech\\n\\nA spokesman for the Syrian National Coalition said that the opposition group was disappointed by Obama\\'s announcement.\\n\\n\"Our fear now is that the lack of action could embolden the regime and they repeat his attacks in a more serious way,\" said spokesman Louay Safi. \"So we are quite concerned.\"\\n\\nSome members of Congress applauded Obama\\'s decision.\\n\\nHouse Speaker John Boehner, Majority Leader Eric Cantor, Majority Whip Kevin McCarthy and Conference Chair Cathy McMorris Rodgers issued a statement Saturday praising the president.\\n\\n\"Under the Constitution, the responsibility to declare war lies with Congress,\" the Republican lawmakers said. \"We are glad the president is seeking authorization for any military action in Syria in response to serious, substantive questions being raised.\"\\n\\nMore than 160 legislators, including 63 of Obama\\'s fellow Democrats, had signed letters calling for either a vote or at least a \"full debate\" before any U.S. action.\\n\\nBritish Prime Minister David Cameron, whose own attempt to get lawmakers in his country to support military action in Syria failed earlier this week, responded to Obama\\'s speech in a Twitter post Saturday.\\n\\n\"I understand and support Barack Obama\\'s position on Syria,\" Cameron said.\\n\\nAn influential lawmaker in Russia -- which has stood by Syria and criticized the United States -- had his own theory.\\n\\n\"The main reason Obama is turning to the Congress:  the military operation did not get enough support either in the world, among allies of the US or in the United States itself,\" Alexei Pushkov, chairman of the international-affairs committee of the Russian State Duma, said in a Twitter post.\\n\\nIn the United States, scattered groups of anti-war protesters around the country took to the streets Saturday.\\n\\n\"Like many other Americans...we\\'re just tired of the United States getting involved and invading and bombing other countries,\" said Robin Rosecrans, who was among hundreds at a Los Angeles demonstration.\\n\\nWhat do Syria\\'s neighbors think?\\n\\nWhy Russia, China, Iran stand by Assad\\n\\nSyria\\'s government unfazed\\n\\nAfter Obama\\'s speech, a military and political analyst on Syrian state TV said Obama is \"embarrassed\" that Russia opposes military action against Syria, is \"crying for help\" for someone to come to his rescue and is facing two defeats -- on the political and military levels.\\n\\nSyria\\'s prime minister appeared unfazed by the saber-rattling.\\n\\n\"The Syrian Army\\'s status is on maximum readiness and fingers are on the trigger to confront all challenges,\" Wael Nader al-Halqi said during a meeting with a delegation of Syrian expatriates from Italy, according to a banner on Syria State TV that was broadcast prior to Obama\\'s address.\\n\\nAn anchor on Syrian state television said Obama \"appeared to be preparing for an aggression on Syria based on repeated lies.\"\\n\\nA top Syrian diplomat told the state television network that Obama was facing pressure to take military action from Israel, Turkey, some Arabs and right-wing extremists in the United States.\\n\\n\"I think he has done well by doing what Cameron did in terms of taking the issue to Parliament,\" said Bashar Jaafari, Syria\\'s ambassador to the United Nations.\\n\\nBoth Obama and Cameron, he said, \"climbed to the top of the tree and don\\'t know how to get down.\"\\n\\nThe Syrian government has denied that it used chemical weapons in the August 21 attack, saying that jihadists fighting with the rebels used them in an effort to turn global sentiments against it.\\n\\nBritish intelligence had put the number of people killed in the attack at more than 350.\\n\\nOn Saturday, Obama said \"all told, well over 1,000 people were murdered.\" U.S. Secretary of State John Kerry on Friday cited a death toll of 1,429, more than 400 of them children. No explanation was offered for the discrepancy.\\n\\nIran: U.S. military action in Syria would spark \\'disaster\\'\\n\\nOpinion: Why strikes in Syria are a bad idea\\n\\n'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1,\n",
       "  76,\n",
       "  144,\n",
       "  35,\n",
       "  41,\n",
       "  96,\n",
       "  581,\n",
       "  68,\n",
       "  457,\n",
       "  3,\n",
       "  5,\n",
       "  3,\n",
       "  8,\n",
       "  13,\n",
       "  150,\n",
       "  5,\n",
       "  263,\n",
       "  95,\n",
       "  222,\n",
       "  8,\n",
       "  80,\n",
       "  68,\n",
       "  3,\n",
       "  7,\n",
       "  3,\n",
       "  5,\n",
       "  4,\n",
       "  3,\n",
       "  6,\n",
       "  4,\n",
       "  159,\n",
       "  9,\n",
       "  264,\n",
       "  13,\n",
       "  192,\n",
       "  404,\n",
       "  582,\n",
       "  58,\n",
       "  3,\n",
       "  10,\n",
       "  18,\n",
       "  515,\n",
       "  95,\n",
       "  165,\n",
       "  83,\n",
       "  88,\n",
       "  3,\n",
       "  11,\n",
       "  4,\n",
       "  171,\n",
       "  516,\n",
       "  5,\n",
       "  114,\n",
       "  87,\n",
       "  4,\n",
       "  3,\n",
       "  263,\n",
       "  6,\n",
       "  405,\n",
       "  177,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  24,\n",
       "  68,\n",
       "  3,\n",
       "  303,\n",
       "  5,\n",
       "  3,\n",
       "  4,\n",
       "  263,\n",
       "  6,\n",
       "  95,\n",
       "  222,\n",
       "  5,\n",
       "  3,\n",
       "  3,\n",
       "  304,\n",
       "  9,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  12,\n",
       "  237,\n",
       "  3,\n",
       "  6,\n",
       "  405,\n",
       "  177,\n",
       "  39,\n",
       "  63,\n",
       "  177,\n",
       "  6,\n",
       "  3,\n",
       "  3,\n",
       "  76,\n",
       "  7,\n",
       "  516,\n",
       "  10,\n",
       "  11,\n",
       "  2]]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sequence = tokenize(\n",
    "    x,\n",
    "    TOKENIZER.texts_to_sequences,\n",
    "    maxlen=MAX_SEQUENCE_LEN,\n",
    "    begin_token=BEGIN_TOKEN,\n",
    "    end_token=END_TOKEN)\n",
    "x_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 88, 144, 68, 3, 5, 4, 207, 6, 4, 3, 272, 195, 160, 5, 121, 139, 2]]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sequence = tokenize(\n",
    "    y,\n",
    "    TOKENIZER.texts_to_sequences,\n",
    "    maxlen=MAX_SEQUENCE_LEN,\n",
    "    begin_token=BEGIN_TOKEN,\n",
    "    end_token=END_TOKEN)\n",
    "y_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<begin>',\n",
       "  \"it's\",\n",
       "  'official',\n",
       "  'u',\n",
       "  's',\n",
       "  'president',\n",
       "  'barack',\n",
       "  'obama',\n",
       "  'wants',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'on',\n",
       "  'whether',\n",
       "  'to',\n",
       "  'use',\n",
       "  'military',\n",
       "  'force',\n",
       "  'in',\n",
       "  'syria',\n",
       "  'obama',\n",
       "  '<unk>',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'the',\n",
       "  'house',\n",
       "  'and',\n",
       "  'senate',\n",
       "  'on',\n",
       "  'saturday',\n",
       "  'night',\n",
       "  'hours',\n",
       "  'after',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'he',\n",
       "  'believes',\n",
       "  'military',\n",
       "  'action',\n",
       "  'against',\n",
       "  'syrian',\n",
       "  '<unk>',\n",
       "  'is',\n",
       "  'the',\n",
       "  'right',\n",
       "  'step',\n",
       "  'to',\n",
       "  'take',\n",
       "  'over',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'use',\n",
       "  'of',\n",
       "  'chemical',\n",
       "  'weapons',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'from',\n",
       "  'obama',\n",
       "  '<unk>',\n",
       "  'congress',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  'use',\n",
       "  'of',\n",
       "  'military',\n",
       "  'force',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'prevent',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'for',\n",
       "  'future',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'chemical',\n",
       "  'weapons',\n",
       "  'or',\n",
       "  'other',\n",
       "  'weapons',\n",
       "  'of',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  \"it's\",\n",
       "  'a',\n",
       "  'step',\n",
       "  'that',\n",
       "  'is',\n",
       "  '<end>']]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[index_to_word[i] for i in L] for L in x_sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<begin>',\n",
       "  'syrian',\n",
       "  'official',\n",
       "  'obama',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  'the',\n",
       "  'top',\n",
       "  'of',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  \"doesn't\",\n",
       "  'know',\n",
       "  'how',\n",
       "  'to',\n",
       "  'get',\n",
       "  'down',\n",
       "  '<end>']]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[index_to_word[i] for i in L] for L in y_sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_sequence), len(x_sequence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,  76, 144,  35,  41,  96, 581,  68, 457,   3,   5,   3,   8,\n",
       "         13, 150,   5, 263,  95, 222,   8,  80,  68,   3,   7,   3,   5,\n",
       "          4,   3,   6,   4, 159,   9, 264,  13, 192, 404, 582,  58,   3,\n",
       "         10,  18, 515,  95, 165,  83,  88,   3,  11,   4, 171, 516,   5,\n",
       "        114,  87,   4,   3, 263,   6, 405, 177,   4,   3,   3,  24,  68,\n",
       "          3, 303,   5,   3,   4, 263,   6,  95, 222,   5,   3,   3, 304,\n",
       "          9,   3,   4,   3,  12, 237,   3,   6, 405, 177,  39,  63, 177,\n",
       "          6,   3,   3,  76,   7, 516,  10,  11,   2]], dtype=int32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "pad_sequences(x_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<begin>',\n",
       "  \"it's\",\n",
       "  'official',\n",
       "  'u',\n",
       "  's',\n",
       "  'president',\n",
       "  'barack',\n",
       "  'obama',\n",
       "  'wants',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'on',\n",
       "  'whether',\n",
       "  'to',\n",
       "  'use',\n",
       "  'military',\n",
       "  'force',\n",
       "  'in',\n",
       "  'syria',\n",
       "  'obama',\n",
       "  '<unk>',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'the',\n",
       "  'house',\n",
       "  'and',\n",
       "  'senate',\n",
       "  'on',\n",
       "  'saturday',\n",
       "  'night',\n",
       "  'hours',\n",
       "  'after',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'he',\n",
       "  'believes',\n",
       "  'military',\n",
       "  'action',\n",
       "  'against',\n",
       "  'syrian',\n",
       "  '<unk>',\n",
       "  'is',\n",
       "  'the',\n",
       "  'right',\n",
       "  'step',\n",
       "  'to',\n",
       "  'take',\n",
       "  'over',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'use',\n",
       "  'of',\n",
       "  'chemical',\n",
       "  'weapons',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'from',\n",
       "  'obama',\n",
       "  '<unk>',\n",
       "  'congress',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  'use',\n",
       "  'of',\n",
       "  'military',\n",
       "  'force',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'prevent',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'for',\n",
       "  'future',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'chemical',\n",
       "  'weapons',\n",
       "  'or',\n",
       "  'other',\n",
       "  'weapons',\n",
       "  'of',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  \"it's\",\n",
       "  'a',\n",
       "  'step',\n",
       "  'that',\n",
       "  'is',\n",
       "  '<end>']]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[index_to_word[i] for i in x] for x in pad_sequences(x_sequence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 76,\n",
       " 144,\n",
       " 35,\n",
       " 41,\n",
       " 96,\n",
       " 581,\n",
       " 68,\n",
       " 457,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 8,\n",
       " 13,\n",
       " 150,\n",
       " 5,\n",
       " 263,\n",
       " 95,\n",
       " 222,\n",
       " 8,\n",
       " 80,\n",
       " 68,\n",
       " 3,\n",
       " 7,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 159,\n",
       " 9,\n",
       " 264,\n",
       " 13,\n",
       " 192,\n",
       " 404,\n",
       " 582,\n",
       " 58,\n",
       " 3,\n",
       " 10,\n",
       " 18,\n",
       " 515,\n",
       " 95,\n",
       " 165,\n",
       " 83,\n",
       " 88,\n",
       " 3,\n",
       " 11,\n",
       " 4,\n",
       " 171,\n",
       " 516,\n",
       " 5,\n",
       " 114,\n",
       " 87,\n",
       " 4,\n",
       " 3,\n",
       " 263,\n",
       " 6,\n",
       " 405,\n",
       " 177,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 24,\n",
       " 68,\n",
       " 3,\n",
       " 303,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 263,\n",
       " 6,\n",
       " 95,\n",
       " 222,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 304,\n",
       " 9,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 12,\n",
       " 237,\n",
       " 3,\n",
       " 6,\n",
       " 405,\n",
       " 177,\n",
       " 39,\n",
       " 63,\n",
       " 177,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 76,\n",
       " 7,\n",
       " 516,\n",
       " 10,\n",
       " 11,\n",
       " 2]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = x_sequence[0]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = TOKENIZER.sequences_to_matrix([[i] for i in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   1],\n",
       "       [  1,  76],\n",
       "       [  2, 144],\n",
       "       [  3,  35],\n",
       "       [  4,  41],\n",
       "       [  5,  96],\n",
       "       [  6, 581],\n",
       "       [  7,  68],\n",
       "       [  8, 457],\n",
       "       [  9,   3],\n",
       "       [ 10,   5],\n",
       "       [ 11,   3],\n",
       "       [ 12,   8],\n",
       "       [ 13,  13],\n",
       "       [ 14, 150],\n",
       "       [ 15,   5],\n",
       "       [ 16, 263],\n",
       "       [ 17,  95],\n",
       "       [ 18, 222],\n",
       "       [ 19,   8],\n",
       "       [ 20,  80],\n",
       "       [ 21,  68],\n",
       "       [ 22,   3],\n",
       "       [ 23,   7],\n",
       "       [ 24,   3],\n",
       "       [ 25,   5],\n",
       "       [ 26,   4],\n",
       "       [ 27,   3],\n",
       "       [ 28,   6],\n",
       "       [ 29,   4],\n",
       "       [ 30, 159],\n",
       "       [ 31,   9],\n",
       "       [ 32, 264],\n",
       "       [ 33,  13],\n",
       "       [ 34, 192],\n",
       "       [ 35, 404],\n",
       "       [ 36, 582],\n",
       "       [ 37,  58],\n",
       "       [ 38,   3],\n",
       "       [ 39,  10],\n",
       "       [ 40,  18],\n",
       "       [ 41, 515],\n",
       "       [ 42,  95],\n",
       "       [ 43, 165],\n",
       "       [ 44,  83],\n",
       "       [ 45,  88],\n",
       "       [ 46,   3],\n",
       "       [ 47,  11],\n",
       "       [ 48,   4],\n",
       "       [ 49, 171]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only one per row\n",
    "import numpy as np\n",
    "np.argwhere(one_hot == 1)[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequencer(tokens):\n",
    "    return [tokens[:i] for i in range(1, len(tokens)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'a ',\n",
       " 'a q',\n",
       " 'a qu',\n",
       " 'a qui',\n",
       " 'a quic',\n",
       " 'a quick',\n",
       " 'a quick ',\n",
       " 'a quick b',\n",
       " 'a quick br',\n",
       " 'a quick bro',\n",
       " 'a quick brow',\n",
       " 'a quick brown',\n",
       " 'a quick brown ',\n",
       " 'a quick brown f',\n",
       " 'a quick brown fo',\n",
       " 'a quick brown fox']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sequencer('a quick brown fox'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class BatchGenerator:\n",
    "    def __init__(self, files, tokenizer, maxlen, batch_size, begin_token, end_token, epoch_end=None):\n",
    "        self.files = files\n",
    "        self.tokenizer = tokenizer\n",
    "        self.maxlen = maxlen\n",
    "        self.batch_size = batch_size\n",
    "        self.begin_token = begin_token\n",
    "        self.end_token = end_token\n",
    "        self.epoch_end = epoch_end\n",
    "        \n",
    "    def generate(self):\n",
    "        steps = []\n",
    "        while True:\n",
    "            random.shuffle(self.files)\n",
    "            for encode_text, decode_text in self.iter_files(self.files):\n",
    "                encode_tokens = self.tokenize(encode_text)\n",
    "                decode_tokens = self.tokenize(decode_text)\n",
    "\n",
    "                for seq_tokens in self.sequence(decode_tokens):\n",
    "                    steps.append((encode_tokens, seq_tokens))\n",
    "                    \n",
    "                while len(steps) >= self.batch_size:\n",
    "                    batch = steps[:self.batch_size]\n",
    "                    encoder_input, decoder_input = zip(*batch)\n",
    "                    x1 = pad_sequences(encoder_input, maxlen=self.maxlen)\n",
    "                    x2 = pad_sequences(decoder_input, maxlen=self.maxlen)\n",
    "                    y = self.tokenizer.sequences_to_matrix([[i] for seq in x2 for i in seq])\n",
    "                    y = y.reshape((self.batch_size, self.maxlen, self.tokenizer.num_words))\n",
    "                    \n",
    "                    # offset\n",
    "                    X = [x1[:-1], x2[:-1]]\n",
    "                    y = y[1:]\n",
    "                    yield X, y\n",
    "                    \n",
    "                    # reset\n",
    "                    steps = steps[self.batch_size:]\n",
    "            yield self.epoch_end\n",
    "        \n",
    "    def preprocess(self, text):\n",
    "        # replace all occurences of multiple newlines and replace them\n",
    "        # with a single newline padded with spaces so it is treated as a\n",
    "        # token\n",
    "        text = ' \\n '.join(t for t in text.split('\\n') if t)\n",
    "        table = {ord(c): None for c in '<>'}\n",
    "        text = text.translate(table)\n",
    "        return text\n",
    "\n",
    "    def iter_files(self, files):\n",
    "        for f in files:\n",
    "            text = open(f).read()\n",
    "            text = self.preprocess(text)\n",
    "            # remove highlights\n",
    "            body, highlight1, *_ = text.split('@highlight')\n",
    "            yield body, highlight1\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        tokens = self.tokenizer.texts_to_sequences([text])[0]\n",
    "        tokens = tokens[:self.maxlen-2]\n",
    "        return [self.begin_token] + tokens + [self.end_token]\n",
    "\n",
    "    def sequence(self, tokens):\n",
    "        return [tokens[:i] for i in range(1, len(tokens)+1)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_gen = BatchGenerator(\n",
    "    files=FILES,\n",
    "    tokenizer=TOKENIZER,\n",
    "    maxlen=MAX_SEQUENCE_LEN,\n",
    "    batch_size=32,\n",
    "    begin_token=BEGIN_TOKEN,\n",
    "    end_token=END_TOKEN).generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = next(batch_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31, 100), (31, 100), (31, 100, 600))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape, X[1].shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,  49,   3, ...,   8,   3,   2],\n",
       "       [  1,  49,   3, ...,   8,   3,   2],\n",
       "       [  1,  49,   3, ...,   8,   3,   2],\n",
       "       ...,\n",
       "       [  1,  57, 396, ..., 489, 523,   2],\n",
       "       [  1,  57, 396, ..., 489, 523,   2],\n",
       "       [  1,  57, 396, ..., 489, 523,   2]], dtype=int32)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, x2 = X\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<begin>',\n",
       "  'cnn',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'at',\n",
       "  'a',\n",
       "  'syrian',\n",
       "  'government',\n",
       "  '<unk>',\n",
       "  'national',\n",
       "  '<unk>',\n",
       "  'meeting',\n",
       "  'sunday',\n",
       "  'criticized',\n",
       "  'recent',\n",
       "  '<unk>',\n",
       "  'by',\n",
       "  'the',\n",
       "  \"country's\",\n",
       "  'security',\n",
       "  'forces',\n",
       "  '<unk>',\n",
       "  'for',\n",
       "  'an',\n",
       "  'end',\n",
       "  'to',\n",
       "  'violence',\n",
       "  'against',\n",
       "  'protesters',\n",
       "  \"syria's\",\n",
       "  '<unk>',\n",
       "  'president',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'university',\n",
       "  'meeting',\n",
       "  'between',\n",
       "  'officials',\n",
       "  'and',\n",
       "  'members',\n",
       "  'of',\n",
       "  'the',\n",
       "  'opposition',\n",
       "  'as',\n",
       "  'a',\n",
       "  'step',\n",
       "  'toward',\n",
       "  '<unk>',\n",
       "  'a',\n",
       "  'democratic',\n",
       "  'nation',\n",
       "  'we',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'at',\n",
       "  'the',\n",
       "  'end',\n",
       "  'of',\n",
       "  'this',\n",
       "  '<unk>',\n",
       "  'meeting',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'syria',\n",
       "  'to',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  'democratic',\n",
       "  'nation',\n",
       "  'where',\n",
       "  'all',\n",
       "  'citizens',\n",
       "  'are',\n",
       "  '<unk>',\n",
       "  'by',\n",
       "  '<unk>',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'the',\n",
       "  'future',\n",
       "  'of',\n",
       "  'their',\n",
       "  'country',\n",
       "  '<unk>',\n",
       "  'president',\n",
       "  '<unk>',\n",
       "  'al',\n",
       "  '<unk>',\n",
       "  'said',\n",
       "  'in',\n",
       "  '<unk>',\n",
       "  '<end>'],\n",
       " ['<begin>',\n",
       "  'cnn',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'at',\n",
       "  'a',\n",
       "  'syrian',\n",
       "  'government',\n",
       "  '<unk>',\n",
       "  'national',\n",
       "  '<unk>',\n",
       "  'meeting',\n",
       "  'sunday',\n",
       "  'criticized',\n",
       "  'recent',\n",
       "  '<unk>',\n",
       "  'by',\n",
       "  'the',\n",
       "  \"country's\",\n",
       "  'security',\n",
       "  'forces',\n",
       "  '<unk>',\n",
       "  'for',\n",
       "  'an',\n",
       "  'end',\n",
       "  'to',\n",
       "  'violence',\n",
       "  'against',\n",
       "  'protesters',\n",
       "  \"syria's\",\n",
       "  '<unk>',\n",
       "  'president',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'university',\n",
       "  'meeting',\n",
       "  'between',\n",
       "  'officials',\n",
       "  'and',\n",
       "  'members',\n",
       "  'of',\n",
       "  'the',\n",
       "  'opposition',\n",
       "  'as',\n",
       "  'a',\n",
       "  'step',\n",
       "  'toward',\n",
       "  '<unk>',\n",
       "  'a',\n",
       "  'democratic',\n",
       "  'nation',\n",
       "  'we',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'at',\n",
       "  'the',\n",
       "  'end',\n",
       "  'of',\n",
       "  'this',\n",
       "  '<unk>',\n",
       "  'meeting',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'syria',\n",
       "  'to',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  'democratic',\n",
       "  'nation',\n",
       "  'where',\n",
       "  'all',\n",
       "  'citizens',\n",
       "  'are',\n",
       "  '<unk>',\n",
       "  'by',\n",
       "  '<unk>',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'the',\n",
       "  'future',\n",
       "  'of',\n",
       "  'their',\n",
       "  'country',\n",
       "  '<unk>',\n",
       "  'president',\n",
       "  '<unk>',\n",
       "  'al',\n",
       "  '<unk>',\n",
       "  'said',\n",
       "  'in',\n",
       "  '<unk>',\n",
       "  '<end>'],\n",
       " ['<begin>',\n",
       "  'cnn',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'at',\n",
       "  'a',\n",
       "  'syrian',\n",
       "  'government',\n",
       "  '<unk>',\n",
       "  'national',\n",
       "  '<unk>',\n",
       "  'meeting',\n",
       "  'sunday',\n",
       "  'criticized',\n",
       "  'recent',\n",
       "  '<unk>',\n",
       "  'by',\n",
       "  'the',\n",
       "  \"country's\",\n",
       "  'security',\n",
       "  'forces',\n",
       "  '<unk>',\n",
       "  'for',\n",
       "  'an',\n",
       "  'end',\n",
       "  'to',\n",
       "  'violence',\n",
       "  'against',\n",
       "  'protesters',\n",
       "  \"syria's\",\n",
       "  '<unk>',\n",
       "  'president',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'university',\n",
       "  'meeting',\n",
       "  'between',\n",
       "  'officials',\n",
       "  'and',\n",
       "  'members',\n",
       "  'of',\n",
       "  'the',\n",
       "  'opposition',\n",
       "  'as',\n",
       "  'a',\n",
       "  'step',\n",
       "  'toward',\n",
       "  '<unk>',\n",
       "  'a',\n",
       "  'democratic',\n",
       "  'nation',\n",
       "  'we',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'at',\n",
       "  'the',\n",
       "  'end',\n",
       "  'of',\n",
       "  'this',\n",
       "  '<unk>',\n",
       "  'meeting',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'syria',\n",
       "  'to',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  'democratic',\n",
       "  'nation',\n",
       "  'where',\n",
       "  'all',\n",
       "  'citizens',\n",
       "  'are',\n",
       "  '<unk>',\n",
       "  'by',\n",
       "  '<unk>',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'the',\n",
       "  'future',\n",
       "  'of',\n",
       "  'their',\n",
       "  'country',\n",
       "  '<unk>',\n",
       "  'president',\n",
       "  '<unk>',\n",
       "  'al',\n",
       "  '<unk>',\n",
       "  'said',\n",
       "  'in',\n",
       "  '<unk>',\n",
       "  '<end>'],\n",
       " ['<begin>',\n",
       "  'cnn',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'at',\n",
       "  'a',\n",
       "  'syrian',\n",
       "  'government',\n",
       "  '<unk>',\n",
       "  'national',\n",
       "  '<unk>',\n",
       "  'meeting',\n",
       "  'sunday',\n",
       "  'criticized',\n",
       "  'recent',\n",
       "  '<unk>',\n",
       "  'by',\n",
       "  'the',\n",
       "  \"country's\",\n",
       "  'security',\n",
       "  'forces',\n",
       "  '<unk>',\n",
       "  'for',\n",
       "  'an',\n",
       "  'end',\n",
       "  'to',\n",
       "  'violence',\n",
       "  'against',\n",
       "  'protesters',\n",
       "  \"syria's\",\n",
       "  '<unk>',\n",
       "  'president',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'university',\n",
       "  'meeting',\n",
       "  'between',\n",
       "  'officials',\n",
       "  'and',\n",
       "  'members',\n",
       "  'of',\n",
       "  'the',\n",
       "  'opposition',\n",
       "  'as',\n",
       "  'a',\n",
       "  'step',\n",
       "  'toward',\n",
       "  '<unk>',\n",
       "  'a',\n",
       "  'democratic',\n",
       "  'nation',\n",
       "  'we',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'at',\n",
       "  'the',\n",
       "  'end',\n",
       "  'of',\n",
       "  'this',\n",
       "  '<unk>',\n",
       "  'meeting',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'syria',\n",
       "  'to',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  'democratic',\n",
       "  'nation',\n",
       "  'where',\n",
       "  'all',\n",
       "  'citizens',\n",
       "  'are',\n",
       "  '<unk>',\n",
       "  'by',\n",
       "  '<unk>',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'the',\n",
       "  'future',\n",
       "  'of',\n",
       "  'their',\n",
       "  'country',\n",
       "  '<unk>',\n",
       "  'president',\n",
       "  '<unk>',\n",
       "  'al',\n",
       "  '<unk>',\n",
       "  'said',\n",
       "  'in',\n",
       "  '<unk>',\n",
       "  '<end>'],\n",
       " ['<begin>',\n",
       "  'cnn',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'at',\n",
       "  'a',\n",
       "  'syrian',\n",
       "  'government',\n",
       "  '<unk>',\n",
       "  'national',\n",
       "  '<unk>',\n",
       "  'meeting',\n",
       "  'sunday',\n",
       "  'criticized',\n",
       "  'recent',\n",
       "  '<unk>',\n",
       "  'by',\n",
       "  'the',\n",
       "  \"country's\",\n",
       "  'security',\n",
       "  'forces',\n",
       "  '<unk>',\n",
       "  'for',\n",
       "  'an',\n",
       "  'end',\n",
       "  'to',\n",
       "  'violence',\n",
       "  'against',\n",
       "  'protesters',\n",
       "  \"syria's\",\n",
       "  '<unk>',\n",
       "  'president',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'university',\n",
       "  'meeting',\n",
       "  'between',\n",
       "  'officials',\n",
       "  'and',\n",
       "  'members',\n",
       "  'of',\n",
       "  'the',\n",
       "  'opposition',\n",
       "  'as',\n",
       "  'a',\n",
       "  'step',\n",
       "  'toward',\n",
       "  '<unk>',\n",
       "  'a',\n",
       "  'democratic',\n",
       "  'nation',\n",
       "  'we',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'at',\n",
       "  'the',\n",
       "  'end',\n",
       "  'of',\n",
       "  'this',\n",
       "  '<unk>',\n",
       "  'meeting',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'syria',\n",
       "  'to',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  'democratic',\n",
       "  'nation',\n",
       "  'where',\n",
       "  'all',\n",
       "  'citizens',\n",
       "  'are',\n",
       "  '<unk>',\n",
       "  'by',\n",
       "  '<unk>',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'the',\n",
       "  'future',\n",
       "  'of',\n",
       "  'their',\n",
       "  'country',\n",
       "  '<unk>',\n",
       "  'president',\n",
       "  '<unk>',\n",
       "  'al',\n",
       "  '<unk>',\n",
       "  'said',\n",
       "  'in',\n",
       "  '<unk>',\n",
       "  '<end>'],\n",
       " ['<begin>',\n",
       "  'cnn',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'at',\n",
       "  'a',\n",
       "  'syrian',\n",
       "  'government',\n",
       "  '<unk>',\n",
       "  'national',\n",
       "  '<unk>',\n",
       "  'meeting',\n",
       "  'sunday',\n",
       "  'criticized',\n",
       "  'recent',\n",
       "  '<unk>',\n",
       "  'by',\n",
       "  'the',\n",
       "  \"country's\",\n",
       "  'security',\n",
       "  'forces',\n",
       "  '<unk>',\n",
       "  'for',\n",
       "  'an',\n",
       "  'end',\n",
       "  'to',\n",
       "  'violence',\n",
       "  'against',\n",
       "  'protesters',\n",
       "  \"syria's\",\n",
       "  '<unk>',\n",
       "  'president',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'university',\n",
       "  'meeting',\n",
       "  'between',\n",
       "  'officials',\n",
       "  'and',\n",
       "  'members',\n",
       "  'of',\n",
       "  'the',\n",
       "  'opposition',\n",
       "  'as',\n",
       "  'a',\n",
       "  'step',\n",
       "  'toward',\n",
       "  '<unk>',\n",
       "  'a',\n",
       "  'democratic',\n",
       "  'nation',\n",
       "  'we',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'at',\n",
       "  'the',\n",
       "  'end',\n",
       "  'of',\n",
       "  'this',\n",
       "  '<unk>',\n",
       "  'meeting',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'syria',\n",
       "  'to',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  'democratic',\n",
       "  'nation',\n",
       "  'where',\n",
       "  'all',\n",
       "  'citizens',\n",
       "  'are',\n",
       "  '<unk>',\n",
       "  'by',\n",
       "  '<unk>',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'the',\n",
       "  'future',\n",
       "  'of',\n",
       "  'their',\n",
       "  'country',\n",
       "  '<unk>',\n",
       "  'president',\n",
       "  '<unk>',\n",
       "  'al',\n",
       "  '<unk>',\n",
       "  'said',\n",
       "  'in',\n",
       "  '<unk>',\n",
       "  '<end>'],\n",
       " ['<begin>',\n",
       "  'cnn',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'at',\n",
       "  'a',\n",
       "  'syrian',\n",
       "  'government',\n",
       "  '<unk>',\n",
       "  'national',\n",
       "  '<unk>',\n",
       "  'meeting',\n",
       "  'sunday',\n",
       "  'criticized',\n",
       "  'recent',\n",
       "  '<unk>',\n",
       "  'by',\n",
       "  'the',\n",
       "  \"country's\",\n",
       "  'security',\n",
       "  'forces',\n",
       "  '<unk>',\n",
       "  'for',\n",
       "  'an',\n",
       "  'end',\n",
       "  'to',\n",
       "  'violence',\n",
       "  'against',\n",
       "  'protesters',\n",
       "  \"syria's\",\n",
       "  '<unk>',\n",
       "  'president',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'university',\n",
       "  'meeting',\n",
       "  'between',\n",
       "  'officials',\n",
       "  'and',\n",
       "  'members',\n",
       "  'of',\n",
       "  'the',\n",
       "  'opposition',\n",
       "  'as',\n",
       "  'a',\n",
       "  'step',\n",
       "  'toward',\n",
       "  '<unk>',\n",
       "  'a',\n",
       "  'democratic',\n",
       "  'nation',\n",
       "  'we',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'at',\n",
       "  'the',\n",
       "  'end',\n",
       "  'of',\n",
       "  'this',\n",
       "  '<unk>',\n",
       "  'meeting',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'syria',\n",
       "  'to',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  'democratic',\n",
       "  'nation',\n",
       "  'where',\n",
       "  'all',\n",
       "  'citizens',\n",
       "  'are',\n",
       "  '<unk>',\n",
       "  'by',\n",
       "  '<unk>',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'the',\n",
       "  'future',\n",
       "  'of',\n",
       "  'their',\n",
       "  'country',\n",
       "  '<unk>',\n",
       "  'president',\n",
       "  '<unk>',\n",
       "  'al',\n",
       "  '<unk>',\n",
       "  'said',\n",
       "  'in',\n",
       "  '<unk>',\n",
       "  '<end>'],\n",
       " ['<begin>',\n",
       "  'cnn',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'at',\n",
       "  'a',\n",
       "  'syrian',\n",
       "  'government',\n",
       "  '<unk>',\n",
       "  'national',\n",
       "  '<unk>',\n",
       "  'meeting',\n",
       "  'sunday',\n",
       "  'criticized',\n",
       "  'recent',\n",
       "  '<unk>',\n",
       "  'by',\n",
       "  'the',\n",
       "  \"country's\",\n",
       "  'security',\n",
       "  'forces',\n",
       "  '<unk>',\n",
       "  'for',\n",
       "  'an',\n",
       "  'end',\n",
       "  'to',\n",
       "  'violence',\n",
       "  'against',\n",
       "  'protesters',\n",
       "  \"syria's\",\n",
       "  '<unk>',\n",
       "  'president',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'university',\n",
       "  'meeting',\n",
       "  'between',\n",
       "  'officials',\n",
       "  'and',\n",
       "  'members',\n",
       "  'of',\n",
       "  'the',\n",
       "  'opposition',\n",
       "  'as',\n",
       "  'a',\n",
       "  'step',\n",
       "  'toward',\n",
       "  '<unk>',\n",
       "  'a',\n",
       "  'democratic',\n",
       "  'nation',\n",
       "  'we',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'at',\n",
       "  'the',\n",
       "  'end',\n",
       "  'of',\n",
       "  'this',\n",
       "  '<unk>',\n",
       "  'meeting',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'syria',\n",
       "  'to',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  'democratic',\n",
       "  'nation',\n",
       "  'where',\n",
       "  'all',\n",
       "  'citizens',\n",
       "  'are',\n",
       "  '<unk>',\n",
       "  'by',\n",
       "  '<unk>',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'the',\n",
       "  'future',\n",
       "  'of',\n",
       "  'their',\n",
       "  'country',\n",
       "  '<unk>',\n",
       "  'president',\n",
       "  '<unk>',\n",
       "  'al',\n",
       "  '<unk>',\n",
       "  'said',\n",
       "  'in',\n",
       "  '<unk>',\n",
       "  '<end>'],\n",
       " ['<begin>',\n",
       "  'cnn',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'at',\n",
       "  'a',\n",
       "  'syrian',\n",
       "  'government',\n",
       "  '<unk>',\n",
       "  'national',\n",
       "  '<unk>',\n",
       "  'meeting',\n",
       "  'sunday',\n",
       "  'criticized',\n",
       "  'recent',\n",
       "  '<unk>',\n",
       "  'by',\n",
       "  'the',\n",
       "  \"country's\",\n",
       "  'security',\n",
       "  'forces',\n",
       "  '<unk>',\n",
       "  'for',\n",
       "  'an',\n",
       "  'end',\n",
       "  'to',\n",
       "  'violence',\n",
       "  'against',\n",
       "  'protesters',\n",
       "  \"syria's\",\n",
       "  '<unk>',\n",
       "  'president',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'university',\n",
       "  'meeting',\n",
       "  'between',\n",
       "  'officials',\n",
       "  'and',\n",
       "  'members',\n",
       "  'of',\n",
       "  'the',\n",
       "  'opposition',\n",
       "  'as',\n",
       "  'a',\n",
       "  'step',\n",
       "  'toward',\n",
       "  '<unk>',\n",
       "  'a',\n",
       "  'democratic',\n",
       "  'nation',\n",
       "  'we',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'at',\n",
       "  'the',\n",
       "  'end',\n",
       "  'of',\n",
       "  'this',\n",
       "  '<unk>',\n",
       "  'meeting',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'syria',\n",
       "  'to',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  'democratic',\n",
       "  'nation',\n",
       "  'where',\n",
       "  'all',\n",
       "  'citizens',\n",
       "  'are',\n",
       "  '<unk>',\n",
       "  'by',\n",
       "  '<unk>',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'the',\n",
       "  'future',\n",
       "  'of',\n",
       "  'their',\n",
       "  'country',\n",
       "  '<unk>',\n",
       "  'president',\n",
       "  '<unk>',\n",
       "  'al',\n",
       "  '<unk>',\n",
       "  'said',\n",
       "  'in',\n",
       "  '<unk>',\n",
       "  '<end>'],\n",
       " ['<begin>',\n",
       "  'cnn',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'at',\n",
       "  'a',\n",
       "  'syrian',\n",
       "  'government',\n",
       "  '<unk>',\n",
       "  'national',\n",
       "  '<unk>',\n",
       "  'meeting',\n",
       "  'sunday',\n",
       "  'criticized',\n",
       "  'recent',\n",
       "  '<unk>',\n",
       "  'by',\n",
       "  'the',\n",
       "  \"country's\",\n",
       "  'security',\n",
       "  'forces',\n",
       "  '<unk>',\n",
       "  'for',\n",
       "  'an',\n",
       "  'end',\n",
       "  'to',\n",
       "  'violence',\n",
       "  'against',\n",
       "  'protesters',\n",
       "  \"syria's\",\n",
       "  '<unk>',\n",
       "  'president',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'university',\n",
       "  'meeting',\n",
       "  'between',\n",
       "  'officials',\n",
       "  'and',\n",
       "  'members',\n",
       "  'of',\n",
       "  'the',\n",
       "  'opposition',\n",
       "  'as',\n",
       "  'a',\n",
       "  'step',\n",
       "  'toward',\n",
       "  '<unk>',\n",
       "  'a',\n",
       "  'democratic',\n",
       "  'nation',\n",
       "  'we',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'at',\n",
       "  'the',\n",
       "  'end',\n",
       "  'of',\n",
       "  'this',\n",
       "  '<unk>',\n",
       "  'meeting',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'syria',\n",
       "  'to',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  'democratic',\n",
       "  'nation',\n",
       "  'where',\n",
       "  'all',\n",
       "  'citizens',\n",
       "  'are',\n",
       "  '<unk>',\n",
       "  'by',\n",
       "  '<unk>',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'the',\n",
       "  'future',\n",
       "  'of',\n",
       "  'their',\n",
       "  'country',\n",
       "  '<unk>',\n",
       "  'president',\n",
       "  '<unk>',\n",
       "  'al',\n",
       "  '<unk>',\n",
       "  'said',\n",
       "  'in',\n",
       "  '<unk>',\n",
       "  '<end>'],\n",
       " ['<begin>',\n",
       "  'cnn',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'at',\n",
       "  'a',\n",
       "  'syrian',\n",
       "  'government',\n",
       "  '<unk>',\n",
       "  'national',\n",
       "  '<unk>',\n",
       "  'meeting',\n",
       "  'sunday',\n",
       "  'criticized',\n",
       "  'recent',\n",
       "  '<unk>',\n",
       "  'by',\n",
       "  'the',\n",
       "  \"country's\",\n",
       "  'security',\n",
       "  'forces',\n",
       "  '<unk>',\n",
       "  'for',\n",
       "  'an',\n",
       "  'end',\n",
       "  'to',\n",
       "  'violence',\n",
       "  'against',\n",
       "  'protesters',\n",
       "  \"syria's\",\n",
       "  '<unk>',\n",
       "  'president',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'university',\n",
       "  'meeting',\n",
       "  'between',\n",
       "  'officials',\n",
       "  'and',\n",
       "  'members',\n",
       "  'of',\n",
       "  'the',\n",
       "  'opposition',\n",
       "  'as',\n",
       "  'a',\n",
       "  'step',\n",
       "  'toward',\n",
       "  '<unk>',\n",
       "  'a',\n",
       "  'democratic',\n",
       "  'nation',\n",
       "  'we',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'at',\n",
       "  'the',\n",
       "  'end',\n",
       "  'of',\n",
       "  'this',\n",
       "  '<unk>',\n",
       "  'meeting',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'syria',\n",
       "  'to',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  'democratic',\n",
       "  'nation',\n",
       "  'where',\n",
       "  'all',\n",
       "  'citizens',\n",
       "  'are',\n",
       "  '<unk>',\n",
       "  'by',\n",
       "  '<unk>',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'the',\n",
       "  'future',\n",
       "  'of',\n",
       "  'their',\n",
       "  'country',\n",
       "  '<unk>',\n",
       "  'president',\n",
       "  '<unk>',\n",
       "  'al',\n",
       "  '<unk>',\n",
       "  'said',\n",
       "  'in',\n",
       "  '<unk>',\n",
       "  '<end>'],\n",
       " ['<begin>',\n",
       "  'cnn',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'at',\n",
       "  'a',\n",
       "  'syrian',\n",
       "  'government',\n",
       "  '<unk>',\n",
       "  'national',\n",
       "  '<unk>',\n",
       "  'meeting',\n",
       "  'sunday',\n",
       "  'criticized',\n",
       "  'recent',\n",
       "  '<unk>',\n",
       "  'by',\n",
       "  'the',\n",
       "  \"country's\",\n",
       "  'security',\n",
       "  'forces',\n",
       "  '<unk>',\n",
       "  'for',\n",
       "  'an',\n",
       "  'end',\n",
       "  'to',\n",
       "  'violence',\n",
       "  'against',\n",
       "  'protesters',\n",
       "  \"syria's\",\n",
       "  '<unk>',\n",
       "  'president',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'university',\n",
       "  'meeting',\n",
       "  'between',\n",
       "  'officials',\n",
       "  'and',\n",
       "  'members',\n",
       "  'of',\n",
       "  'the',\n",
       "  'opposition',\n",
       "  'as',\n",
       "  'a',\n",
       "  'step',\n",
       "  'toward',\n",
       "  '<unk>',\n",
       "  'a',\n",
       "  'democratic',\n",
       "  'nation',\n",
       "  'we',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'at',\n",
       "  'the',\n",
       "  'end',\n",
       "  'of',\n",
       "  'this',\n",
       "  '<unk>',\n",
       "  'meeting',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'syria',\n",
       "  'to',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  'democratic',\n",
       "  'nation',\n",
       "  'where',\n",
       "  'all',\n",
       "  'citizens',\n",
       "  'are',\n",
       "  '<unk>',\n",
       "  'by',\n",
       "  '<unk>',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'the',\n",
       "  'future',\n",
       "  'of',\n",
       "  'their',\n",
       "  'country',\n",
       "  '<unk>',\n",
       "  'president',\n",
       "  '<unk>',\n",
       "  'al',\n",
       "  '<unk>',\n",
       "  'said',\n",
       "  'in',\n",
       "  '<unk>',\n",
       "  '<end>'],\n",
       " ['<begin>',\n",
       "  'cnn',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'at',\n",
       "  'a',\n",
       "  'syrian',\n",
       "  'government',\n",
       "  '<unk>',\n",
       "  'national',\n",
       "  '<unk>',\n",
       "  'meeting',\n",
       "  'sunday',\n",
       "  'criticized',\n",
       "  'recent',\n",
       "  '<unk>',\n",
       "  'by',\n",
       "  'the',\n",
       "  \"country's\",\n",
       "  'security',\n",
       "  'forces',\n",
       "  '<unk>',\n",
       "  'for',\n",
       "  'an',\n",
       "  'end',\n",
       "  'to',\n",
       "  'violence',\n",
       "  'against',\n",
       "  'protesters',\n",
       "  \"syria's\",\n",
       "  '<unk>',\n",
       "  'president',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'university',\n",
       "  'meeting',\n",
       "  'between',\n",
       "  'officials',\n",
       "  'and',\n",
       "  'members',\n",
       "  'of',\n",
       "  'the',\n",
       "  'opposition',\n",
       "  'as',\n",
       "  'a',\n",
       "  'step',\n",
       "  'toward',\n",
       "  '<unk>',\n",
       "  'a',\n",
       "  'democratic',\n",
       "  'nation',\n",
       "  'we',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'at',\n",
       "  'the',\n",
       "  'end',\n",
       "  'of',\n",
       "  'this',\n",
       "  '<unk>',\n",
       "  'meeting',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'syria',\n",
       "  'to',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  'democratic',\n",
       "  'nation',\n",
       "  'where',\n",
       "  'all',\n",
       "  'citizens',\n",
       "  'are',\n",
       "  '<unk>',\n",
       "  'by',\n",
       "  '<unk>',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'the',\n",
       "  'future',\n",
       "  'of',\n",
       "  'their',\n",
       "  'country',\n",
       "  '<unk>',\n",
       "  'president',\n",
       "  '<unk>',\n",
       "  'al',\n",
       "  '<unk>',\n",
       "  'said',\n",
       "  'in',\n",
       "  '<unk>',\n",
       "  '<end>'],\n",
       " ['<begin>',\n",
       "  'cnn',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'at',\n",
       "  'a',\n",
       "  'syrian',\n",
       "  'government',\n",
       "  '<unk>',\n",
       "  'national',\n",
       "  '<unk>',\n",
       "  'meeting',\n",
       "  'sunday',\n",
       "  'criticized',\n",
       "  'recent',\n",
       "  '<unk>',\n",
       "  'by',\n",
       "  'the',\n",
       "  \"country's\",\n",
       "  'security',\n",
       "  'forces',\n",
       "  '<unk>',\n",
       "  'for',\n",
       "  'an',\n",
       "  'end',\n",
       "  'to',\n",
       "  'violence',\n",
       "  'against',\n",
       "  'protesters',\n",
       "  \"syria's\",\n",
       "  '<unk>',\n",
       "  'president',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'university',\n",
       "  'meeting',\n",
       "  'between',\n",
       "  'officials',\n",
       "  'and',\n",
       "  'members',\n",
       "  'of',\n",
       "  'the',\n",
       "  'opposition',\n",
       "  'as',\n",
       "  'a',\n",
       "  'step',\n",
       "  'toward',\n",
       "  '<unk>',\n",
       "  'a',\n",
       "  'democratic',\n",
       "  'nation',\n",
       "  'we',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'at',\n",
       "  'the',\n",
       "  'end',\n",
       "  'of',\n",
       "  'this',\n",
       "  '<unk>',\n",
       "  'meeting',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'syria',\n",
       "  'to',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  'democratic',\n",
       "  'nation',\n",
       "  'where',\n",
       "  'all',\n",
       "  'citizens',\n",
       "  'are',\n",
       "  '<unk>',\n",
       "  'by',\n",
       "  '<unk>',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'of',\n",
       "  'the',\n",
       "  'future',\n",
       "  'of',\n",
       "  'their',\n",
       "  'country',\n",
       "  '<unk>',\n",
       "  'president',\n",
       "  '<unk>',\n",
       "  'al',\n",
       "  '<unk>',\n",
       "  'said',\n",
       "  'in',\n",
       "  '<unk>',\n",
       "  '<end>'],\n",
       " ['<begin>',\n",
       "  'new',\n",
       "  'york',\n",
       "  'cnn',\n",
       "  '<unk>',\n",
       "  'florida',\n",
       "  'state',\n",
       "  'attorney',\n",
       "  '<unk>',\n",
       "  'corey',\n",
       "  'has',\n",
       "  'made',\n",
       "  'it',\n",
       "  'clear',\n",
       "  'that',\n",
       "  'she',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  '<unk>',\n",
       "  'whether',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'will',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'death',\n",
       "  'of',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'as',\n",
       "  '<unk>',\n",
       "  'her',\n",
       "  'decision',\n",
       "  'not',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  'the',\n",
       "  'grand',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'on',\n",
       "  '<unk>',\n",
       "  '10',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  'family',\n",
       "  'was',\n",
       "  'also',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'corey',\n",
       "  'would',\n",
       "  'make',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'decision',\n",
       "  'but',\n",
       "  'the',\n",
       "  'question',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'tuesday',\n",
       "  'in',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'george',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'during',\n",
       "  'a',\n",
       "  'news',\n",
       "  'conference',\n",
       "  '<end>'],\n",
       " ['<begin>',\n",
       "  'new',\n",
       "  'york',\n",
       "  'cnn',\n",
       "  '<unk>',\n",
       "  'florida',\n",
       "  'state',\n",
       "  'attorney',\n",
       "  '<unk>',\n",
       "  'corey',\n",
       "  'has',\n",
       "  'made',\n",
       "  'it',\n",
       "  'clear',\n",
       "  'that',\n",
       "  'she',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  '<unk>',\n",
       "  'whether',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'will',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'death',\n",
       "  'of',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'as',\n",
       "  '<unk>',\n",
       "  'her',\n",
       "  'decision',\n",
       "  'not',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  'the',\n",
       "  'grand',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'on',\n",
       "  '<unk>',\n",
       "  '10',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  'family',\n",
       "  'was',\n",
       "  'also',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'corey',\n",
       "  'would',\n",
       "  'make',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'decision',\n",
       "  'but',\n",
       "  'the',\n",
       "  'question',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'tuesday',\n",
       "  'in',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'george',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'during',\n",
       "  'a',\n",
       "  'news',\n",
       "  'conference',\n",
       "  '<end>'],\n",
       " ['<begin>',\n",
       "  'new',\n",
       "  'york',\n",
       "  'cnn',\n",
       "  '<unk>',\n",
       "  'florida',\n",
       "  'state',\n",
       "  'attorney',\n",
       "  '<unk>',\n",
       "  'corey',\n",
       "  'has',\n",
       "  'made',\n",
       "  'it',\n",
       "  'clear',\n",
       "  'that',\n",
       "  'she',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  '<unk>',\n",
       "  'whether',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'will',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'death',\n",
       "  'of',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'as',\n",
       "  '<unk>',\n",
       "  'her',\n",
       "  'decision',\n",
       "  'not',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  'the',\n",
       "  'grand',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'on',\n",
       "  '<unk>',\n",
       "  '10',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  'family',\n",
       "  'was',\n",
       "  'also',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'corey',\n",
       "  'would',\n",
       "  'make',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'decision',\n",
       "  'but',\n",
       "  'the',\n",
       "  'question',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'tuesday',\n",
       "  'in',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'george',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'during',\n",
       "  'a',\n",
       "  'news',\n",
       "  'conference',\n",
       "  '<end>'],\n",
       " ['<begin>',\n",
       "  'new',\n",
       "  'york',\n",
       "  'cnn',\n",
       "  '<unk>',\n",
       "  'florida',\n",
       "  'state',\n",
       "  'attorney',\n",
       "  '<unk>',\n",
       "  'corey',\n",
       "  'has',\n",
       "  'made',\n",
       "  'it',\n",
       "  'clear',\n",
       "  'that',\n",
       "  'she',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  '<unk>',\n",
       "  'whether',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'will',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'death',\n",
       "  'of',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'as',\n",
       "  '<unk>',\n",
       "  'her',\n",
       "  'decision',\n",
       "  'not',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  'the',\n",
       "  'grand',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'on',\n",
       "  '<unk>',\n",
       "  '10',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  'family',\n",
       "  'was',\n",
       "  'also',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'corey',\n",
       "  'would',\n",
       "  'make',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'decision',\n",
       "  'but',\n",
       "  'the',\n",
       "  'question',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'tuesday',\n",
       "  'in',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'george',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'during',\n",
       "  'a',\n",
       "  'news',\n",
       "  'conference',\n",
       "  '<end>'],\n",
       " ['<begin>',\n",
       "  'new',\n",
       "  'york',\n",
       "  'cnn',\n",
       "  '<unk>',\n",
       "  'florida',\n",
       "  'state',\n",
       "  'attorney',\n",
       "  '<unk>',\n",
       "  'corey',\n",
       "  'has',\n",
       "  'made',\n",
       "  'it',\n",
       "  'clear',\n",
       "  'that',\n",
       "  'she',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  '<unk>',\n",
       "  'whether',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'will',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'death',\n",
       "  'of',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'as',\n",
       "  '<unk>',\n",
       "  'her',\n",
       "  'decision',\n",
       "  'not',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  'the',\n",
       "  'grand',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'on',\n",
       "  '<unk>',\n",
       "  '10',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  'family',\n",
       "  'was',\n",
       "  'also',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'corey',\n",
       "  'would',\n",
       "  'make',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'decision',\n",
       "  'but',\n",
       "  'the',\n",
       "  'question',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'tuesday',\n",
       "  'in',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'george',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'during',\n",
       "  'a',\n",
       "  'news',\n",
       "  'conference',\n",
       "  '<end>'],\n",
       " ['<begin>',\n",
       "  'new',\n",
       "  'york',\n",
       "  'cnn',\n",
       "  '<unk>',\n",
       "  'florida',\n",
       "  'state',\n",
       "  'attorney',\n",
       "  '<unk>',\n",
       "  'corey',\n",
       "  'has',\n",
       "  'made',\n",
       "  'it',\n",
       "  'clear',\n",
       "  'that',\n",
       "  'she',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  '<unk>',\n",
       "  'whether',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'will',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'death',\n",
       "  'of',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'as',\n",
       "  '<unk>',\n",
       "  'her',\n",
       "  'decision',\n",
       "  'not',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  'the',\n",
       "  'grand',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'on',\n",
       "  '<unk>',\n",
       "  '10',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  'family',\n",
       "  'was',\n",
       "  'also',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'corey',\n",
       "  'would',\n",
       "  'make',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'decision',\n",
       "  'but',\n",
       "  'the',\n",
       "  'question',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'tuesday',\n",
       "  'in',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'george',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'during',\n",
       "  'a',\n",
       "  'news',\n",
       "  'conference',\n",
       "  '<end>'],\n",
       " ['<begin>',\n",
       "  'new',\n",
       "  'york',\n",
       "  'cnn',\n",
       "  '<unk>',\n",
       "  'florida',\n",
       "  'state',\n",
       "  'attorney',\n",
       "  '<unk>',\n",
       "  'corey',\n",
       "  'has',\n",
       "  'made',\n",
       "  'it',\n",
       "  'clear',\n",
       "  'that',\n",
       "  'she',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  '<unk>',\n",
       "  'whether',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'will',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'death',\n",
       "  'of',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'as',\n",
       "  '<unk>',\n",
       "  'her',\n",
       "  'decision',\n",
       "  'not',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  'the',\n",
       "  'grand',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'on',\n",
       "  '<unk>',\n",
       "  '10',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  'family',\n",
       "  'was',\n",
       "  'also',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'corey',\n",
       "  'would',\n",
       "  'make',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'decision',\n",
       "  'but',\n",
       "  'the',\n",
       "  'question',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'tuesday',\n",
       "  'in',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'george',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'during',\n",
       "  'a',\n",
       "  'news',\n",
       "  'conference',\n",
       "  '<end>'],\n",
       " ['<begin>',\n",
       "  'new',\n",
       "  'york',\n",
       "  'cnn',\n",
       "  '<unk>',\n",
       "  'florida',\n",
       "  'state',\n",
       "  'attorney',\n",
       "  '<unk>',\n",
       "  'corey',\n",
       "  'has',\n",
       "  'made',\n",
       "  'it',\n",
       "  'clear',\n",
       "  'that',\n",
       "  'she',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  '<unk>',\n",
       "  'whether',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'will',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'death',\n",
       "  'of',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'as',\n",
       "  '<unk>',\n",
       "  'her',\n",
       "  'decision',\n",
       "  'not',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  'the',\n",
       "  'grand',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'on',\n",
       "  '<unk>',\n",
       "  '10',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  'family',\n",
       "  'was',\n",
       "  'also',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'corey',\n",
       "  'would',\n",
       "  'make',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'decision',\n",
       "  'but',\n",
       "  'the',\n",
       "  'question',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'tuesday',\n",
       "  'in',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'george',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'during',\n",
       "  'a',\n",
       "  'news',\n",
       "  'conference',\n",
       "  '<end>'],\n",
       " ['<begin>',\n",
       "  'new',\n",
       "  'york',\n",
       "  'cnn',\n",
       "  '<unk>',\n",
       "  'florida',\n",
       "  'state',\n",
       "  'attorney',\n",
       "  '<unk>',\n",
       "  'corey',\n",
       "  'has',\n",
       "  'made',\n",
       "  'it',\n",
       "  'clear',\n",
       "  'that',\n",
       "  'she',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  '<unk>',\n",
       "  'whether',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'will',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'death',\n",
       "  'of',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'as',\n",
       "  '<unk>',\n",
       "  'her',\n",
       "  'decision',\n",
       "  'not',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  'the',\n",
       "  'grand',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'on',\n",
       "  '<unk>',\n",
       "  '10',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  'family',\n",
       "  'was',\n",
       "  'also',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'corey',\n",
       "  'would',\n",
       "  'make',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'decision',\n",
       "  'but',\n",
       "  'the',\n",
       "  'question',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'tuesday',\n",
       "  'in',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'george',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'during',\n",
       "  'a',\n",
       "  'news',\n",
       "  'conference',\n",
       "  '<end>'],\n",
       " ['<begin>',\n",
       "  'new',\n",
       "  'york',\n",
       "  'cnn',\n",
       "  '<unk>',\n",
       "  'florida',\n",
       "  'state',\n",
       "  'attorney',\n",
       "  '<unk>',\n",
       "  'corey',\n",
       "  'has',\n",
       "  'made',\n",
       "  'it',\n",
       "  'clear',\n",
       "  'that',\n",
       "  'she',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  '<unk>',\n",
       "  'whether',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'will',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'death',\n",
       "  'of',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'as',\n",
       "  '<unk>',\n",
       "  'her',\n",
       "  'decision',\n",
       "  'not',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  'the',\n",
       "  'grand',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'on',\n",
       "  '<unk>',\n",
       "  '10',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  'family',\n",
       "  'was',\n",
       "  'also',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'corey',\n",
       "  'would',\n",
       "  'make',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'decision',\n",
       "  'but',\n",
       "  'the',\n",
       "  'question',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'tuesday',\n",
       "  'in',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'george',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'during',\n",
       "  'a',\n",
       "  'news',\n",
       "  'conference',\n",
       "  '<end>'],\n",
       " ['<begin>',\n",
       "  'new',\n",
       "  'york',\n",
       "  'cnn',\n",
       "  '<unk>',\n",
       "  'florida',\n",
       "  'state',\n",
       "  'attorney',\n",
       "  '<unk>',\n",
       "  'corey',\n",
       "  'has',\n",
       "  'made',\n",
       "  'it',\n",
       "  'clear',\n",
       "  'that',\n",
       "  'she',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  '<unk>',\n",
       "  'whether',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'will',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'death',\n",
       "  'of',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'as',\n",
       "  '<unk>',\n",
       "  'her',\n",
       "  'decision',\n",
       "  'not',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  'the',\n",
       "  'grand',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'on',\n",
       "  '<unk>',\n",
       "  '10',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  'family',\n",
       "  'was',\n",
       "  'also',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'corey',\n",
       "  'would',\n",
       "  'make',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'decision',\n",
       "  'but',\n",
       "  'the',\n",
       "  'question',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'tuesday',\n",
       "  'in',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'george',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'during',\n",
       "  'a',\n",
       "  'news',\n",
       "  'conference',\n",
       "  '<end>'],\n",
       " ['<begin>',\n",
       "  'new',\n",
       "  'york',\n",
       "  'cnn',\n",
       "  '<unk>',\n",
       "  'florida',\n",
       "  'state',\n",
       "  'attorney',\n",
       "  '<unk>',\n",
       "  'corey',\n",
       "  'has',\n",
       "  'made',\n",
       "  'it',\n",
       "  'clear',\n",
       "  'that',\n",
       "  'she',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  '<unk>',\n",
       "  'whether',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'will',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'death',\n",
       "  'of',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'as',\n",
       "  '<unk>',\n",
       "  'her',\n",
       "  'decision',\n",
       "  'not',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  'the',\n",
       "  'grand',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'on',\n",
       "  '<unk>',\n",
       "  '10',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  'family',\n",
       "  'was',\n",
       "  'also',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'corey',\n",
       "  'would',\n",
       "  'make',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'decision',\n",
       "  'but',\n",
       "  'the',\n",
       "  'question',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'tuesday',\n",
       "  'in',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'george',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'during',\n",
       "  'a',\n",
       "  'news',\n",
       "  'conference',\n",
       "  '<end>'],\n",
       " ['<begin>',\n",
       "  'new',\n",
       "  'york',\n",
       "  'cnn',\n",
       "  '<unk>',\n",
       "  'florida',\n",
       "  'state',\n",
       "  'attorney',\n",
       "  '<unk>',\n",
       "  'corey',\n",
       "  'has',\n",
       "  'made',\n",
       "  'it',\n",
       "  'clear',\n",
       "  'that',\n",
       "  'she',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  '<unk>',\n",
       "  'whether',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'will',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'death',\n",
       "  'of',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'as',\n",
       "  '<unk>',\n",
       "  'her',\n",
       "  'decision',\n",
       "  'not',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  'the',\n",
       "  'grand',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'on',\n",
       "  '<unk>',\n",
       "  '10',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  'family',\n",
       "  'was',\n",
       "  'also',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'corey',\n",
       "  'would',\n",
       "  'make',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'decision',\n",
       "  'but',\n",
       "  'the',\n",
       "  'question',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'tuesday',\n",
       "  'in',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'george',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'during',\n",
       "  'a',\n",
       "  'news',\n",
       "  'conference',\n",
       "  '<end>'],\n",
       " ['<begin>',\n",
       "  'new',\n",
       "  'york',\n",
       "  'cnn',\n",
       "  '<unk>',\n",
       "  'florida',\n",
       "  'state',\n",
       "  'attorney',\n",
       "  '<unk>',\n",
       "  'corey',\n",
       "  'has',\n",
       "  'made',\n",
       "  'it',\n",
       "  'clear',\n",
       "  'that',\n",
       "  'she',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  '<unk>',\n",
       "  'whether',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'will',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'death',\n",
       "  'of',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'as',\n",
       "  '<unk>',\n",
       "  'her',\n",
       "  'decision',\n",
       "  'not',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  'the',\n",
       "  'grand',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'on',\n",
       "  '<unk>',\n",
       "  '10',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  'family',\n",
       "  'was',\n",
       "  'also',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'corey',\n",
       "  'would',\n",
       "  'make',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'decision',\n",
       "  'but',\n",
       "  'the',\n",
       "  'question',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'tuesday',\n",
       "  'in',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'george',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'during',\n",
       "  'a',\n",
       "  'news',\n",
       "  'conference',\n",
       "  '<end>'],\n",
       " ['<begin>',\n",
       "  'new',\n",
       "  'york',\n",
       "  'cnn',\n",
       "  '<unk>',\n",
       "  'florida',\n",
       "  'state',\n",
       "  'attorney',\n",
       "  '<unk>',\n",
       "  'corey',\n",
       "  'has',\n",
       "  'made',\n",
       "  'it',\n",
       "  'clear',\n",
       "  'that',\n",
       "  'she',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  '<unk>',\n",
       "  'whether',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'will',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'death',\n",
       "  'of',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'as',\n",
       "  '<unk>',\n",
       "  'her',\n",
       "  'decision',\n",
       "  'not',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  'the',\n",
       "  'grand',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'on',\n",
       "  '<unk>',\n",
       "  '10',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  'family',\n",
       "  'was',\n",
       "  'also',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'corey',\n",
       "  'would',\n",
       "  'make',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'decision',\n",
       "  'but',\n",
       "  'the',\n",
       "  'question',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'tuesday',\n",
       "  'in',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'george',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'during',\n",
       "  'a',\n",
       "  'news',\n",
       "  'conference',\n",
       "  '<end>'],\n",
       " ['<begin>',\n",
       "  'new',\n",
       "  'york',\n",
       "  'cnn',\n",
       "  '<unk>',\n",
       "  'florida',\n",
       "  'state',\n",
       "  'attorney',\n",
       "  '<unk>',\n",
       "  'corey',\n",
       "  'has',\n",
       "  'made',\n",
       "  'it',\n",
       "  'clear',\n",
       "  'that',\n",
       "  'she',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  '<unk>',\n",
       "  'whether',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'will',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'death',\n",
       "  'of',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'as',\n",
       "  '<unk>',\n",
       "  'her',\n",
       "  'decision',\n",
       "  'not',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  'the',\n",
       "  'grand',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'on',\n",
       "  '<unk>',\n",
       "  '10',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  'family',\n",
       "  'was',\n",
       "  'also',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'corey',\n",
       "  'would',\n",
       "  'make',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'decision',\n",
       "  'but',\n",
       "  'the',\n",
       "  'question',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'tuesday',\n",
       "  'in',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'george',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'during',\n",
       "  'a',\n",
       "  'news',\n",
       "  'conference',\n",
       "  '<end>'],\n",
       " ['<begin>',\n",
       "  'new',\n",
       "  'york',\n",
       "  'cnn',\n",
       "  '<unk>',\n",
       "  'florida',\n",
       "  'state',\n",
       "  'attorney',\n",
       "  '<unk>',\n",
       "  'corey',\n",
       "  'has',\n",
       "  'made',\n",
       "  'it',\n",
       "  'clear',\n",
       "  'that',\n",
       "  'she',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  '<unk>',\n",
       "  'whether',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'will',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'in',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'death',\n",
       "  'of',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'as',\n",
       "  '<unk>',\n",
       "  'her',\n",
       "  'decision',\n",
       "  'not',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  'the',\n",
       "  'grand',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  '<unk>',\n",
       "  'on',\n",
       "  '<unk>',\n",
       "  '10',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'martin',\n",
       "  'family',\n",
       "  'was',\n",
       "  'also',\n",
       "  '<unk>',\n",
       "  'that',\n",
       "  'corey',\n",
       "  'would',\n",
       "  'make',\n",
       "  'the',\n",
       "  '<unk>',\n",
       "  'decision',\n",
       "  'but',\n",
       "  'the',\n",
       "  'question',\n",
       "  '<unk>',\n",
       "  'will',\n",
       "  'george',\n",
       "  'zimmerman',\n",
       "  'be',\n",
       "  '<unk>',\n",
       "  'tuesday',\n",
       "  'in',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'george',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'and',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'during',\n",
       "  'a',\n",
       "  'news',\n",
       "  'conference',\n",
       "  '<end>']]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[index_to_word.get(i, '<pad>') for i in x] for x in x1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only one per row\n",
    "import numpy as np\n",
    "ys = np.argwhere(y[0] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <begin> new\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <begin> new <unk> <unk> syria's ambassador to\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <begin> new <unk> <unk> syria's ambassador to <unk> an attack on its\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <begin> <unk> <unk>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <begin> <unk> <unk> the state attorney has made\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <begin> <unk> <unk> the state attorney has made it clear that she <unk>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <begin> <unk> <unk> the state attorney has made it clear that she <unk> will <unk> on charges <end>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for j in range(0, len(y), 5):\n",
    "    ys = np.argwhere(y[j] == 1)\n",
    "    assert len(ys) == len({row for row, idx in ys})\n",
    "    print(' '.join(index_to_word[idx] for row, idx in ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_HEADS = 8\n",
    "N_LAYERS = 6\n",
    "D_MODEL = 64*N_HEADS\n",
    "VOCAB_SIZE = TOKENIZER.num_words\n",
    "WARMUP_STEPS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_gen = BatchGenerator(\n",
    "    files=FILES,\n",
    "    tokenizer=TOKENIZER,\n",
    "    maxlen=MAX_SEQUENCE_LEN,\n",
    "    batch_size=32,\n",
    "    begin_token=BEGIN_TOKEN,\n",
    "    end_token=END_TOKEN).generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over batch generator until we hit the end of the epoch\n",
    "# to calculate number of batches in epoch and compute some\n",
    "# stats along the way\n",
    "steps_per_epoch = 0\n",
    "for batch in batch_gen:\n",
    "    if batch is None:\n",
    "        break\n",
    "    steps_per_epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps per epoch 23\n"
     ]
    }
   ],
   "source": [
    "print('steps per epoch', steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = (X for X in batch_gen if not X == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TerminateOnNaN\n",
    "callbacks = [TerminateOnNaN()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Transformer\n",
    "model = Transformer(\n",
    "        n_heads=N_HEADS, encoder_layers=N_LAYERS, decoder_layers=N_LAYERS,\n",
    "        d_model=D_MODEL, vocab_size=VOCAB_SIZE, sequence_len=MAX_SEQUENCE_LEN,\n",
    "        layer_normalization=True, dropout=True,\n",
    "        residual_connections=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 100, 512)     307200      encoder_input[0][0]              \n",
      "                                                                 decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "positional_encoding_1 (Position (None, 100, 512)     0           embedding[0][0]                  \n",
      "                                                                 embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_scalar (Scalar)       (None, 100, 512)     0           positional_encoding_1[0][0]      \n",
      "                                                                 positional_encoding_1[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100, 512)     0           embedding_scalar[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer1_mha (MultiHeadAt (None, 100, 512)     262144      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 100, 512)     0           encoder_layer1_mha[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer1_residual1 (Add)  (None, 100, 512)     0           dropout_1[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer1_layernorm1 (Laye (None, 100, 512)     102400      encoder_layer1_residual1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer1_ffn1 (Dense)     (None, 100, 512)     262656      encoder_layer1_layernorm1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer1_ffn2 (Dense)     (None, 100, 512)     262656      encoder_layer1_ffn1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 100, 512)     0           encoder_layer1_ffn2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer1_residual2 (Add)  (None, 100, 512)     0           encoder_layer1_layernorm1[0][0]  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer1_layernorm2 (Laye (None, 100, 512)     102400      encoder_layer1_residual2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer2_mha (MultiHeadAt (None, 100, 512)     262144      encoder_layer1_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 100, 512)     0           encoder_layer2_mha[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer2_residual1 (Add)  (None, 100, 512)     0           encoder_layer1_layernorm2[0][0]  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer2_layernorm1 (Laye (None, 100, 512)     102400      encoder_layer2_residual1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer2_ffn1 (Dense)     (None, 100, 512)     262656      encoder_layer2_layernorm1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer2_ffn2 (Dense)     (None, 100, 512)     262656      encoder_layer2_ffn1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 100, 512)     0           encoder_layer2_ffn2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer2_residual2 (Add)  (None, 100, 512)     0           encoder_layer2_layernorm1[0][0]  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer2_layernorm2 (Laye (None, 100, 512)     102400      encoder_layer2_residual2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer3_mha (MultiHeadAt (None, 100, 512)     262144      encoder_layer2_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 100, 512)     0           encoder_layer3_mha[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer3_residual1 (Add)  (None, 100, 512)     0           encoder_layer2_layernorm2[0][0]  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer3_layernorm1 (Laye (None, 100, 512)     102400      encoder_layer3_residual1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer3_ffn1 (Dense)     (None, 100, 512)     262656      encoder_layer3_layernorm1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer3_ffn2 (Dense)     (None, 100, 512)     262656      encoder_layer3_ffn1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 100, 512)     0           encoder_layer3_ffn2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer3_residual2 (Add)  (None, 100, 512)     0           encoder_layer3_layernorm1[0][0]  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer3_layernorm2 (Laye (None, 100, 512)     102400      encoder_layer3_residual2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer4_mha (MultiHeadAt (None, 100, 512)     262144      encoder_layer3_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 100, 512)     0           encoder_layer4_mha[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer4_residual1 (Add)  (None, 100, 512)     0           encoder_layer3_layernorm2[0][0]  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer4_layernorm1 (Laye (None, 100, 512)     102400      encoder_layer4_residual1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer4_ffn1 (Dense)     (None, 100, 512)     262656      encoder_layer4_layernorm1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer4_ffn2 (Dense)     (None, 100, 512)     262656      encoder_layer4_ffn1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 100, 512)     0           encoder_layer4_ffn2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer4_residual2 (Add)  (None, 100, 512)     0           encoder_layer4_layernorm1[0][0]  \n",
      "                                                                 dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer4_layernorm2 (Laye (None, 100, 512)     102400      encoder_layer4_residual2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer5_mha (MultiHeadAt (None, 100, 512)     262144      encoder_layer4_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 100, 512)     0           encoder_layer5_mha[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer5_residual1 (Add)  (None, 100, 512)     0           encoder_layer4_layernorm2[0][0]  \n",
      "                                                                 dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer5_layernorm1 (Laye (None, 100, 512)     102400      encoder_layer5_residual1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer5_ffn1 (Dense)     (None, 100, 512)     262656      encoder_layer5_layernorm1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer5_ffn2 (Dense)     (None, 100, 512)     262656      encoder_layer5_ffn1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 100, 512)     0           encoder_layer5_ffn2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer5_residual2 (Add)  (None, 100, 512)     0           encoder_layer5_layernorm1[0][0]  \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer5_layernorm2 (Laye (None, 100, 512)     102400      encoder_layer5_residual2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer6_mha (MultiHeadAt (None, 100, 512)     262144      encoder_layer5_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 100, 512)     0           encoder_layer6_mha[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer6_residual1 (Add)  (None, 100, 512)     0           encoder_layer5_layernorm2[0][0]  \n",
      "                                                                 dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer6_layernorm1 (Laye (None, 100, 512)     102400      encoder_layer6_residual1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100, 512)     0           embedding_scalar[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer6_ffn1 (Dense)     (None, 100, 512)     262656      encoder_layer6_layernorm1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer1_mha1 (MultiHeadA (None, 100, 512)     262144      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer6_ffn2 (Dense)     (None, 100, 512)     262656      encoder_layer6_ffn1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 100, 512)     0           decoder_layer1_mha1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 100, 512)     0           encoder_layer6_ffn2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer1_residual1 (Add)  (None, 100, 512)     0           dropout_2[0][0]                  \n",
      "                                                                 dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer6_residual2 (Add)  (None, 100, 512)     0           encoder_layer6_layernorm1[0][0]  \n",
      "                                                                 dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer1_layernorm1 (Laye (None, 100, 512)     102400      decoder_layer1_residual1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer6_layernorm2 (Laye (None, 100, 512)     102400      encoder_layer6_residual2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer1_mha2 (MultiHeadA (None, 100, 512)     262144      decoder_layer1_layernorm1[0][0]  \n",
      "                                                                 encoder_layer6_layernorm2[0][0]  \n",
      "                                                                 encoder_layer6_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 100, 512)     0           decoder_layer1_mha2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer1_residual2 (Add)  (None, 100, 512)     0           decoder_layer1_layernorm1[0][0]  \n",
      "                                                                 dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer1_layernorm2 (Laye (None, 100, 512)     102400      decoder_layer1_residual2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer1_ffn1 (Dense)     (None, 100, 512)     262656      decoder_layer1_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer1_ffn2 (Dense)     (None, 100, 512)     262656      decoder_layer1_ffn1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 100, 512)     0           decoder_layer1_ffn2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer1_residual3 (Add)  (None, 100, 512)     0           decoder_layer1_layernorm2[0][0]  \n",
      "                                                                 dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer1_layernorm3 (Laye (None, 100, 512)     102400      decoder_layer1_residual3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer2_mha1 (MultiHeadA (None, 100, 512)     262144      decoder_layer1_layernorm3[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 100, 512)     0           decoder_layer2_mha1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer2_residual1 (Add)  (None, 100, 512)     0           decoder_layer1_layernorm3[0][0]  \n",
      "                                                                 dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer2_layernorm1 (Laye (None, 100, 512)     102400      decoder_layer2_residual1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer2_mha2 (MultiHeadA (None, 100, 512)     262144      decoder_layer2_layernorm1[0][0]  \n",
      "                                                                 encoder_layer6_layernorm2[0][0]  \n",
      "                                                                 encoder_layer6_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 100, 512)     0           decoder_layer2_mha2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer2_residual2 (Add)  (None, 100, 512)     0           decoder_layer2_layernorm1[0][0]  \n",
      "                                                                 dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer2_layernorm2 (Laye (None, 100, 512)     102400      decoder_layer2_residual2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer2_ffn1 (Dense)     (None, 100, 512)     262656      decoder_layer2_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer2_ffn2 (Dense)     (None, 100, 512)     262656      decoder_layer2_ffn1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 100, 512)     0           decoder_layer2_ffn2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer2_residual3 (Add)  (None, 100, 512)     0           decoder_layer2_layernorm2[0][0]  \n",
      "                                                                 dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer2_layernorm3 (Laye (None, 100, 512)     102400      decoder_layer2_residual3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer3_mha1 (MultiHeadA (None, 100, 512)     262144      decoder_layer2_layernorm3[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 100, 512)     0           decoder_layer3_mha1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer3_residual1 (Add)  (None, 100, 512)     0           decoder_layer2_layernorm3[0][0]  \n",
      "                                                                 dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer3_layernorm1 (Laye (None, 100, 512)     102400      decoder_layer3_residual1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer3_mha2 (MultiHeadA (None, 100, 512)     262144      decoder_layer3_layernorm1[0][0]  \n",
      "                                                                 encoder_layer6_layernorm2[0][0]  \n",
      "                                                                 encoder_layer6_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 100, 512)     0           decoder_layer3_mha2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer3_residual2 (Add)  (None, 100, 512)     0           decoder_layer3_layernorm1[0][0]  \n",
      "                                                                 dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer3_layernorm2 (Laye (None, 100, 512)     102400      decoder_layer3_residual2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer3_ffn1 (Dense)     (None, 100, 512)     262656      decoder_layer3_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer3_ffn2 (Dense)     (None, 100, 512)     262656      decoder_layer3_ffn1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 100, 512)     0           decoder_layer3_ffn2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer3_residual3 (Add)  (None, 100, 512)     0           decoder_layer3_layernorm2[0][0]  \n",
      "                                                                 dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer3_layernorm3 (Laye (None, 100, 512)     102400      decoder_layer3_residual3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer4_mha1 (MultiHeadA (None, 100, 512)     262144      decoder_layer3_layernorm3[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 100, 512)     0           decoder_layer4_mha1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer4_residual1 (Add)  (None, 100, 512)     0           decoder_layer3_layernorm3[0][0]  \n",
      "                                                                 dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer4_layernorm1 (Laye (None, 100, 512)     102400      decoder_layer4_residual1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer4_mha2 (MultiHeadA (None, 100, 512)     262144      decoder_layer4_layernorm1[0][0]  \n",
      "                                                                 encoder_layer6_layernorm2[0][0]  \n",
      "                                                                 encoder_layer6_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 100, 512)     0           decoder_layer4_mha2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer4_residual2 (Add)  (None, 100, 512)     0           decoder_layer4_layernorm1[0][0]  \n",
      "                                                                 dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer4_layernorm2 (Laye (None, 100, 512)     102400      decoder_layer4_residual2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer4_ffn1 (Dense)     (None, 100, 512)     262656      decoder_layer4_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer4_ffn2 (Dense)     (None, 100, 512)     262656      decoder_layer4_ffn1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 100, 512)     0           decoder_layer4_ffn2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer4_residual3 (Add)  (None, 100, 512)     0           decoder_layer4_layernorm2[0][0]  \n",
      "                                                                 dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer4_layernorm3 (Laye (None, 100, 512)     102400      decoder_layer4_residual3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer5_mha1 (MultiHeadA (None, 100, 512)     262144      decoder_layer4_layernorm3[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 100, 512)     0           decoder_layer5_mha1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer5_residual1 (Add)  (None, 100, 512)     0           decoder_layer4_layernorm3[0][0]  \n",
      "                                                                 dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer5_layernorm1 (Laye (None, 100, 512)     102400      decoder_layer5_residual1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer5_mha2 (MultiHeadA (None, 100, 512)     262144      decoder_layer5_layernorm1[0][0]  \n",
      "                                                                 encoder_layer6_layernorm2[0][0]  \n",
      "                                                                 encoder_layer6_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 100, 512)     0           decoder_layer5_mha2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer5_residual2 (Add)  (None, 100, 512)     0           decoder_layer5_layernorm1[0][0]  \n",
      "                                                                 dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer5_layernorm2 (Laye (None, 100, 512)     102400      decoder_layer5_residual2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer5_ffn1 (Dense)     (None, 100, 512)     262656      decoder_layer5_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer5_ffn2 (Dense)     (None, 100, 512)     262656      decoder_layer5_ffn1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 100, 512)     0           decoder_layer5_ffn2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer5_residual3 (Add)  (None, 100, 512)     0           decoder_layer5_layernorm2[0][0]  \n",
      "                                                                 dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer5_layernorm3 (Laye (None, 100, 512)     102400      decoder_layer5_residual3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer6_mha1 (MultiHeadA (None, 100, 512)     262144      decoder_layer5_layernorm3[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 100, 512)     0           decoder_layer6_mha1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer6_residual1 (Add)  (None, 100, 512)     0           decoder_layer5_layernorm3[0][0]  \n",
      "                                                                 dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer6_layernorm1 (Laye (None, 100, 512)     102400      decoder_layer6_residual1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer6_mha2 (MultiHeadA (None, 100, 512)     262144      decoder_layer6_layernorm1[0][0]  \n",
      "                                                                 encoder_layer6_layernorm2[0][0]  \n",
      "                                                                 encoder_layer6_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 100, 512)     0           decoder_layer6_mha2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer6_residual2 (Add)  (None, 100, 512)     0           decoder_layer6_layernorm1[0][0]  \n",
      "                                                                 dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer6_layernorm2 (Laye (None, 100, 512)     102400      decoder_layer6_residual2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer6_ffn1 (Dense)     (None, 100, 512)     262656      decoder_layer6_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer6_ffn2 (Dense)     (None, 100, 512)     262656      decoder_layer6_ffn1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 100, 512)     0           decoder_layer6_ffn2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer6_residual3 (Add)  (None, 100, 512)     0           decoder_layer6_layernorm2[0][0]  \n",
      "                                                                 dropout_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer6_layernorm3 (Laye (None, 100, 512)     102400      decoder_layer6_residual3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "shared_weights_1 (SharedWeights (None, 100, 600)     0           decoder_layer6_layernorm3[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 14,401,536\n",
      "Trainable params: 14,401,536\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras.backend as K\n",
    "# def loss(y_true, y_pred):\n",
    "#    return K.categorical_crossentropy(y_true[:,-1:,:], y_pred[:,-1:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 'categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRScheduler:\n",
    "    def __init__(self, d_model, warmup_steps):\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.epoch = 1\n",
    "\n",
    "    def lr(self, epoch):\n",
    "        lr = self.d_model**-.5 * min(self.epoch**-.5, epoch*(self.warmup_steps**-1.5))\n",
    "        self.epoch += 1\n",
    "        return lr\n",
    "lr_scheduler = LRScheduler(d_model, warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler\n",
    "# callbacks.append(LearningRateScheduler(lr_scheduler.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import adam\n",
    "model.compile(loss=loss, optimizer=adam(lr=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras import backend as K\n",
    "# old_lr = K.get_value(model.optimizer.lr)\n",
    "# K.set_value(model.optimizer.lr, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "83/83 [==============================] - 31s 373ms/step - loss: 6.1677\n",
      "Epoch 2/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 5.8673\n",
      "Epoch 3/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 5.6662\n",
      "Epoch 4/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 5.5196\n",
      "Epoch 5/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 5.4028\n",
      "Epoch 6/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 5.3272\n",
      "Epoch 7/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 5.2618\n",
      "Epoch 8/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 5.1847\n",
      "Epoch 9/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 5.1433\n",
      "Epoch 10/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 5.0959\n",
      "Epoch 11/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 5.0487\n",
      "Epoch 12/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 5.0119\n",
      "Epoch 13/1000\n",
      "83/83 [==============================] - 13s 160ms/step - loss: 4.9695\n",
      "Epoch 14/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.9409\n",
      "Epoch 15/1000\n",
      "83/83 [==============================] - 13s 159ms/step - loss: 4.8644\n",
      "Epoch 16/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.9381\n",
      "Epoch 17/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.7523\n",
      "Epoch 18/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.7778\n",
      "Epoch 19/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.8525\n",
      "Epoch 20/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.6316\n",
      "Epoch 21/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.6541\n",
      "Epoch 22/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.7223\n",
      "Epoch 23/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.6038\n",
      "Epoch 24/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.6639\n",
      "Epoch 25/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.6175\n",
      "Epoch 26/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.5357\n",
      "Epoch 27/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.6245\n",
      "Epoch 28/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.5503\n",
      "Epoch 29/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.4891\n",
      "Epoch 30/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.5050\n",
      "Epoch 31/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.5102\n",
      "Epoch 32/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.4990\n",
      "Epoch 33/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.4923\n",
      "Epoch 34/1000\n",
      "83/83 [==============================] - 13s 159ms/step - loss: 4.5345\n",
      "Epoch 35/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3690\n",
      "Epoch 36/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.4526\n",
      "Epoch 37/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.4524\n",
      "Epoch 38/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.4053\n",
      "Epoch 39/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.3895\n",
      "Epoch 40/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.5238\n",
      "Epoch 41/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2094\n",
      "Epoch 42/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.4143\n",
      "Epoch 43/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2779\n",
      "Epoch 44/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.4136\n",
      "Epoch 45/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2920\n",
      "Epoch 46/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3541\n",
      "Epoch 47/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3828\n",
      "Epoch 48/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.3871\n",
      "Epoch 49/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2390\n",
      "Epoch 50/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2908\n",
      "Epoch 51/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2624\n",
      "Epoch 52/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.3439\n",
      "Epoch 53/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3347\n",
      "Epoch 54/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.3655\n",
      "Epoch 55/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.0886\n",
      "Epoch 56/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2888\n",
      "Epoch 57/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2352\n",
      "Epoch 58/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3509\n",
      "Epoch 59/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2379\n",
      "Epoch 60/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3240\n",
      "Epoch 61/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.1985\n",
      "Epoch 62/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.4506\n",
      "Epoch 63/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.0884\n",
      "Epoch 64/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.4506\n",
      "Epoch 65/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.0927\n",
      "Epoch 66/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2018\n",
      "Epoch 67/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2463\n",
      "Epoch 68/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2406\n",
      "Epoch 69/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3683\n",
      "Epoch 70/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2238\n",
      "Epoch 71/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.4016\n",
      "Epoch 72/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.0137\n",
      "Epoch 73/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2813\n",
      "Epoch 74/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.3393\n",
      "Epoch 75/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2311\n",
      "Epoch 76/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.4127\n",
      "Epoch 77/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.0934\n",
      "Epoch 78/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2918\n",
      "Epoch 79/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.0894\n",
      "Epoch 80/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2935\n",
      "Epoch 81/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3435\n",
      "Epoch 82/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1755\n",
      "Epoch 83/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2911\n",
      "Epoch 84/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2112\n",
      "Epoch 85/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2978\n",
      "Epoch 86/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2425\n",
      "Epoch 87/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2352\n",
      "Epoch 88/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2159\n",
      "Epoch 89/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2880\n",
      "Epoch 90/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.4031\n",
      "Epoch 91/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1715\n",
      "Epoch 92/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.0801\n",
      "Epoch 93/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2768\n",
      "Epoch 94/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2480\n",
      "Epoch 95/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.3592\n",
      "Epoch 96/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.3641\n",
      "Epoch 97/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 3.9942\n",
      "Epoch 98/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2602\n",
      "Epoch 99/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.4024\n",
      "Epoch 100/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1989\n",
      "Epoch 101/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3688\n",
      "Epoch 102/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1511\n",
      "Epoch 103/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1294\n",
      "Epoch 104/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.3938\n",
      "Epoch 105/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.1227\n",
      "Epoch 106/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2554\n",
      "Epoch 107/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.3595\n",
      "Epoch 108/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.0706\n",
      "Epoch 109/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.5764\n",
      "Epoch 110/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1305\n",
      "Epoch 111/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2177\n",
      "Epoch 112/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3473\n",
      "Epoch 113/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.0442\n",
      "Epoch 114/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3725\n",
      "Epoch 115/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2368\n",
      "Epoch 116/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2264\n",
      "Epoch 117/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.4218\n",
      "Epoch 118/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.0895\n",
      "Epoch 119/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2254\n",
      "Epoch 120/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2378\n",
      "Epoch 121/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.4417\n",
      "Epoch 122/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1035\n",
      "Epoch 123/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.3073\n",
      "Epoch 124/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3336\n",
      "Epoch 125/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1365\n",
      "Epoch 126/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2699\n",
      "Epoch 127/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.3396\n",
      "Epoch 128/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3424\n",
      "Epoch 129/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.1642\n",
      "Epoch 130/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1855\n",
      "Epoch 131/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2222\n",
      "Epoch 132/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.3764\n",
      "Epoch 133/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3189\n",
      "Epoch 134/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1837\n",
      "Epoch 135/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2858\n",
      "Epoch 136/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1613\n",
      "Epoch 137/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3026\n",
      "Epoch 138/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3050\n",
      "Epoch 139/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2108\n",
      "Epoch 140/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.0977\n",
      "Epoch 141/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.4571\n",
      "Epoch 142/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.0992\n",
      "Epoch 143/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2682\n",
      "Epoch 144/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2863\n",
      "Epoch 145/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2756\n",
      "Epoch 146/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.1248\n",
      "Epoch 147/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3417\n",
      "Epoch 148/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2881\n",
      "Epoch 149/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3452\n",
      "Epoch 150/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2401\n",
      "Epoch 151/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.1431\n",
      "Epoch 152/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.4481\n",
      "Epoch 153/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1409\n",
      "Epoch 154/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.3410\n",
      "Epoch 155/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2525\n",
      "Epoch 156/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2590\n",
      "Epoch 157/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2025\n",
      "Epoch 158/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2214\n",
      "Epoch 159/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.4012\n",
      "Epoch 160/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2463\n",
      "Epoch 161/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1918\n",
      "Epoch 162/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2823\n",
      "Epoch 163/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.1110\n",
      "Epoch 164/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.4385\n",
      "Epoch 165/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2002\n",
      "Epoch 166/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3146\n",
      "Epoch 167/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2529\n",
      "Epoch 168/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2752\n",
      "Epoch 169/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2015\n",
      "Epoch 170/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3303\n",
      "Epoch 171/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2127\n",
      "Epoch 172/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1320\n",
      "Epoch 173/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2282\n",
      "Epoch 174/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2556\n",
      "Epoch 175/1000\n",
      "83/83 [==============================] - 13s 160ms/step - loss: 4.4499\n",
      "Epoch 176/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2699\n",
      "Epoch 177/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2184\n",
      "Epoch 178/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2474\n",
      "Epoch 179/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2564\n",
      "Epoch 180/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1656\n",
      "Epoch 181/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2240\n",
      "Epoch 182/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.4149\n",
      "Epoch 183/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2777\n",
      "Epoch 184/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2110\n",
      "Epoch 185/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2632\n",
      "Epoch 186/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2216\n",
      "Epoch 187/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2672\n",
      "Epoch 188/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2158\n",
      "Epoch 189/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2555\n",
      "Epoch 190/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3270\n",
      "Epoch 191/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3007\n",
      "Epoch 192/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2119\n",
      "Epoch 193/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2916\n",
      "Epoch 194/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2718\n",
      "Epoch 195/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2281\n",
      "Epoch 196/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.0842\n",
      "Epoch 197/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.4146\n",
      "Epoch 198/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.1248\n",
      "Epoch 199/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2627\n",
      "Epoch 200/1000\n",
      "83/83 [==============================] - 13s 159ms/step - loss: 4.4229\n",
      "Epoch 201/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2359\n",
      "Epoch 202/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2389\n",
      "Epoch 203/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2609\n",
      "Epoch 204/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2451\n",
      "Epoch 205/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2450\n",
      "Epoch 206/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2725\n",
      "Epoch 207/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2492\n",
      "Epoch 208/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.1863\n",
      "Epoch 209/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3198\n",
      "Epoch 210/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2798\n",
      "Epoch 211/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2503\n",
      "Epoch 212/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2404\n",
      "Epoch 213/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2164\n",
      "Epoch 214/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2943\n",
      "Epoch 215/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2522\n",
      "Epoch 216/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2642\n",
      "Epoch 217/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2521\n",
      "Epoch 218/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2559\n",
      "Epoch 219/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2588\n",
      "Epoch 220/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2523\n",
      "Epoch 221/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2504\n",
      "Epoch 222/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2601\n",
      "Epoch 223/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2590\n",
      "Epoch 224/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2443\n",
      "Epoch 225/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2784\n",
      "Epoch 226/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2356\n",
      "Epoch 227/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2865\n",
      "Epoch 228/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2238\n",
      "Epoch 229/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3129\n",
      "Epoch 230/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.1764\n",
      "Epoch 231/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3432\n",
      "Epoch 232/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1934\n",
      "Epoch 233/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2018\n",
      "Epoch 234/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2559\n",
      "Epoch 235/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2956\n",
      "Epoch 236/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2518\n",
      "Epoch 237/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2561\n",
      "Epoch 238/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2558\n",
      "Epoch 239/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2462\n",
      "Epoch 240/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2541\n",
      "Epoch 241/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2190\n",
      "Epoch 242/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3698\n",
      "Epoch 243/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1818\n",
      "Epoch 244/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1918\n",
      "Epoch 245/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2541\n",
      "Epoch 246/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.3218\n",
      "Epoch 247/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2329\n",
      "Epoch 248/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2424\n",
      "Epoch 249/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2569\n",
      "Epoch 250/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2799\n",
      "Epoch 251/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.3634\n",
      "Epoch 252/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.0987\n",
      "Epoch 253/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2534\n",
      "Epoch 254/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2480\n",
      "Epoch 255/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.3189\n",
      "Epoch 256/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1887\n",
      "Epoch 257/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2305\n",
      "Epoch 258/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2184\n",
      "Epoch 259/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2764\n",
      "Epoch 260/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2687\n",
      "Epoch 261/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2114\n",
      "Epoch 262/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.3656\n",
      "Epoch 263/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2235\n",
      "Epoch 264/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1978\n",
      "Epoch 265/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.4173\n",
      "Epoch 266/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.1548\n",
      "Epoch 267/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2583\n",
      "Epoch 268/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2685\n",
      "Epoch 269/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1734\n",
      "Epoch 270/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.3601\n",
      "Epoch 271/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1080\n",
      "Epoch 272/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3976\n",
      "Epoch 273/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2102\n",
      "Epoch 274/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.1035\n",
      "Epoch 275/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3022\n",
      "Epoch 276/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2336\n",
      "Epoch 277/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3557\n",
      "Epoch 278/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2518\n",
      "Epoch 279/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3177\n",
      "Epoch 280/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1018\n",
      "Epoch 281/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.4179\n",
      "Epoch 282/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.0017\n",
      "Epoch 283/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2495\n",
      "Epoch 284/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.3327\n",
      "Epoch 285/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.4617\n",
      "Epoch 286/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 3.9756\n",
      "Epoch 287/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.4251\n",
      "Epoch 288/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1203\n",
      "Epoch 289/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.3348\n",
      "Epoch 290/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1197\n",
      "Epoch 291/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2273\n",
      "Epoch 292/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.5381\n",
      "Epoch 293/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.0232\n",
      "Epoch 294/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.3440\n",
      "Epoch 295/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2758\n",
      "Epoch 296/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2068\n",
      "Epoch 297/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3359\n",
      "Epoch 298/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.1869\n",
      "Epoch 299/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2919\n",
      "Epoch 300/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2210\n",
      "Epoch 301/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.0798\n",
      "Epoch 302/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.4316\n",
      "Epoch 303/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2828\n",
      "Epoch 304/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 3.9993\n",
      "Epoch 305/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2769\n",
      "Epoch 306/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.3603\n",
      "Epoch 307/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2424\n",
      "Epoch 308/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.1627\n",
      "Epoch 309/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2711\n",
      "Epoch 310/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.5077\n",
      "Epoch 311/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.0156\n",
      "Epoch 312/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3638\n",
      "Epoch 313/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1687\n",
      "Epoch 314/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3242\n",
      "Epoch 315/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3336\n",
      "Epoch 316/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1508\n",
      "Epoch 317/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3471\n",
      "Epoch 318/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 3.9696\n",
      "Epoch 319/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.4415\n",
      "Epoch 320/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1586\n",
      "Epoch 321/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2525\n",
      "Epoch 322/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.1935\n",
      "Epoch 323/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1630\n",
      "Epoch 324/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.4311\n",
      "Epoch 325/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1790\n",
      "Epoch 326/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1711\n",
      "Epoch 327/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.3448\n",
      "Epoch 328/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.4123\n",
      "Epoch 329/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.0301\n",
      "Epoch 330/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.3216\n",
      "Epoch 331/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.3261\n",
      "Epoch 332/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2392\n",
      "Epoch 333/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2831\n",
      "Epoch 334/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1754\n",
      "Epoch 335/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.0390\n",
      "Epoch 336/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.4212\n",
      "Epoch 337/1000\n",
      "83/83 [==============================] - 13s 152ms/step - loss: 4.2683\n",
      "Epoch 338/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2090\n",
      "Epoch 339/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2031\n",
      "Epoch 340/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.3161\n",
      "Epoch 341/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3042\n",
      "Epoch 342/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1033\n",
      "Epoch 343/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.4024\n",
      "Epoch 344/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2404\n",
      "Epoch 345/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2137\n",
      "Epoch 346/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.0699\n",
      "Epoch 347/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.5142\n",
      "Epoch 348/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.0351\n",
      "Epoch 349/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.4868\n",
      "Epoch 350/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1080\n",
      "Epoch 351/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2500\n",
      "Epoch 352/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3161\n",
      "Epoch 353/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1662\n",
      "Epoch 354/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1424\n",
      "Epoch 355/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3618\n",
      "Epoch 356/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.3151\n",
      "Epoch 357/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1641\n",
      "Epoch 358/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2822\n",
      "Epoch 359/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2221\n",
      "Epoch 360/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1328\n",
      "Epoch 361/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.4114\n",
      "Epoch 362/1000\n",
      "83/83 [==============================] - 13s 159ms/step - loss: 4.1532\n",
      "Epoch 363/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3148\n",
      "Epoch 364/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.3216\n",
      "Epoch 365/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2368\n",
      "Epoch 366/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 3.9966\n",
      "Epoch 367/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.3687\n",
      "Epoch 368/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2227\n",
      "Epoch 369/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.3347\n",
      "Epoch 370/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3048\n",
      "Epoch 371/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1291\n",
      "Epoch 372/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1832\n",
      "Epoch 373/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.3676\n",
      "Epoch 374/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.1627\n",
      "Epoch 375/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2654\n",
      "Epoch 376/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2412\n",
      "Epoch 377/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2863\n",
      "Epoch 378/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2474\n",
      "Epoch 379/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2683\n",
      "Epoch 380/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2603\n",
      "Epoch 381/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.0876\n",
      "Epoch 382/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3498\n",
      "Epoch 383/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1534\n",
      "Epoch 384/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3944\n",
      "Epoch 385/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2010\n",
      "Epoch 386/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2862\n",
      "Epoch 387/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2597\n",
      "Epoch 388/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2519\n",
      "Epoch 389/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2110\n",
      "Epoch 390/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2593\n",
      "Epoch 391/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1078\n",
      "Epoch 392/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.3673\n",
      "Epoch 393/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2549\n",
      "Epoch 394/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2111\n",
      "Epoch 395/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2632\n",
      "Epoch 396/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2418\n",
      "Epoch 397/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.0752\n",
      "Epoch 398/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.4205\n",
      "Epoch 399/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1036\n",
      "Epoch 400/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.3721\n",
      "Epoch 401/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1657\n",
      "Epoch 402/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3168\n",
      "Epoch 403/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2726\n",
      "Epoch 404/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1052\n",
      "Epoch 405/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3260\n",
      "Epoch 406/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2707\n",
      "Epoch 407/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2827\n",
      "Epoch 408/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2493\n",
      "Epoch 409/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1722\n",
      "Epoch 410/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1991\n",
      "Epoch 411/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3205\n",
      "Epoch 412/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2270\n",
      "Epoch 413/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2836\n",
      "Epoch 414/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2337\n",
      "Epoch 415/1000\n",
      "83/83 [==============================] - 13s 159ms/step - loss: 4.2320\n",
      "Epoch 416/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2423\n",
      "Epoch 417/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2609\n",
      "Epoch 418/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1966\n",
      "Epoch 419/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2442\n",
      "Epoch 420/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2748\n",
      "Epoch 421/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2439\n",
      "Epoch 422/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.2156\n",
      "Epoch 423/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2690\n",
      "Epoch 424/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2492\n",
      "Epoch 425/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2429\n",
      "Epoch 426/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2508\n",
      "Epoch 427/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2675\n",
      "Epoch 428/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1912\n",
      "Epoch 429/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2644\n",
      "Epoch 430/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2691\n",
      "Epoch 431/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2198\n",
      "Epoch 432/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2586\n",
      "Epoch 433/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2333\n",
      "Epoch 434/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2381\n",
      "Epoch 435/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2368\n",
      "Epoch 436/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2438\n",
      "Epoch 437/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2386\n",
      "Epoch 438/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2397\n",
      "Epoch 439/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.2467\n",
      "Epoch 440/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2417\n",
      "Epoch 441/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2317\n",
      "Epoch 442/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2480\n",
      "Epoch 443/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2450\n",
      "Epoch 444/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2414\n",
      "Epoch 445/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.2490\n",
      "Epoch 446/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.2357\n",
      "Epoch 447/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2659\n",
      "Epoch 448/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2480\n",
      "Epoch 449/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2555\n",
      "Epoch 450/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2110\n",
      "Epoch 451/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2746\n",
      "Epoch 452/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2076\n",
      "Epoch 453/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2147\n",
      "Epoch 454/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2776\n",
      "Epoch 455/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2379\n",
      "Epoch 456/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1888\n",
      "Epoch 457/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2909\n",
      "Epoch 458/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2566\n",
      "Epoch 459/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3284\n",
      "Epoch 460/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1303\n",
      "Epoch 461/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2487\n",
      "Epoch 462/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1915\n",
      "Epoch 463/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3151\n",
      "Epoch 464/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1924\n",
      "Epoch 465/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2697\n",
      "Epoch 466/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.3768\n",
      "Epoch 467/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.0990\n",
      "Epoch 468/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2425\n",
      "Epoch 469/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3240\n",
      "Epoch 470/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1735\n",
      "Epoch 471/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2587\n",
      "Epoch 472/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2958\n",
      "Epoch 473/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.1297\n",
      "Epoch 474/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.3239\n",
      "Epoch 475/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2692\n",
      "Epoch 476/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3062\n",
      "Epoch 477/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2580\n",
      "Epoch 478/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1181\n",
      "Epoch 479/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3422\n",
      "Epoch 480/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.0932\n",
      "Epoch 481/1000\n",
      "83/83 [==============================] - 13s 152ms/step - loss: 4.3365\n",
      "Epoch 482/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.1630\n",
      "Epoch 483/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1989\n",
      "Epoch 484/1000\n",
      "83/83 [==============================] - 13s 152ms/step - loss: 4.3123\n",
      "Epoch 485/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1925\n",
      "Epoch 486/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1746\n",
      "Epoch 487/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2633\n",
      "Epoch 488/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.4220\n",
      "Epoch 489/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1091\n",
      "Epoch 490/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2391\n",
      "Epoch 491/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2482\n",
      "Epoch 492/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3767\n",
      "Epoch 493/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.0687\n",
      "Epoch 494/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.4135\n",
      "Epoch 495/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1194\n",
      "Epoch 496/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.3494\n",
      "Epoch 497/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.0899\n",
      "Epoch 498/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3214\n",
      "Epoch 499/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2423\n",
      "Epoch 500/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2277\n",
      "Epoch 501/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1867\n",
      "Epoch 502/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.3699\n",
      "Epoch 503/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1634\n",
      "Epoch 504/1000\n",
      "83/83 [==============================] - 13s 152ms/step - loss: 4.4325\n",
      "Epoch 505/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.0135\n",
      "Epoch 506/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2782\n",
      "Epoch 507/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2543\n",
      "Epoch 508/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.2315\n",
      "Epoch 509/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.4094\n",
      "Epoch 510/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.0194\n",
      "Epoch 511/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.3216\n",
      "Epoch 512/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2446\n",
      "Epoch 513/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2055\n",
      "Epoch 514/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1468\n",
      "Epoch 515/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.4239\n",
      "Epoch 516/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2543\n",
      "Epoch 517/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.1294\n",
      "Epoch 518/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2507\n",
      "Epoch 519/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2653\n",
      "Epoch 520/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2847\n",
      "Epoch 521/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1369\n",
      "Epoch 522/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.3152\n",
      "Epoch 523/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1716\n",
      "Epoch 524/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1906\n",
      "Epoch 525/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3384\n",
      "Epoch 526/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1797\n",
      "Epoch 527/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1589\n",
      "Epoch 528/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2708\n",
      "Epoch 529/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.3307\n",
      "Epoch 530/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.2156\n",
      "Epoch 531/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.2742\n",
      "Epoch 532/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2406\n",
      "Epoch 533/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2722\n",
      "Epoch 534/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1041\n",
      "Epoch 535/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.4131\n",
      "Epoch 536/1000\n",
      "83/83 [==============================] - 13s 159ms/step - loss: 4.2346\n",
      "Epoch 537/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1709\n",
      "Epoch 538/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2901\n",
      "Epoch 539/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2292\n",
      "Epoch 540/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.1833\n",
      "Epoch 541/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.3348\n",
      "Epoch 542/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2427\n",
      "Epoch 543/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.3295\n",
      "Epoch 544/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 3.9897\n",
      "Epoch 545/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.3407\n",
      "Epoch 546/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2508\n",
      "Epoch 547/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2106\n",
      "Epoch 548/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.3123\n",
      "Epoch 549/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1618\n",
      "Epoch 550/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3509\n",
      "Epoch 551/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1996\n",
      "Epoch 552/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2233\n",
      "Epoch 553/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1690\n",
      "Epoch 554/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3336\n",
      "Epoch 555/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.0577\n",
      "Epoch 556/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.3718\n",
      "Epoch 557/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2305\n",
      "Epoch 558/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.0852\n",
      "Epoch 559/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.4433\n",
      "Epoch 560/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.3189\n",
      "Epoch 561/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2705\n",
      "Epoch 562/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.0412\n",
      "Epoch 563/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2794\n",
      "Epoch 564/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3661\n",
      "Epoch 565/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.1253\n",
      "Epoch 566/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2279\n",
      "Epoch 567/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2860\n",
      "Epoch 568/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2894\n",
      "Epoch 569/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2591\n",
      "Epoch 570/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.3051\n",
      "Epoch 571/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.0004\n",
      "Epoch 572/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 13s 158ms/step - loss: 4.3314\n",
      "Epoch 573/1000\n",
      "83/83 [==============================] - 13s 152ms/step - loss: 4.3353\n",
      "Epoch 574/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.0708\n",
      "Epoch 575/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2803\n",
      "Epoch 576/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2928\n",
      "Epoch 577/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2818\n",
      "Epoch 578/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.0417\n",
      "Epoch 579/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.3572\n",
      "Epoch 580/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1611\n",
      "Epoch 581/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2172\n",
      "Epoch 582/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3853\n",
      "Epoch 583/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.0386\n",
      "Epoch 584/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.3859\n",
      "Epoch 585/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.2583\n",
      "Epoch 586/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.3013\n",
      "Epoch 587/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1899\n",
      "Epoch 588/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.3226\n",
      "Epoch 589/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.0947\n",
      "Epoch 590/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.3915\n",
      "Epoch 591/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.0195\n",
      "Epoch 592/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3645\n",
      "Epoch 593/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2126\n",
      "Epoch 594/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3385\n",
      "Epoch 595/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1344\n",
      "Epoch 596/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3675\n",
      "Epoch 597/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2131\n",
      "Epoch 598/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.2612\n",
      "Epoch 599/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2436\n",
      "Epoch 600/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1628\n",
      "Epoch 601/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.3037\n",
      "Epoch 602/1000\n",
      "83/83 [==============================] - 13s 152ms/step - loss: 4.2681\n",
      "Epoch 603/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.0918\n",
      "Epoch 604/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2610\n",
      "Epoch 605/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1762\n",
      "Epoch 606/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3556\n",
      "Epoch 607/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.1608\n",
      "Epoch 608/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1539\n",
      "Epoch 609/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.4543\n",
      "Epoch 610/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1635\n",
      "Epoch 611/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1733\n",
      "Epoch 612/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3749\n",
      "Epoch 613/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.1969\n",
      "Epoch 614/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.0464\n",
      "Epoch 615/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2732\n",
      "Epoch 616/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3605\n",
      "Epoch 617/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2490\n",
      "Epoch 618/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2299\n",
      "Epoch 619/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2703\n",
      "Epoch 620/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.0949\n",
      "Epoch 621/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.3181\n",
      "Epoch 622/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2844\n",
      "Epoch 623/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2810\n",
      "Epoch 624/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2084\n",
      "Epoch 625/1000\n",
      "83/83 [==============================] - 13s 152ms/step - loss: 4.2359\n",
      "Epoch 626/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2503\n",
      "Epoch 627/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2399\n",
      "Epoch 628/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.0936\n",
      "Epoch 629/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.3658\n",
      "Epoch 630/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1648\n",
      "Epoch 631/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3260\n",
      "Epoch 632/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2448\n",
      "Epoch 633/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1719\n",
      "Epoch 634/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.3049\n",
      "Epoch 635/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1728\n",
      "Epoch 636/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2610\n",
      "Epoch 637/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2737\n",
      "Epoch 638/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2006\n",
      "Epoch 639/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2555\n",
      "Epoch 640/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2329\n",
      "Epoch 641/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2469\n",
      "Epoch 642/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2916\n",
      "Epoch 643/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1960\n",
      "Epoch 644/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2771\n",
      "Epoch 645/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.0906\n",
      "Epoch 646/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.3623\n",
      "Epoch 647/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2326\n",
      "Epoch 648/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2061\n",
      "Epoch 649/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2645\n",
      "Epoch 650/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2697\n",
      "Epoch 651/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2158\n",
      "Epoch 652/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2482\n",
      "Epoch 653/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2334\n",
      "Epoch 654/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2257\n",
      "Epoch 655/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2479\n",
      "Epoch 656/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2345\n",
      "Epoch 657/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2455\n",
      "Epoch 658/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.2431\n",
      "Epoch 659/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2208\n",
      "Epoch 660/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2429\n",
      "Epoch 661/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2419\n",
      "Epoch 662/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2313\n",
      "Epoch 663/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2313\n",
      "Epoch 664/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2442\n",
      "Epoch 665/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2366\n",
      "Epoch 666/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2349\n",
      "Epoch 667/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2385\n",
      "Epoch 668/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2458\n",
      "Epoch 669/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2499\n",
      "Epoch 670/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2267\n",
      "Epoch 671/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2065\n",
      "Epoch 672/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.2969\n",
      "Epoch 673/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1871\n",
      "Epoch 674/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2508\n",
      "Epoch 675/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2493\n",
      "Epoch 676/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2372\n",
      "Epoch 677/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2355\n",
      "Epoch 678/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2255\n",
      "Epoch 679/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2429\n",
      "Epoch 680/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2289\n",
      "Epoch 681/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2155\n",
      "Epoch 682/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2553\n",
      "Epoch 683/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2494\n",
      "Epoch 684/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2569\n",
      "Epoch 685/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2359\n",
      "Epoch 686/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2077\n",
      "Epoch 687/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2162\n",
      "Epoch 688/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2621\n",
      "Epoch 689/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2393\n",
      "Epoch 690/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2321\n",
      "Epoch 691/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2588\n",
      "Epoch 692/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.1780\n",
      "Epoch 693/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2795\n",
      "Epoch 694/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.3113\n",
      "Epoch 695/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1004\n",
      "Epoch 696/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2762\n",
      "Epoch 697/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2740\n",
      "Epoch 698/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2182\n",
      "Epoch 699/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2260\n",
      "Epoch 700/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2148\n",
      "Epoch 701/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3779\n",
      "Epoch 702/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.0960\n",
      "Epoch 703/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3554\n",
      "Epoch 704/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1087\n",
      "Epoch 705/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2811\n",
      "Epoch 706/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1913\n",
      "Epoch 707/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.4054\n",
      "Epoch 708/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1547\n",
      "Epoch 709/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1444\n",
      "Epoch 710/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.4203\n",
      "Epoch 711/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.0614\n",
      "Epoch 712/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.4243\n",
      "Epoch 713/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.0374\n",
      "Epoch 714/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2659\n",
      "Epoch 715/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.3142\n",
      "Epoch 716/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1832\n",
      "Epoch 717/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2853\n",
      "Epoch 718/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3170\n",
      "Epoch 719/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1510\n",
      "Epoch 720/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2113\n",
      "Epoch 721/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2126\n",
      "Epoch 722/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3119\n",
      "Epoch 723/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2431\n",
      "Epoch 724/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.1871\n",
      "Epoch 725/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3680\n",
      "Epoch 726/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1120\n",
      "Epoch 727/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2935\n",
      "Epoch 728/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.1942\n",
      "Epoch 729/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1228\n",
      "Epoch 730/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.3776\n",
      "Epoch 731/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.1286\n",
      "Epoch 732/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.4434\n",
      "Epoch 733/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1459\n",
      "Epoch 734/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1817\n",
      "Epoch 735/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2477\n",
      "Epoch 736/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.3336\n",
      "Epoch 737/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1316\n",
      "Epoch 738/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.3008\n",
      "Epoch 739/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2739\n",
      "Epoch 740/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.0457\n",
      "Epoch 741/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.4352\n",
      "Epoch 742/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1444\n",
      "Epoch 743/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.4400\n",
      "Epoch 744/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 3.9481\n",
      "Epoch 745/1000\n",
      "83/83 [==============================] - 13s 159ms/step - loss: 4.2910\n",
      "Epoch 746/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2824\n",
      "Epoch 747/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.1700\n",
      "Epoch 748/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2469\n",
      "Epoch 749/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3522\n",
      "Epoch 750/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2568\n",
      "Epoch 751/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3607\n",
      "Epoch 752/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.0882\n",
      "Epoch 753/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2741\n",
      "Epoch 754/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3603\n",
      "Epoch 755/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.0031\n",
      "Epoch 756/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.4546\n",
      "Epoch 757/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.0787\n",
      "Epoch 758/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2279\n",
      "Epoch 759/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1561\n",
      "Epoch 760/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3208\n",
      "Epoch 761/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1594\n",
      "Epoch 762/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 13s 157ms/step - loss: 4.3508\n",
      "Epoch 763/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2670\n",
      "Epoch 764/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3202\n",
      "Epoch 765/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2152\n",
      "Epoch 766/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.0144\n",
      "Epoch 767/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.5115\n",
      "Epoch 768/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 3.9304\n",
      "Epoch 769/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2439\n",
      "Epoch 770/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.4499\n",
      "Epoch 771/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1880\n",
      "Epoch 772/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2040\n",
      "Epoch 773/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3007\n",
      "Epoch 774/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2442\n",
      "Epoch 775/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.0984\n",
      "Epoch 776/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2598\n",
      "Epoch 777/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.3835\n",
      "Epoch 778/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.1290\n",
      "Epoch 779/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3156\n",
      "Epoch 780/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.1755\n",
      "Epoch 781/1000\n",
      "83/83 [==============================] - 13s 152ms/step - loss: 4.0880\n",
      "Epoch 782/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3069\n",
      "Epoch 783/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.3883\n",
      "Epoch 784/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 3.9527\n",
      "Epoch 785/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3233\n",
      "Epoch 786/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.4796\n",
      "Epoch 787/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 3.9858\n",
      "Epoch 788/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3594\n",
      "Epoch 789/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2980\n",
      "Epoch 790/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1401\n",
      "Epoch 791/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2199\n",
      "Epoch 792/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.2445\n",
      "Epoch 793/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.3667\n",
      "Epoch 794/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.0465\n",
      "Epoch 795/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3623\n",
      "Epoch 796/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2349\n",
      "Epoch 797/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2812\n",
      "Epoch 798/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2216\n",
      "Epoch 799/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1400\n",
      "Epoch 800/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3265\n",
      "Epoch 801/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1366\n",
      "Epoch 802/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.2315\n",
      "Epoch 803/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2484\n",
      "Epoch 804/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.0772\n",
      "Epoch 805/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.4515\n",
      "Epoch 806/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1692\n",
      "Epoch 807/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.3685\n",
      "Epoch 808/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2364\n",
      "Epoch 809/1000\n",
      "83/83 [==============================] - 13s 152ms/step - loss: 4.2214\n",
      "Epoch 810/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2305\n",
      "Epoch 811/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.0002\n",
      "Epoch 812/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2604\n",
      "Epoch 813/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2920\n",
      "Epoch 814/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3996\n",
      "Epoch 815/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2092\n",
      "Epoch 816/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1969\n",
      "Epoch 817/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2249\n",
      "Epoch 818/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2854\n",
      "Epoch 819/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.0918\n",
      "Epoch 820/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3889\n",
      "Epoch 821/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.1305\n",
      "Epoch 822/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3566\n",
      "Epoch 823/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.3030\n",
      "Epoch 824/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.1929\n",
      "Epoch 825/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.1145\n",
      "Epoch 826/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2570\n",
      "Epoch 827/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3123\n",
      "Epoch 828/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2480\n",
      "Epoch 829/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2445\n",
      "Epoch 830/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1254\n",
      "Epoch 831/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3719\n",
      "Epoch 832/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.0962\n",
      "Epoch 833/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.3141\n",
      "Epoch 834/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1569\n",
      "Epoch 835/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3578\n",
      "Epoch 836/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.0500\n",
      "Epoch 837/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2656\n",
      "Epoch 838/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.3662\n",
      "Epoch 839/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2026\n",
      "Epoch 840/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2856\n",
      "Epoch 841/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2407\n",
      "Epoch 842/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2206\n",
      "Epoch 843/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1863\n",
      "Epoch 844/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.2722\n",
      "Epoch 845/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3086\n",
      "Epoch 846/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2018\n",
      "Epoch 847/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2027\n",
      "Epoch 848/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1555\n",
      "Epoch 849/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3205\n",
      "Epoch 850/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2442\n",
      "Epoch 851/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2262\n",
      "Epoch 852/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1445\n",
      "Epoch 853/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.3627\n",
      "Epoch 854/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2157\n",
      "Epoch 855/1000\n",
      "83/83 [==============================] - 13s 152ms/step - loss: 4.0996\n",
      "Epoch 856/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3567\n",
      "Epoch 857/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2370\n",
      "Epoch 858/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2387\n",
      "Epoch 859/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2662\n",
      "Epoch 860/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1878\n",
      "Epoch 861/1000\n",
      "83/83 [==============================] - 13s 152ms/step - loss: 4.2395\n",
      "Epoch 862/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.1972\n",
      "Epoch 863/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2763\n",
      "Epoch 864/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2200\n",
      "Epoch 865/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2712\n",
      "Epoch 866/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.2095\n",
      "Epoch 867/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2809\n",
      "Epoch 868/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1942\n",
      "Epoch 869/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2373\n",
      "Epoch 870/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2383\n",
      "Epoch 871/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2350\n",
      "Epoch 872/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2403\n",
      "Epoch 873/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2337\n",
      "Epoch 874/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.1585\n",
      "Epoch 875/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3402\n",
      "Epoch 876/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2305\n",
      "Epoch 877/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2152\n",
      "Epoch 878/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.2166\n",
      "Epoch 879/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2503\n",
      "Epoch 880/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2333\n",
      "Epoch 881/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2354\n",
      "Epoch 882/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2349\n",
      "Epoch 883/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2338\n",
      "Epoch 884/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2273\n",
      "Epoch 885/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2353\n",
      "Epoch 886/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2473\n",
      "Epoch 887/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2294\n",
      "Epoch 888/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2369\n",
      "Epoch 889/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2350\n",
      "Epoch 890/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2343\n",
      "Epoch 891/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2343\n",
      "Epoch 892/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2495\n",
      "Epoch 893/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2158\n",
      "Epoch 894/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2376\n",
      "Epoch 895/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2332\n",
      "Epoch 896/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2922\n",
      "Epoch 897/1000\n",
      "83/83 [==============================] - 13s 152ms/step - loss: 4.1534\n",
      "Epoch 898/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2147\n",
      "Epoch 899/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.3474\n",
      "Epoch 900/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1343\n",
      "Epoch 901/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2205\n",
      "Epoch 902/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2642\n",
      "Epoch 903/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2497\n",
      "Epoch 904/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2368\n",
      "Epoch 905/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3445\n",
      "Epoch 906/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1210\n",
      "Epoch 907/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3283\n",
      "Epoch 908/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2769\n",
      "Epoch 909/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2013\n",
      "Epoch 910/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1288\n",
      "Epoch 911/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2492\n",
      "Epoch 912/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.2529\n",
      "Epoch 913/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2191\n",
      "Epoch 914/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.3714\n",
      "Epoch 915/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.0471\n",
      "Epoch 916/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2495\n",
      "Epoch 917/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.3266\n",
      "Epoch 918/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2022\n",
      "Epoch 919/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3065\n",
      "Epoch 920/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1612\n",
      "Epoch 921/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2816\n",
      "Epoch 922/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1698\n",
      "Epoch 923/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1769\n",
      "Epoch 924/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2045\n",
      "Epoch 925/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.4148\n",
      "Epoch 926/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1021\n",
      "Epoch 927/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.4920\n",
      "Epoch 928/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 3.9957\n",
      "Epoch 929/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2208\n",
      "Epoch 930/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2579\n",
      "Epoch 931/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2698\n",
      "Epoch 932/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2031\n",
      "Epoch 933/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1966\n",
      "Epoch 934/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2570\n",
      "Epoch 935/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.3932\n",
      "Epoch 936/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1232\n",
      "Epoch 937/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2340\n",
      "Epoch 938/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2307\n",
      "Epoch 939/1000\n",
      "83/83 [==============================] - 13s 152ms/step - loss: 4.2380\n",
      "Epoch 940/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1975\n",
      "Epoch 941/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2705\n",
      "Epoch 942/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.1498\n",
      "Epoch 943/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.3347\n",
      "Epoch 944/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2319\n",
      "Epoch 945/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3424\n",
      "Epoch 946/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 3.9952\n",
      "Epoch 947/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2723\n",
      "Epoch 948/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2976\n",
      "Epoch 949/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.2325\n",
      "Epoch 950/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2677\n",
      "Epoch 951/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.4023\n",
      "Epoch 952/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 13s 157ms/step - loss: 3.9397\n",
      "Epoch 953/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2589\n",
      "Epoch 954/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.4080\n",
      "Epoch 955/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1978\n",
      "Epoch 956/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2281\n",
      "Epoch 957/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2783\n",
      "Epoch 958/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.1752\n",
      "Epoch 959/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2613\n",
      "Epoch 960/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.0888\n",
      "Epoch 961/1000\n",
      "83/83 [==============================] - 13s 159ms/step - loss: 4.3022\n",
      "Epoch 962/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1627\n",
      "Epoch 963/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.3921\n",
      "Epoch 964/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.3136\n",
      "Epoch 965/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.1202\n",
      "Epoch 966/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2255\n",
      "Epoch 967/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2337\n",
      "Epoch 968/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1432\n",
      "Epoch 969/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.4461\n",
      "Epoch 970/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.0226\n",
      "Epoch 971/1000\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 4.3827\n",
      "Epoch 972/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.0754\n",
      "Epoch 973/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3082\n",
      "Epoch 974/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1597\n",
      "Epoch 975/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3724\n",
      "Epoch 976/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.1240\n",
      "Epoch 977/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1996\n",
      "Epoch 978/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2909\n",
      "Epoch 979/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3899\n",
      "Epoch 980/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2380\n",
      "Epoch 981/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2102\n",
      "Epoch 982/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2145\n",
      "Epoch 983/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.1698\n",
      "Epoch 984/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2122\n",
      "Epoch 985/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2944\n",
      "Epoch 986/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.2352\n",
      "Epoch 987/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.3843\n",
      "Epoch 988/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.0262\n",
      "Epoch 989/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2872\n",
      "Epoch 990/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1565\n",
      "Epoch 991/1000\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 4.2689\n",
      "Epoch 992/1000\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 4.1969\n",
      "Epoch 993/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.4073\n",
      "Epoch 994/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.1353\n",
      "Epoch 995/1000\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 4.1589\n",
      "Epoch 996/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3042\n",
      "Epoch 997/1000\n",
      "83/83 [==============================] - 13s 156ms/step - loss: 4.2517\n",
      "Epoch 998/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.3193\n",
      "Epoch 999/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.0414\n",
      "Epoch 1000/1000\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 4.2490\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb9b46324a8>"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "model.fit_generator(\n",
    "    train_gen, steps_per_epoch=steps_per_epoch,\n",
    "    epochs=n_epochs, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'songbegin \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = ['songbegin']\n",
    "tokens = [[tokenizer.word_index[i] for i in seed]]\n",
    "s = pad_sequences(tokens, maxlen=sequence_len)\n",
    "for _ in range(30):\n",
    "    X = [s, s]\n",
    "    y_hat = model.predict(X)\n",
    "    i = np.argmax(y_hat[0][-1])\n",
    "    tokens[0].append(i)\n",
    "    s = pad_sequences(tokens, maxlen=sequence_len)\n",
    "' '.join([index_to_word.get(x, '<pad>') for x in tokens[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = ['you', 'are']\n",
    "tokens = [[tokenizer.word_index[i] for i in seed]]\n",
    "print(tokens)\n",
    "s = pad_sequences(tokens, maxlen=sequence_len)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, _ = X\n",
    "[index_to_word.get(i, '<pad>') for i in x1[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = [s, s]\n",
    "y_hat = model.predict(X)\n",
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[index_to_word.get(i, '<pad>') for i in np.argmax(y_hat[-1], axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(y_hat[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
