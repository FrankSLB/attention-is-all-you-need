{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = glob.glob('cnn/stories/*.story')\n",
    "# print(len(files))\n",
    "# files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"beatles/i'll-cry-instead.txt\",\n",
       " 'beatles/hey-jude.txt',\n",
       " \"beatles/nothin'-shakin'.txt\",\n",
       " 'beatles/i-will.txt',\n",
       " 'beatles/child-of-nature.txt',\n",
       " 'beatles/helter-skelter.txt',\n",
       " 'beatles/i-am-the-walrus.txt',\n",
       " 'beatles/glad-all-over.txt',\n",
       " 'beatles/johnny-b.-goode.txt',\n",
       " 'beatles/christmas-time.txt',\n",
       " 'beatles/i-call-your-name.txt',\n",
       " 'beatles/no-reply.txt',\n",
       " 'beatles/hello,-little-girl.txt',\n",
       " 'beatles/good-day-sunshine.txt',\n",
       " 'beatles/clarabella.txt',\n",
       " 'beatles/act-naturally.txt',\n",
       " 'beatles/dear-wack!.txt',\n",
       " 'beatles/chains.txt',\n",
       " 'beatles/blackbird.txt',\n",
       " 'beatles/keep-your-hands-off-my-baby.txt',\n",
       " 'beatles/get-back.txt',\n",
       " 'beatles/happiness-is-a-warm-gun.txt',\n",
       " 'beatles/if-i-needed-someone-to-love.txt',\n",
       " 'beatles/crinsk-dee-night.txt',\n",
       " 'beatles/commonwealth.txt',\n",
       " 'beatles/norwegian-wood.txt',\n",
       " \"beatles/ain't-she-sweet.txt\",\n",
       " 'beatles/lucille.txt',\n",
       " 'beatles/nowhere-man.txt',\n",
       " 'beatles/give-peace-a-chance.txt',\n",
       " 'beatles/from-me-to-you.txt',\n",
       " 'beatles/help!.txt',\n",
       " \"beatles/i'll-be-on-my-way.txt\",\n",
       " \"beatles/i'll-be-back.txt\",\n",
       " \"beatles/don't-bother-me.txt\",\n",
       " 'beatles/kansa-city.txt',\n",
       " 'beatles/michelle.txt',\n",
       " 'beatles/day-tripper.txt',\n",
       " \"beatles/it's-all-too-much.txt\",\n",
       " 'beatles/a-taste-of-honey.txt',\n",
       " \"beatles/all-i've-got-to-do.txt\",\n",
       " 'beatles/jingle-bells.txt',\n",
       " 'beatles/have-a-banana!.txt',\n",
       " 'beatles/across-the-universe.txt',\n",
       " 'beatles/money.txt',\n",
       " 'beatles/cry-baby-cry.txt',\n",
       " 'beatles/come-and-get-it.txt',\n",
       " 'beatles/blue-suede-shoes.txt',\n",
       " 'beatles/love-you-to.txt',\n",
       " 'beatles/honey-pie.txt',\n",
       " 'beatles/love-these-goon-shows!.txt',\n",
       " \"beatles/honey,-don't.txt\",\n",
       " 'beatles/and-i-love-her.txt',\n",
       " 'beatles/a-day-in-the-life.txt',\n",
       " \"beatles/i'm-looking-through-you.txt\",\n",
       " 'beatles/my-bonnie.txt',\n",
       " 'beatles/do-you-want-to-know-a-secret.txt',\n",
       " 'beatles/dear-prudence.txt',\n",
       " 'beatles/i-got-a-woman.txt',\n",
       " 'beatles/come-together.txt',\n",
       " 'beatles/mean-mr.-mustard.txt',\n",
       " 'beatles/old-brown-shoe.txt',\n",
       " 'beatles/komm,-gib-mir-deine-hand.txt',\n",
       " \"beatles/i'm-happy-just-to-dance-with-you.txt\",\n",
       " 'beatles/any-time-at-all.txt',\n",
       " 'beatles/bad-to-me.txt',\n",
       " 'beatles/hallelujah,-i-love-her-so.txt',\n",
       " 'beatles/hold-me-tight.txt',\n",
       " 'beatles/i-saw-her-standing.txt',\n",
       " \"beatles/i've-got-a-feeling.txt\",\n",
       " 'beatles/lend-me-your-comb.txt',\n",
       " \"beatles/a-hard-day's-night.txt\",\n",
       " 'beatles/and-your-bird-can-sing.txt',\n",
       " 'beatles/being-for-the-benefit-of-mr.-kite.txt',\n",
       " 'beatles/i-me-mine.txt',\n",
       " 'beatles/follow-the-sun.txt',\n",
       " \"beatles/don't-let-me-down.txt\",\n",
       " 'beatles/all-together-now.txt',\n",
       " 'beatles/girl.txt',\n",
       " \"beatles/i've-just-seen-a-face.txt\",\n",
       " 'beatles/glass-onion.txt',\n",
       " 'beatles/misery.txt',\n",
       " 'beatles/how-do-you-do-it.txt',\n",
       " 'beatles/in-my-life.txt',\n",
       " 'beatles/all-you-need-is-love.txt',\n",
       " 'beatles/imagine.txt',\n",
       " 'beatles/dizzy-miss-lizzy.txt',\n",
       " 'beatles/got-to-get-it-into-my-life.txt',\n",
       " 'beatles/mr.-moonlight.txt',\n",
       " 'beatles/carol.txt',\n",
       " 'beatles/every-little-thing.txt',\n",
       " 'beatles/all-things-must-pass.txt',\n",
       " 'beatles/love-of-the-loved.txt',\n",
       " 'beatles/for-no-one.txt',\n",
       " 'beatles/i-wanna-be-your-man.txt',\n",
       " 'beatles/like-dreamers-do.txt',\n",
       " 'beatles/all-my-loving.txt',\n",
       " 'beatles/i-want-to-tell-you.txt',\n",
       " 'beatles/ask-me-why.txt',\n",
       " 'beatles/lovely-rita.txt',\n",
       " 'beatles/anna,-go-to-him.txt',\n",
       " 'beatles/devil-in-her-heart.txt',\n",
       " 'beatles/in-spite-of-all-the-danger.txt',\n",
       " \"beatles/i'm-down.txt\",\n",
       " 'beatles/oh!-darling.txt',\n",
       " 'beatles/if-i-needed-someone.txt',\n",
       " 'beatles/cold-turkey.txt',\n",
       " 'beatles/a-little-rhyme.txt',\n",
       " 'beatles/dr.-robert.txt',\n",
       " \"beatles/it's-only-love.txt\",\n",
       " 'beatles/magical-mystery-tour.txt',\n",
       " 'beatles/leave-my-kitten-alone.txt',\n",
       " \"beatles/i-don't-want-to-spoil-the-party.txt\",\n",
       " 'beatles/good-morning,-good-morning.txt',\n",
       " 'beatles/long-tall-sally.txt',\n",
       " 'beatles/ob-la-di,-ob-la-da.txt',\n",
       " 'beatles/not-guilty.txt',\n",
       " 'beatles/little-child.txt',\n",
       " 'beatles/i-want-to-hold-your-hand.txt',\n",
       " \"beatles/i'm-only-sleeping.txt\",\n",
       " 'beatles/let-it-be.txt',\n",
       " 'beatles/another-girl.txt',\n",
       " 'beatles/boys.txt',\n",
       " \"beatles/don't-pass-me-by.txt\",\n",
       " \"beatles/baby,-it's-you.txt\",\n",
       " \"beatles/octopus's-garden.txt\",\n",
       " \"beatles/i'm-so-tired.txt\",\n",
       " 'beatles/memphis,-tenessee.txt',\n",
       " 'beatles/los-paranoias.txt',\n",
       " 'beatles/blue-jay-way.txt',\n",
       " 'beatles/back-in-the-ussr.txt',\n",
       " \"beatles/i'm-a-loser.txt\",\n",
       " 'beatles/here-comes-the-sun.txt',\n",
       " 'beatles/junk.txt',\n",
       " 'beatles/lucy-in-the-sky-with-diamonds.txt',\n",
       " 'beatles/if-i-fell.txt',\n",
       " \"beatles/everybody's-got-something-to-hide-except-me-and-my-monkey.txt\",\n",
       " 'beatles/eleanor-rigby.txt',\n",
       " 'beatles/i-feel-fine.txt',\n",
       " 'beatles/julia.txt',\n",
       " 'beatles/hello,-goodbye.txt',\n",
       " \"beatles/don't-ever-change.txt\",\n",
       " 'beatles/just-a-rumour.txt',\n",
       " \"beatles/i'll-get-you.txt\",\n",
       " 'beatles/fixing-a-hole.txt',\n",
       " 'beatles/golden-slumbers.txt',\n",
       " 'beatles/good-night.txt',\n",
       " \"beatles/everybody's-trying-to-be-my-baby.txt\",\n",
       " \"beatles/i'm-gonna-sit-right-down-and-cry.txt\",\n",
       " 'beatles/birthday.txt',\n",
       " 'beatles/eight-days-a-week.txt',\n",
       " 'beatles/besame-mucho.txt',\n",
       " 'beatles/bad-boy.txt',\n",
       " 'beatles/martha,-my-dear.txt',\n",
       " 'beatles/crying,-waiting,-hoping.txt',\n",
       " 'beatles/i-want-you.txt',\n",
       " 'beatles/getting-better.txt',\n",
       " 'beatles/mrs.-robinson.txt',\n",
       " 'beatles/love-me-do.txt',\n",
       " 'beatles/matchbox.txt',\n",
       " 'beatles/lady-madonna.txt',\n",
       " \"beatles/baby's-in-black.txt\",\n",
       " 'beatles/i-need-you.txt',\n",
       " \"beatles/if-you've-got-trouble.txt\",\n",
       " 'beatles/if-you-love-me-baby.txt',\n",
       " \"beatles/can't-buy-me-love.txt\",\n",
       " 'beatles/i-got-to-find-my-baby.txt',\n",
       " 'beatles/carry-that-weight.txt',\n",
       " 'beatles/a-shot-of-rhythm-and-blues.txt',\n",
       " \"beatles/it-won't-be-long.txt\",\n",
       " 'beatles/hey-bulldog.txt',\n",
       " 'beatles/free-as-a-bird.txt',\n",
       " \"beatles/baby,-you're-a-rich-man.txt\",\n",
       " 'beatles/drive-my-car.txt',\n",
       " \"beatles/nobody's-child.txt\",\n",
       " 'beatles/not-a-second-time.txt',\n",
       " 'beatles/dig-a-pony.txt',\n",
       " 'beatles/for-you-blue.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob.glob('beatles/*.txt')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = files[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define method for generating text from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generator(files, splitter=None):\n",
    "    # splitter is responsible for breaking up text into smaller sequences\n",
    "    # i.e. perhaps by splitting on \\n\n",
    "    for f in files:\n",
    "        text = open(f).read()\n",
    "        # remove highlights\n",
    "        text = text.split('@highlight')[0]\n",
    "        if splitter is None:\n",
    "            text = [text]\n",
    "        else:\n",
    "            text = splitter(text)\n",
    "        for t in text:\n",
    "            if not t or len(t) < 10:\n",
    "                continue\n",
    "            yield t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I've got every reason on earth to be mad\\n'Cause I just lost the only girl I had\\nIf I could get my way\\nI'd get myself locked up today\\nBut I can't, so I'll cry instead\\n\\nI've got a chip on my shoulder that's bigger that my feet\\nI can't talk to people that I meet\\nIf I could see you now\\nI'd try to make you sad somehow\\nBut I can't, so I'll cry instead\\n\\nDon't want to cry when there's people there\\nI get shy when they start to stare\\nI'm gonna hide myself away\\nBut I'll come back again someday\\n\\nAnd when you do you'd better hide all the girls\\nI'm gonna break their hearts all round the world\\nYes, I'm gonna break them in two\\nAnd show you what your lovin' man can do\\nUntil then I'll cry instead\\n\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(text_generator(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = text_generator(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dante/venvs/default/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 10.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer.fit_on_texts(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'cause\": 126,\n",
       " '<unk>': 511,\n",
       " 'a': 7,\n",
       " 'aah': 173,\n",
       " 'about': 194,\n",
       " 'above': 355,\n",
       " 'act': 199,\n",
       " 'afraid': 287,\n",
       " 'again': 20,\n",
       " \"ain't\": 74,\n",
       " 'air': 338,\n",
       " 'all': 22,\n",
       " 'allan': 421,\n",
       " 'always': 332,\n",
       " 'am': 59,\n",
       " 'among': 451,\n",
       " 'an': 241,\n",
       " 'and': 8,\n",
       " 'answer': 163,\n",
       " 'any': 352,\n",
       " 'anytime': 290,\n",
       " 'apart': 335,\n",
       " 'are': 64,\n",
       " 'around': 486,\n",
       " 'as': 236,\n",
       " 'at': 93,\n",
       " 'away': 135,\n",
       " 'b': 106,\n",
       " 'baby': 57,\n",
       " 'back': 113,\n",
       " 'background': 493,\n",
       " 'bad': 185,\n",
       " 'band': 484,\n",
       " 'be': 31,\n",
       " 'beating': 347,\n",
       " 'beatles': 500,\n",
       " 'been': 101,\n",
       " 'beg': 322,\n",
       " 'began': 224,\n",
       " 'begging': 323,\n",
       " 'begin': 140,\n",
       " 'behalf': 499,\n",
       " 'bell': 466,\n",
       " 'bended': 203,\n",
       " 'beneath': 470,\n",
       " 'better': 34,\n",
       " 'big': 482,\n",
       " 'bigger': 272,\n",
       " 'blisters': 357,\n",
       " 'bloody': 366,\n",
       " 'blow': 210,\n",
       " 'bonnie': 247,\n",
       " 'book': 464,\n",
       " 'bottom': 121,\n",
       " 'boy': 124,\n",
       " 'break': 114,\n",
       " 'but': 11,\n",
       " 'by': 145,\n",
       " 'ca': 168,\n",
       " 'cabin': 454,\n",
       " 'cachoo': 83,\n",
       " 'cachou': 423,\n",
       " 'call': 427,\n",
       " 'can': 88,\n",
       " \"can't\": 110,\n",
       " 'carry': 189,\n",
       " 'catch': 218,\n",
       " 'cats': 312,\n",
       " 'cheek': 249,\n",
       " 'chip': 270,\n",
       " 'choking': 399,\n",
       " 'christmas': 19,\n",
       " 'city': 371,\n",
       " 'clay': 455,\n",
       " 'climbing': 409,\n",
       " 'close': 448,\n",
       " 'colder': 295,\n",
       " 'come': 77,\n",
       " 'comes': 356,\n",
       " 'coming': 98,\n",
       " 'continues': 491,\n",
       " 'control': 226,\n",
       " 'coo': 25,\n",
       " 'cooking': 438,\n",
       " 'cool': 144,\n",
       " 'cornflake': 360,\n",
       " 'corporation': 362,\n",
       " 'could': 127,\n",
       " 'country': 171,\n",
       " 'crabalocker': 385,\n",
       " 'cry': 67,\n",
       " 'crying': 81,\n",
       " 'custard': 380,\n",
       " 'dancer': 166,\n",
       " 'dang': 103,\n",
       " 'day': 120,\n",
       " 'dead': 382,\n",
       " 'deep': 446,\n",
       " \"didn't\": 73,\n",
       " 'do': 56,\n",
       " \"dog's\": 383,\n",
       " 'doggone': 320,\n",
       " \"don't\": 28,\n",
       " 'doubt': 424,\n",
       " 'down': 62,\n",
       " 'dreaming': 345,\n",
       " 'dripping': 381,\n",
       " 'drivers': 477,\n",
       " 'drumming': 475,\n",
       " 'earth': 264,\n",
       " 'ease': 213,\n",
       " 'easy': 341,\n",
       " 'edgar': 420,\n",
       " 'egg': 82,\n",
       " 'eggman': 390,\n",
       " 'eggmen': 391,\n",
       " 'eiffel': 410,\n",
       " 'electric': 434,\n",
       " 'elementary': 412,\n",
       " 'endear': 344,\n",
       " 'english': 242,\n",
       " 'ever': 217,\n",
       " 'evergreens': 452,\n",
       " 'every': 108,\n",
       " 'everything': 497,\n",
       " 'everywhere': 505,\n",
       " 'expert': 396,\n",
       " 'eye': 384,\n",
       " 'eyes': 353,\n",
       " 'face': 368,\n",
       " 'fades': 492,\n",
       " 'fast': 94,\n",
       " 'feel': 70,\n",
       " 'feeling': 348,\n",
       " 'feet': 273,\n",
       " 'fever': 441,\n",
       " 'fill': 337,\n",
       " 'find': 193,\n",
       " 'fingers': 358,\n",
       " 'fishwife': 386,\n",
       " 'fly': 239,\n",
       " 'fool': 142,\n",
       " 'for': 43,\n",
       " 'forever': 221,\n",
       " 'found': 296,\n",
       " 'from': 100,\n",
       " 'gang': 309,\n",
       " 'garden': 392,\n",
       " 'george': 501,\n",
       " 'get': 39,\n",
       " 'gilly': 104,\n",
       " 'girl': 177,\n",
       " 'girls': 280,\n",
       " 'give': 89,\n",
       " 'glad': 54,\n",
       " 'go': 5,\n",
       " 'goes': 487,\n",
       " 'gone': 430,\n",
       " 'gonna': 133,\n",
       " 'good': 244,\n",
       " 'goode': 107,\n",
       " 'goosepimples': 426,\n",
       " 'got': 75,\n",
       " 'grab': 316,\n",
       " 'grow': 369,\n",
       " 'guitar': 250,\n",
       " 'gun': 359,\n",
       " 'gunny': 467,\n",
       " 'guy': 96,\n",
       " 'had': 268,\n",
       " 'happy': 169,\n",
       " 'hard': 154,\n",
       " 'hari': 415,\n",
       " 'harrison': 502,\n",
       " 'have': 147,\n",
       " 'he': 99,\n",
       " 'hear': 223,\n",
       " 'heart': 61,\n",
       " 'hearts': 282,\n",
       " 'heels': 314,\n",
       " 'helter': 52,\n",
       " 'her': 35,\n",
       " 'here': 24,\n",
       " 'hey': 14,\n",
       " 'hide': 134,\n",
       " 'high': 442,\n",
       " 'him': 480,\n",
       " 'his': 118,\n",
       " 'honey': 246,\n",
       " 'hot': 102,\n",
       " 'how': 66,\n",
       " 'hurt': 80,\n",
       " 'i': 4,\n",
       " \"i'd\": 87,\n",
       " \"i'll\": 111,\n",
       " \"i'm\": 9,\n",
       " \"i've\": 85,\n",
       " 'if': 86,\n",
       " 'in': 30,\n",
       " 'insecure': 349,\n",
       " 'inside': 160,\n",
       " 'instead': 130,\n",
       " 'into': 186,\n",
       " 'is': 16,\n",
       " 'it': 18,\n",
       " \"it's\": 72,\n",
       " 'jealous': 95,\n",
       " 'joes': 311,\n",
       " 'john': 258,\n",
       " 'johnny': 21,\n",
       " 'joker': 402,\n",
       " 'jude': 15,\n",
       " 'just': 50,\n",
       " 'keep': 206,\n",
       " 'keys': 215,\n",
       " 'kicking': 419,\n",
       " 'kiss': 425,\n",
       " 'kisses': 202,\n",
       " 'knees': 150,\n",
       " 'knickers': 389,\n",
       " 'know': 71,\n",
       " 'knows': 324,\n",
       " 'krishna': 416,\n",
       " 'last': 336,\n",
       " 'laughs': 403,\n",
       " 'leader': 481,\n",
       " 'learned': 460,\n",
       " 'leaves': 48,\n",
       " 'lennon': 259,\n",
       " 'let': 42,\n",
       " 'lets': 303,\n",
       " 'lifetime': 329,\n",
       " 'light': 489,\n",
       " 'like': 38,\n",
       " 'listeners': 262,\n",
       " 'little': 146,\n",
       " 'lives': 457,\n",
       " 'locked': 128,\n",
       " 'lonely': 328,\n",
       " 'long': 216,\n",
       " 'look': 97,\n",
       " 'looking': 437,\n",
       " 'lose': 225,\n",
       " 'lost': 266,\n",
       " 'loud': 340,\n",
       " 'louisianna': 447,\n",
       " 'love': 45,\n",
       " 'loved': 325,\n",
       " 'lover': 165,\n",
       " \"lovin'\": 285,\n",
       " 'loving': 204,\n",
       " 'low': 440,\n",
       " 'lucy': 376,\n",
       " 'mad': 265,\n",
       " 'made': 78,\n",
       " 'make': 27,\n",
       " 'makes': 198,\n",
       " 'making': 294,\n",
       " 'mama': 479,\n",
       " 'man': 68,\n",
       " 'many': 485,\n",
       " 'matter': 379,\n",
       " 'mattered': 331,\n",
       " 'may': 164,\n",
       " 'maybe': 488,\n",
       " 'mccartney': 496,\n",
       " 'me': 10,\n",
       " 'mean': 157,\n",
       " 'meet': 180,\n",
       " 'men': 167,\n",
       " 'mercy': 428,\n",
       " 'merry': 261,\n",
       " 'might': 350,\n",
       " 'miles': 235,\n",
       " 'mine': 208,\n",
       " 'minute': 289,\n",
       " 'mister': 370,\n",
       " 'money': 201,\n",
       " 'more': 159,\n",
       " 'movement': 300,\n",
       " 'much': 436,\n",
       " 'music': 253,\n",
       " 'must': 197,\n",
       " 'my': 13,\n",
       " 'myself': 178,\n",
       " 'nah': 1,\n",
       " 'name': 219,\n",
       " 'named': 459,\n",
       " 'naughty': 240,\n",
       " 'near': 342,\n",
       " 'need': 301,\n",
       " 'never': 155,\n",
       " 'new': 170,\n",
       " 'no': 122,\n",
       " 'not': 351,\n",
       " 'nothing': 46,\n",
       " 'now': 181,\n",
       " 'o': 255,\n",
       " 'of': 65,\n",
       " 'oh': 63,\n",
       " 'old': 483,\n",
       " 'on': 12,\n",
       " 'one': 90,\n",
       " 'only': 267,\n",
       " 'oo': 243,\n",
       " 'ooo': 248,\n",
       " 'opportunity': 504,\n",
       " 'or': 462,\n",
       " 'orleans': 449,\n",
       " 'out': 36,\n",
       " 'over': 55,\n",
       " 'pain': 141,\n",
       " 'pappie': 305,\n",
       " 'passing': 478,\n",
       " 'past': 346,\n",
       " 'pastiche': 508,\n",
       " 'paul': 495,\n",
       " 'penguin': 413,\n",
       " 'people': 112,\n",
       " 'perform': 299,\n",
       " 'pert': 398,\n",
       " 'pigs': 238,\n",
       " 'pilchard': 408,\n",
       " 'play': 172,\n",
       " 'playing': 473,\n",
       " 'plays': 143,\n",
       " 'please': 79,\n",
       " 'poe': 422,\n",
       " 'point': 509,\n",
       " 'policeman': 372,\n",
       " 'policemen': 374,\n",
       " 'pornographic': 387,\n",
       " 'pretty': 373,\n",
       " 'priestess': 388,\n",
       " 'puppy': 431,\n",
       " 'puts': 212,\n",
       " 'railroad': 471,\n",
       " 'rain': 395,\n",
       " 'read': 461,\n",
       " 'really': 220,\n",
       " 'reason': 263,\n",
       " 'refrain': 291,\n",
       " 'remember': 139,\n",
       " 'rhythm': 476,\n",
       " 'ride': 234,\n",
       " 'ringing': 465,\n",
       " 'ringo': 506,\n",
       " 'rock': 429,\n",
       " 'rocking': 310,\n",
       " 'round': 115,\n",
       " 'row': 375,\n",
       " 'run': 237,\n",
       " 'sack': 468,\n",
       " 'sad': 131,\n",
       " 'same': 333,\n",
       " 'saw': 330,\n",
       " 'say': 252,\n",
       " 'saying': 254,\n",
       " 'see': 40,\n",
       " 'seen': 418,\n",
       " 'semolina': 407,\n",
       " 'sewed': 433,\n",
       " 'shade': 474,\n",
       " 'shaking': 47,\n",
       " 'she': 51,\n",
       " \"she's\": 151,\n",
       " 'shine': 211,\n",
       " 'shirt': 364,\n",
       " 'shivering': 227,\n",
       " 'should': 417,\n",
       " 'shoulder': 179,\n",
       " 'shoulders': 293,\n",
       " 'show': 284,\n",
       " 'shows': 245,\n",
       " 'shy': 277,\n",
       " 'silly': 105,\n",
       " 'since': 174,\n",
       " 'sing': 339,\n",
       " 'singing': 414,\n",
       " 'sit': 469,\n",
       " 'sitting': 123,\n",
       " 'skelter': 53,\n",
       " 'skin': 188,\n",
       " 'sky': 377,\n",
       " 'slide': 232,\n",
       " 'smile': 404,\n",
       " 'smokers': 400,\n",
       " 'snide': 406,\n",
       " 'so': 76,\n",
       " 'some': 153,\n",
       " 'someday': 183,\n",
       " 'somehow': 276,\n",
       " 'someone': 298,\n",
       " 'something': 313,\n",
       " 'song': 138,\n",
       " 'sorry': 158,\n",
       " 'speak': 443,\n",
       " 'speaking': 503,\n",
       " 'spells': 257,\n",
       " 'spent': 200,\n",
       " 'spoken': 494,\n",
       " 'squeeze': 317,\n",
       " 'stand': 453,\n",
       " 'standing': 394,\n",
       " 'stare': 278,\n",
       " 'starr': 507,\n",
       " 'start': 132,\n",
       " 'still': 326,\n",
       " 'stop': 162,\n",
       " 'stupid': 365,\n",
       " 'sty': 405,\n",
       " 'such': 319,\n",
       " 'sun': 91,\n",
       " 'sure': 302,\n",
       " 'swallowing': 230,\n",
       " 't': 256,\n",
       " 'take': 117,\n",
       " 'talk': 274,\n",
       " 'tan': 393,\n",
       " 'tease': 321,\n",
       " 'tee': 363,\n",
       " 'tell': 49,\n",
       " 'temperature': 439,\n",
       " 'text': 397,\n",
       " 'that': 26,\n",
       " \"that's\": 271,\n",
       " 'the': 3,\n",
       " 'their': 281,\n",
       " 'them': 184,\n",
       " 'then': 69,\n",
       " 'there': 182,\n",
       " \"there'd\": 306,\n",
       " \"there's\": 33,\n",
       " 'these': 308,\n",
       " 'they': 41,\n",
       " 'things': 343,\n",
       " 'think': 401,\n",
       " 'this': 84,\n",
       " 'though': 205,\n",
       " 'thought': 229,\n",
       " 'threw': 214,\n",
       " 'till': 92,\n",
       " 'time': 17,\n",
       " 'times': 307,\n",
       " 'to': 6,\n",
       " 'today': 269,\n",
       " 'toes': 315,\n",
       " 'together': 222,\n",
       " 'told': 195,\n",
       " 'tonight': 490,\n",
       " 'too': 444,\n",
       " 'top': 231,\n",
       " 'touch': 432,\n",
       " 'tower': 411,\n",
       " 'track': 472,\n",
       " 'trees': 37,\n",
       " 'tried': 149,\n",
       " 'try': 275,\n",
       " \"tryin'\": 228,\n",
       " 'trying': 207,\n",
       " 'tuesday': 367,\n",
       " 'turn': 233,\n",
       " 'two': 283,\n",
       " 'under': 187,\n",
       " 'understand': 510,\n",
       " 'until': 286,\n",
       " 'up': 129,\n",
       " 'upon': 292,\n",
       " 'used': 251,\n",
       " 'van': 361,\n",
       " 'very': 176,\n",
       " 'wait': 327,\n",
       " 'waiting': 148,\n",
       " 'walrus': 125,\n",
       " 'want': 60,\n",
       " 'was': 58,\n",
       " 'watch': 354,\n",
       " 'way': 109,\n",
       " 'we': 196,\n",
       " \"we're\": 156,\n",
       " 'weak': 445,\n",
       " 'week': 304,\n",
       " 'well': 44,\n",
       " 'were': 288,\n",
       " 'what': 137,\n",
       " 'when': 32,\n",
       " 'whenever': 334,\n",
       " 'where': 161,\n",
       " 'who': 190,\n",
       " 'why': 318,\n",
       " 'will': 29,\n",
       " 'wind': 209,\n",
       " 'wire': 435,\n",
       " 'wish': 175,\n",
       " 'with': 191,\n",
       " \"won't\": 152,\n",
       " 'wood': 456,\n",
       " 'woods': 450,\n",
       " 'world': 136,\n",
       " 'write': 463,\n",
       " 'yeah': 119,\n",
       " 'year': 260,\n",
       " 'yellow': 378,\n",
       " 'yes': 116,\n",
       " 'you': 2,\n",
       " \"you'd\": 279,\n",
       " \"you'll\": 192,\n",
       " \"you're\": 297,\n",
       " 'young': 458,\n",
       " 'your': 23,\n",
       " 'yourself': 498}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {v: k for k, v in tokenizer.word_index.items()}\n",
    "index_to_word[0] = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<pad>',\n",
       " 1: 'nah',\n",
       " 2: 'you',\n",
       " 3: 'the',\n",
       " 4: 'i',\n",
       " 5: 'go',\n",
       " 6: 'to',\n",
       " 7: 'a',\n",
       " 8: 'and',\n",
       " 9: \"i'm\",\n",
       " 10: 'me',\n",
       " 11: 'but',\n",
       " 12: 'on',\n",
       " 13: 'my',\n",
       " 14: 'hey',\n",
       " 15: 'jude',\n",
       " 16: 'is',\n",
       " 17: 'time',\n",
       " 18: 'it',\n",
       " 19: 'christmas',\n",
       " 20: 'again',\n",
       " 21: 'johnny',\n",
       " 22: 'all',\n",
       " 23: 'your',\n",
       " 24: 'here',\n",
       " 25: 'coo',\n",
       " 26: 'that',\n",
       " 27: 'make',\n",
       " 28: \"don't\",\n",
       " 29: 'will',\n",
       " 30: 'in',\n",
       " 31: 'be',\n",
       " 32: 'when',\n",
       " 33: \"there's\",\n",
       " 34: 'better',\n",
       " 35: 'her',\n",
       " 36: 'out',\n",
       " 37: 'trees',\n",
       " 38: 'like',\n",
       " 39: 'get',\n",
       " 40: 'see',\n",
       " 41: 'they',\n",
       " 42: 'let',\n",
       " 43: 'for',\n",
       " 44: 'well',\n",
       " 45: 'love',\n",
       " 46: 'nothing',\n",
       " 47: 'shaking',\n",
       " 48: 'leaves',\n",
       " 49: 'tell',\n",
       " 50: 'just',\n",
       " 51: 'she',\n",
       " 52: 'helter',\n",
       " 53: 'skelter',\n",
       " 54: 'glad',\n",
       " 55: 'over',\n",
       " 56: 'do',\n",
       " 57: 'baby',\n",
       " 58: 'was',\n",
       " 59: 'am',\n",
       " 60: 'want',\n",
       " 61: 'heart',\n",
       " 62: 'down',\n",
       " 63: 'oh',\n",
       " 64: 'are',\n",
       " 65: 'of',\n",
       " 66: 'how',\n",
       " 67: 'cry',\n",
       " 68: 'man',\n",
       " 69: 'then',\n",
       " 70: 'feel',\n",
       " 71: 'know',\n",
       " 72: \"it's\",\n",
       " 73: \"didn't\",\n",
       " 74: \"ain't\",\n",
       " 75: 'got',\n",
       " 76: 'so',\n",
       " 77: 'come',\n",
       " 78: 'made',\n",
       " 79: 'please',\n",
       " 80: 'hurt',\n",
       " 81: 'crying',\n",
       " 82: 'egg',\n",
       " 83: 'cachoo',\n",
       " 84: 'this',\n",
       " 85: \"i've\",\n",
       " 86: 'if',\n",
       " 87: \"i'd\",\n",
       " 88: 'can',\n",
       " 89: 'give',\n",
       " 90: 'one',\n",
       " 91: 'sun',\n",
       " 92: 'till',\n",
       " 93: 'at',\n",
       " 94: 'fast',\n",
       " 95: 'jealous',\n",
       " 96: 'guy',\n",
       " 97: 'look',\n",
       " 98: 'coming',\n",
       " 99: 'he',\n",
       " 100: 'from',\n",
       " 101: 'been',\n",
       " 102: 'hot',\n",
       " 103: 'dang',\n",
       " 104: 'gilly',\n",
       " 105: 'silly',\n",
       " 106: 'b',\n",
       " 107: 'goode',\n",
       " 108: 'every',\n",
       " 109: 'way',\n",
       " 110: \"can't\",\n",
       " 111: \"i'll\",\n",
       " 112: 'people',\n",
       " 113: 'back',\n",
       " 114: 'break',\n",
       " 115: 'round',\n",
       " 116: 'yes',\n",
       " 117: 'take',\n",
       " 118: 'his',\n",
       " 119: 'yeah',\n",
       " 120: 'day',\n",
       " 121: 'bottom',\n",
       " 122: 'no',\n",
       " 123: 'sitting',\n",
       " 124: 'boy',\n",
       " 125: 'walrus',\n",
       " 126: \"'cause\",\n",
       " 127: 'could',\n",
       " 128: 'locked',\n",
       " 129: 'up',\n",
       " 130: 'instead',\n",
       " 131: 'sad',\n",
       " 132: 'start',\n",
       " 133: 'gonna',\n",
       " 134: 'hide',\n",
       " 135: 'away',\n",
       " 136: 'world',\n",
       " 137: 'what',\n",
       " 138: 'song',\n",
       " 139: 'remember',\n",
       " 140: 'begin',\n",
       " 141: 'pain',\n",
       " 142: 'fool',\n",
       " 143: 'plays',\n",
       " 144: 'cool',\n",
       " 145: 'by',\n",
       " 146: 'little',\n",
       " 147: 'have',\n",
       " 148: 'waiting',\n",
       " 149: 'tried',\n",
       " 150: 'knees',\n",
       " 151: \"she's\",\n",
       " 152: \"won't\",\n",
       " 153: 'some',\n",
       " 154: 'hard',\n",
       " 155: 'never',\n",
       " 156: \"we're\",\n",
       " 157: 'mean',\n",
       " 158: 'sorry',\n",
       " 159: 'more',\n",
       " 160: 'inside',\n",
       " 161: 'where',\n",
       " 162: 'stop',\n",
       " 163: 'answer',\n",
       " 164: 'may',\n",
       " 165: 'lover',\n",
       " 166: 'dancer',\n",
       " 167: 'men',\n",
       " 168: 'ca',\n",
       " 169: 'happy',\n",
       " 170: 'new',\n",
       " 171: 'country',\n",
       " 172: 'play',\n",
       " 173: 'aah',\n",
       " 174: 'since',\n",
       " 175: 'wish',\n",
       " 176: 'very',\n",
       " 177: 'girl',\n",
       " 178: 'myself',\n",
       " 179: 'shoulder',\n",
       " 180: 'meet',\n",
       " 181: 'now',\n",
       " 182: 'there',\n",
       " 183: 'someday',\n",
       " 184: 'them',\n",
       " 185: 'bad',\n",
       " 186: 'into',\n",
       " 187: 'under',\n",
       " 188: 'skin',\n",
       " 189: 'carry',\n",
       " 190: 'who',\n",
       " 191: 'with',\n",
       " 192: \"you'll\",\n",
       " 193: 'find',\n",
       " 194: 'about',\n",
       " 195: 'told',\n",
       " 196: 'we',\n",
       " 197: 'must',\n",
       " 198: 'makes',\n",
       " 199: 'act',\n",
       " 200: 'spent',\n",
       " 201: 'money',\n",
       " 202: 'kisses',\n",
       " 203: 'bended',\n",
       " 204: 'loving',\n",
       " 205: 'though',\n",
       " 206: 'keep',\n",
       " 207: 'trying',\n",
       " 208: 'mine',\n",
       " 209: 'wind',\n",
       " 210: 'blow',\n",
       " 211: 'shine',\n",
       " 212: 'puts',\n",
       " 213: 'ease',\n",
       " 214: 'threw',\n",
       " 215: 'keys',\n",
       " 216: 'long',\n",
       " 217: 'ever',\n",
       " 218: 'catch',\n",
       " 219: 'name',\n",
       " 220: 'really',\n",
       " 221: 'forever',\n",
       " 222: 'together',\n",
       " 223: 'hear',\n",
       " 224: 'began',\n",
       " 225: 'lose',\n",
       " 226: 'control',\n",
       " 227: 'shivering',\n",
       " 228: \"tryin'\",\n",
       " 229: 'thought',\n",
       " 230: 'swallowing',\n",
       " 231: 'top',\n",
       " 232: 'slide',\n",
       " 233: 'turn',\n",
       " 234: 'ride',\n",
       " 235: 'miles',\n",
       " 236: 'as',\n",
       " 237: 'run',\n",
       " 238: 'pigs',\n",
       " 239: 'fly',\n",
       " 240: 'naughty',\n",
       " 241: 'an',\n",
       " 242: 'english',\n",
       " 243: 'oo',\n",
       " 244: 'good',\n",
       " 245: 'shows',\n",
       " 246: 'honey',\n",
       " 247: 'bonnie',\n",
       " 248: 'ooo',\n",
       " 249: 'cheek',\n",
       " 250: 'guitar',\n",
       " 251: 'used',\n",
       " 252: 'say',\n",
       " 253: 'music',\n",
       " 254: 'saying',\n",
       " 255: 'o',\n",
       " 256: 't',\n",
       " 257: 'spells',\n",
       " 258: 'john',\n",
       " 259: 'lennon',\n",
       " 260: 'year',\n",
       " 261: 'merry',\n",
       " 262: 'listeners',\n",
       " 263: 'reason',\n",
       " 264: 'earth',\n",
       " 265: 'mad',\n",
       " 266: 'lost',\n",
       " 267: 'only',\n",
       " 268: 'had',\n",
       " 269: 'today',\n",
       " 270: 'chip',\n",
       " 271: \"that's\",\n",
       " 272: 'bigger',\n",
       " 273: 'feet',\n",
       " 274: 'talk',\n",
       " 275: 'try',\n",
       " 276: 'somehow',\n",
       " 277: 'shy',\n",
       " 278: 'stare',\n",
       " 279: \"you'd\",\n",
       " 280: 'girls',\n",
       " 281: 'their',\n",
       " 282: 'hearts',\n",
       " 283: 'two',\n",
       " 284: 'show',\n",
       " 285: \"lovin'\",\n",
       " 286: 'until',\n",
       " 287: 'afraid',\n",
       " 288: 'were',\n",
       " 289: 'minute',\n",
       " 290: 'anytime',\n",
       " 291: 'refrain',\n",
       " 292: 'upon',\n",
       " 293: 'shoulders',\n",
       " 294: 'making',\n",
       " 295: 'colder',\n",
       " 296: 'found',\n",
       " 297: \"you're\",\n",
       " 298: 'someone',\n",
       " 299: 'perform',\n",
       " 300: 'movement',\n",
       " 301: 'need',\n",
       " 302: 'sure',\n",
       " 303: 'lets',\n",
       " 304: 'week',\n",
       " 305: 'pappie',\n",
       " 306: \"there'd\",\n",
       " 307: 'times',\n",
       " 308: 'these',\n",
       " 309: 'gang',\n",
       " 310: 'rocking',\n",
       " 311: 'joes',\n",
       " 312: 'cats',\n",
       " 313: 'something',\n",
       " 314: 'heels',\n",
       " 315: 'toes',\n",
       " 316: 'grab',\n",
       " 317: 'squeeze',\n",
       " 318: 'why',\n",
       " 319: 'such',\n",
       " 320: 'doggone',\n",
       " 321: 'tease',\n",
       " 322: 'beg',\n",
       " 323: 'begging',\n",
       " 324: 'knows',\n",
       " 325: 'loved',\n",
       " 326: 'still',\n",
       " 327: 'wait',\n",
       " 328: 'lonely',\n",
       " 329: 'lifetime',\n",
       " 330: 'saw',\n",
       " 331: 'mattered',\n",
       " 332: 'always',\n",
       " 333: 'same',\n",
       " 334: 'whenever',\n",
       " 335: 'apart',\n",
       " 336: 'last',\n",
       " 337: 'fill',\n",
       " 338: 'air',\n",
       " 339: 'sing',\n",
       " 340: 'loud',\n",
       " 341: 'easy',\n",
       " 342: 'near',\n",
       " 343: 'things',\n",
       " 344: 'endear',\n",
       " 345: 'dreaming',\n",
       " 346: 'past',\n",
       " 347: 'beating',\n",
       " 348: 'feeling',\n",
       " 349: 'insecure',\n",
       " 350: 'might',\n",
       " 351: 'not',\n",
       " 352: 'any',\n",
       " 353: 'eyes',\n",
       " 354: 'watch',\n",
       " 355: 'above',\n",
       " 356: 'comes',\n",
       " 357: 'blisters',\n",
       " 358: 'fingers',\n",
       " 359: 'gun',\n",
       " 360: 'cornflake',\n",
       " 361: 'van',\n",
       " 362: 'corporation',\n",
       " 363: 'tee',\n",
       " 364: 'shirt',\n",
       " 365: 'stupid',\n",
       " 366: 'bloody',\n",
       " 367: 'tuesday',\n",
       " 368: 'face',\n",
       " 369: 'grow',\n",
       " 370: 'mister',\n",
       " 371: 'city',\n",
       " 372: 'policeman',\n",
       " 373: 'pretty',\n",
       " 374: 'policemen',\n",
       " 375: 'row',\n",
       " 376: 'lucy',\n",
       " 377: 'sky',\n",
       " 378: 'yellow',\n",
       " 379: 'matter',\n",
       " 380: 'custard',\n",
       " 381: 'dripping',\n",
       " 382: 'dead',\n",
       " 383: \"dog's\",\n",
       " 384: 'eye',\n",
       " 385: 'crabalocker',\n",
       " 386: 'fishwife',\n",
       " 387: 'pornographic',\n",
       " 388: 'priestess',\n",
       " 389: 'knickers',\n",
       " 390: 'eggman',\n",
       " 391: 'eggmen',\n",
       " 392: 'garden',\n",
       " 393: 'tan',\n",
       " 394: 'standing',\n",
       " 395: 'rain',\n",
       " 396: 'expert',\n",
       " 397: 'text',\n",
       " 398: 'pert',\n",
       " 399: 'choking',\n",
       " 400: 'smokers',\n",
       " 401: 'think',\n",
       " 402: 'joker',\n",
       " 403: 'laughs',\n",
       " 404: 'smile',\n",
       " 405: 'sty',\n",
       " 406: 'snide',\n",
       " 407: 'semolina',\n",
       " 408: 'pilchard',\n",
       " 409: 'climbing',\n",
       " 410: 'eiffel',\n",
       " 411: 'tower',\n",
       " 412: 'elementary',\n",
       " 413: 'penguin',\n",
       " 414: 'singing',\n",
       " 415: 'hari',\n",
       " 416: 'krishna',\n",
       " 417: 'should',\n",
       " 418: 'seen',\n",
       " 419: 'kicking',\n",
       " 420: 'edgar',\n",
       " 421: 'allan',\n",
       " 422: 'poe',\n",
       " 423: 'cachou',\n",
       " 424: 'doubt',\n",
       " 425: 'kiss',\n",
       " 426: 'goosepimples',\n",
       " 427: 'call',\n",
       " 428: 'mercy',\n",
       " 429: 'rock',\n",
       " 430: 'gone',\n",
       " 431: 'puppy',\n",
       " 432: 'touch',\n",
       " 433: 'sewed',\n",
       " 434: 'electric',\n",
       " 435: 'wire',\n",
       " 436: 'much',\n",
       " 437: 'looking',\n",
       " 438: 'cooking',\n",
       " 439: 'temperature',\n",
       " 440: 'low',\n",
       " 441: 'fever',\n",
       " 442: 'high',\n",
       " 443: 'speak',\n",
       " 444: 'too',\n",
       " 445: 'weak',\n",
       " 446: 'deep',\n",
       " 447: 'louisianna',\n",
       " 448: 'close',\n",
       " 449: 'orleans',\n",
       " 450: 'woods',\n",
       " 451: 'among',\n",
       " 452: 'evergreens',\n",
       " 453: 'stand',\n",
       " 454: 'cabin',\n",
       " 455: 'clay',\n",
       " 456: 'wood',\n",
       " 457: 'lives',\n",
       " 458: 'young',\n",
       " 459: 'named',\n",
       " 460: 'learned',\n",
       " 461: 'read',\n",
       " 462: 'or',\n",
       " 463: 'write',\n",
       " 464: 'book',\n",
       " 465: 'ringing',\n",
       " 466: 'bell',\n",
       " 467: 'gunny',\n",
       " 468: 'sack',\n",
       " 469: 'sit',\n",
       " 470: 'beneath',\n",
       " 471: 'railroad',\n",
       " 472: 'track',\n",
       " 473: 'playing',\n",
       " 474: 'shade',\n",
       " 475: 'drumming',\n",
       " 476: 'rhythm',\n",
       " 477: 'drivers',\n",
       " 478: 'passing',\n",
       " 479: 'mama',\n",
       " 480: 'him',\n",
       " 481: 'leader',\n",
       " 482: 'big',\n",
       " 483: 'old',\n",
       " 484: 'band',\n",
       " 485: 'many',\n",
       " 486: 'around',\n",
       " 487: 'goes',\n",
       " 488: 'maybe',\n",
       " 489: 'light',\n",
       " 490: 'tonight',\n",
       " 491: 'continues',\n",
       " 492: 'fades',\n",
       " 493: 'background',\n",
       " 494: 'spoken',\n",
       " 495: 'paul',\n",
       " 496: 'mccartney',\n",
       " 497: 'everything',\n",
       " 498: 'yourself',\n",
       " 499: 'behalf',\n",
       " 500: 'beatles',\n",
       " 501: 'george',\n",
       " 502: 'harrison',\n",
       " 503: 'speaking',\n",
       " 504: 'opportunity',\n",
       " 505: 'everywhere',\n",
       " 506: 'ringo',\n",
       " 507: 'starr',\n",
       " 508: 'pastiche',\n",
       " 509: 'point',\n",
       " 510: 'understand',\n",
       " 511: '<unk>'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "511"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "511"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = text_generator(files)\n",
    "text = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I've got every reason on earth to be mad\\n'Cause I just lost the only girl I had\\nIf I could get my way\\nI'd get myself locked up today\\nBut I can't, so I'll cry instead\\n\\nI've got a chip on my shoulder that's bigger that my feet\\nI can't talk to people that I meet\\nIf I could see you now\\nI'd try to make you sad somehow\\nBut I can't, so I'll cry instead\\n\\nDon't want to cry when there's people there\\nI get shy when they start to stare\\nI'm gonna hide myself away\\nBut I'll come back again someday\\n\\nAnd when you do you'd better hide all the girls\\nI'm gonna break their hearts all round the world\\nYes, I'm gonna break them in two\\nAnd show you what your lovin' man can do\\nUntil then I'll cry instead\\n\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[85, 75, 108, 263, 12, 264, 6, 31, 265],\n",
       " [126, 4, 50, 266, 3, 267, 177, 4, 268],\n",
       " [86, 4, 127, 39, 13, 109],\n",
       " [87, 39, 178, 128, 129, 269],\n",
       " [11, 4, 110, 76, 111, 67, 130],\n",
       " [],\n",
       " [85, 75, 7, 270, 12, 13, 179, 271, 272, 26, 13, 273],\n",
       " [4, 110, 274, 6, 112, 26, 4, 180],\n",
       " [86, 4, 127, 40, 2, 181],\n",
       " [87, 275, 6, 27, 2, 131, 276],\n",
       " [11, 4, 110, 76, 111, 67, 130],\n",
       " [],\n",
       " [28, 60, 6, 67, 32, 33, 112, 182],\n",
       " [4, 39, 277, 32, 41, 132, 6, 278],\n",
       " [9, 133, 134, 178, 135],\n",
       " [11, 111, 77, 113, 20, 183],\n",
       " [],\n",
       " [8, 32, 2, 56, 279, 34, 134, 22, 3, 280],\n",
       " [9, 133, 114, 281, 282, 22, 115, 3, 136],\n",
       " [116, 9, 133, 114, 184, 30, 283],\n",
       " [8, 284, 2, 137, 23, 285, 68, 88, 56],\n",
       " [286, 69, 111, 67, 130],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = tokenizer.texts_to_sequences(text.split('\\n'))\n",
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"i've\", 'got', 'every', 'reason', 'on', 'earth', 'to', 'be', 'mad'],\n",
       " [\"'cause\", 'i', 'just', 'lost', 'the', 'only', 'girl', 'i', 'had'],\n",
       " ['if', 'i', 'could', 'get', 'my', 'way'],\n",
       " [\"i'd\", 'get', 'myself', 'locked', 'up', 'today'],\n",
       " ['but', 'i', \"can't\", 'so', \"i'll\", 'cry', 'instead'],\n",
       " [],\n",
       " [\"i've\",\n",
       "  'got',\n",
       "  'a',\n",
       "  'chip',\n",
       "  'on',\n",
       "  'my',\n",
       "  'shoulder',\n",
       "  \"that's\",\n",
       "  'bigger',\n",
       "  'that',\n",
       "  'my',\n",
       "  'feet'],\n",
       " ['i', \"can't\", 'talk', 'to', 'people', 'that', 'i', 'meet'],\n",
       " ['if', 'i', 'could', 'see', 'you', 'now'],\n",
       " [\"i'd\", 'try', 'to', 'make', 'you', 'sad', 'somehow'],\n",
       " ['but', 'i', \"can't\", 'so', \"i'll\", 'cry', 'instead'],\n",
       " [],\n",
       " [\"don't\", 'want', 'to', 'cry', 'when', \"there's\", 'people', 'there'],\n",
       " ['i', 'get', 'shy', 'when', 'they', 'start', 'to', 'stare'],\n",
       " [\"i'm\", 'gonna', 'hide', 'myself', 'away'],\n",
       " ['but', \"i'll\", 'come', 'back', 'again', 'someday'],\n",
       " [],\n",
       " ['and',\n",
       "  'when',\n",
       "  'you',\n",
       "  'do',\n",
       "  \"you'd\",\n",
       "  'better',\n",
       "  'hide',\n",
       "  'all',\n",
       "  'the',\n",
       "  'girls'],\n",
       " [\"i'm\", 'gonna', 'break', 'their', 'hearts', 'all', 'round', 'the', 'world'],\n",
       " ['yes', \"i'm\", 'gonna', 'break', 'them', 'in', 'two'],\n",
       " ['and', 'show', 'you', 'what', 'your', \"lovin'\", 'man', 'can', 'do'],\n",
       " ['until', 'then', \"i'll\", 'cry', 'instead'],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[index_to_word[i] for i in L] for L in sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 9)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequence), len(sequence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,  85,  75, 108, 263,  12, 264,   6,  31, 265],\n",
       "       [  0,   0,   0, 126,   4,  50, 266,   3, 267, 177,   4, 268],\n",
       "       [  0,   0,   0,   0,   0,   0,  86,   4, 127,  39,  13, 109],\n",
       "       [  0,   0,   0,   0,   0,   0,  87,  39, 178, 128, 129, 269],\n",
       "       [  0,   0,   0,   0,   0,  11,   4, 110,  76, 111,  67, 130],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [ 85,  75,   7, 270,  12,  13, 179, 271, 272,  26,  13, 273],\n",
       "       [  0,   0,   0,   0,   4, 110, 274,   6, 112,  26,   4, 180],\n",
       "       [  0,   0,   0,   0,   0,   0,  86,   4, 127,  40,   2, 181],\n",
       "       [  0,   0,   0,   0,   0,  87, 275,   6,  27,   2, 131, 276],\n",
       "       [  0,   0,   0,   0,   0,  11,   4, 110,  76, 111,  67, 130],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,  28,  60,   6,  67,  32,  33, 112, 182],\n",
       "       [  0,   0,   0,   0,   4,  39, 277,  32,  41, 132,   6, 278],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   9, 133, 134, 178, 135],\n",
       "       [  0,   0,   0,   0,   0,   0,  11, 111,  77, 113,  20, 183],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   8,  32,   2,  56, 279,  34, 134,  22,   3, 280],\n",
       "       [  0,   0,   0,   9, 133, 114, 281, 282,  22, 115,   3, 136],\n",
       "       [  0,   0,   0,   0,   0, 116,   9, 133, 114, 184,  30, 283],\n",
       "       [  0,   0,   0,   8, 284,   2, 137,  23, 285,  68,  88,  56],\n",
       "       [  0,   0,   0,   0,   0,   0,   0, 286,  69, 111,  67, 130],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "pad_sequences(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  \"i've\",\n",
       "  'got',\n",
       "  'every',\n",
       "  'reason',\n",
       "  'on',\n",
       "  'earth',\n",
       "  'to',\n",
       "  'be',\n",
       "  'mad'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  \"'cause\",\n",
       "  'i',\n",
       "  'just',\n",
       "  'lost',\n",
       "  'the',\n",
       "  'only',\n",
       "  'girl',\n",
       "  'i',\n",
       "  'had'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  'if',\n",
       "  'i',\n",
       "  'could',\n",
       "  'get',\n",
       "  'my',\n",
       "  'way'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  \"i'd\",\n",
       "  'get',\n",
       "  'myself',\n",
       "  'locked',\n",
       "  'up',\n",
       "  'today'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  'but',\n",
       "  'i',\n",
       "  \"can't\",\n",
       "  'so',\n",
       "  \"i'll\",\n",
       "  'cry',\n",
       "  'instead'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>'],\n",
       " [\"i've\",\n",
       "  'got',\n",
       "  'a',\n",
       "  'chip',\n",
       "  'on',\n",
       "  'my',\n",
       "  'shoulder',\n",
       "  \"that's\",\n",
       "  'bigger',\n",
       "  'that',\n",
       "  'my',\n",
       "  'feet'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  'i',\n",
       "  \"can't\",\n",
       "  'talk',\n",
       "  'to',\n",
       "  'people',\n",
       "  'that',\n",
       "  'i',\n",
       "  'meet'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  'if',\n",
       "  'i',\n",
       "  'could',\n",
       "  'see',\n",
       "  'you',\n",
       "  'now'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  \"i'd\",\n",
       "  'try',\n",
       "  'to',\n",
       "  'make',\n",
       "  'you',\n",
       "  'sad',\n",
       "  'somehow'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  'but',\n",
       "  'i',\n",
       "  \"can't\",\n",
       "  'so',\n",
       "  \"i'll\",\n",
       "  'cry',\n",
       "  'instead'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  \"don't\",\n",
       "  'want',\n",
       "  'to',\n",
       "  'cry',\n",
       "  'when',\n",
       "  \"there's\",\n",
       "  'people',\n",
       "  'there'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  'i',\n",
       "  'get',\n",
       "  'shy',\n",
       "  'when',\n",
       "  'they',\n",
       "  'start',\n",
       "  'to',\n",
       "  'stare'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  \"i'm\",\n",
       "  'gonna',\n",
       "  'hide',\n",
       "  'myself',\n",
       "  'away'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  'but',\n",
       "  \"i'll\",\n",
       "  'come',\n",
       "  'back',\n",
       "  'again',\n",
       "  'someday'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  'and',\n",
       "  'when',\n",
       "  'you',\n",
       "  'do',\n",
       "  \"you'd\",\n",
       "  'better',\n",
       "  'hide',\n",
       "  'all',\n",
       "  'the',\n",
       "  'girls'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  \"i'm\",\n",
       "  'gonna',\n",
       "  'break',\n",
       "  'their',\n",
       "  'hearts',\n",
       "  'all',\n",
       "  'round',\n",
       "  'the',\n",
       "  'world'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  'yes',\n",
       "  \"i'm\",\n",
       "  'gonna',\n",
       "  'break',\n",
       "  'them',\n",
       "  'in',\n",
       "  'two'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  'and',\n",
       "  'show',\n",
       "  'you',\n",
       "  'what',\n",
       "  'your',\n",
       "  \"lovin'\",\n",
       "  'man',\n",
       "  'can',\n",
       "  'do'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  'until',\n",
       "  'then',\n",
       "  \"i'll\",\n",
       "  'cry',\n",
       "  'instead'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[index_to_word[i] for i in x] for x in pad_sequences(sequence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[85, 75, 108, 263, 12, 264, 6, 31, 265]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = sequence[0]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = tokenizer.sequences_to_matrix([[i] for i in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,  85],\n",
       "       [  1,  75],\n",
       "       [  2, 108],\n",
       "       [  3, 263],\n",
       "       [  4,  12],\n",
       "       [  5, 264],\n",
       "       [  6,   6],\n",
       "       [  7,  31],\n",
       "       [  8, 265]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only one per row\n",
    "import numpy as np\n",
    "np.argwhere(one_hot == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = lambda x: x.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequencer(tokens):\n",
    "    return [tokens[:i] for i in range(1, len(tokens)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'a ',\n",
       " 'a q',\n",
       " 'a qu',\n",
       " 'a qui',\n",
       " 'a quic',\n",
       " 'a quick',\n",
       " 'a quick ',\n",
       " 'a quick b',\n",
       " 'a quick br',\n",
       " 'a quick bro',\n",
       " 'a quick brow',\n",
       " 'a quick brown',\n",
       " 'a quick brown ',\n",
       " 'a quick brown f',\n",
       " 'a quick brown fo',\n",
       " 'a quick brown fox']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sequencer('a quick brown fox'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def batch_generator(files, maxlen, batch_size=32, splitter=None, sequencer=None, epoch_end=0):\n",
    "    batch_sequences = []\n",
    "    while True:\n",
    "        random.shuffle(files)\n",
    "        for text in text_generator(files, splitter):\n",
    "            tokens = tokenizer.texts_to_sequences([text])[0]\n",
    "            sequences = sequencer(tokens)\n",
    "            batch_sequences.extend(sequences)\n",
    "            \n",
    "            if len(batch_sequences) >= batch_size:\n",
    "                X = pad_sequences(batch_sequences, maxlen=maxlen)\n",
    "                y = tokenizer.sequences_to_matrix([[i] for seq in X for i in seq])\n",
    "                y = y.reshape((len(batch_sequences), maxlen, tokenizer.num_words))\n",
    "                \n",
    "                # offset input/output\n",
    "                X = [X[:-1], X[:-1]]\n",
    "                y = y[1:]\n",
    "                \n",
    "                # reset\n",
    "                batch_sequences = []\n",
    "                \n",
    "                yield X, y\n",
    "        yield epoch_end\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "batch_gen = batch_generator(files, 10, splitter=splitter, sequencer=sequencer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(batch_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37, 10), (37, 10), (37, 10, 10000))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape, X[1].shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,  32],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  32,   4],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  32,   4,  39],\n",
       "       [  0,   0,   0,   0,   0,   0,  32,   4,  39,   6],\n",
       "       [  0,   0,   0,   0,   0,  32,   4,  39,   6,   3],\n",
       "       [  0,   0,   0,   0,  32,   4,  39,   6,   3, 121],\n",
       "       [  0,   0,   0,  32,   4,  39,   6,   3, 121,   4],\n",
       "       [  0,   0,  32,   4,  39,   6,   3, 121,   4,   5],\n",
       "       [  0,  32,   4,  39,   6,   3, 121,   4,   5, 113],\n",
       "       [ 32,   4,  39,   6,   3, 121,   4,   5, 113,   6],\n",
       "       [  4,  39,   6,   3, 121,   4,   5, 113,   6,   3],\n",
       "       [ 39,   6,   3, 121,   4,   5, 113,   6,   3, 231],\n",
       "       [  6,   3, 121,   4,   5, 113,   6,   3, 231,  65],\n",
       "       [  3, 121,   4,   5, 113,   6,   3, 231,  65,   3],\n",
       "       [121,   4,   5, 113,   6,   3, 231,  65,   3, 232],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0, 161],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0, 161,   4],\n",
       "       [  0,   0,   0,   0,   0,   0,   0, 161,   4, 162],\n",
       "       [  0,   0,   0,   0,   0,   0, 161,   4, 162,   8],\n",
       "       [  0,   0,   0,   0,   0, 161,   4, 162,   8,   4],\n",
       "       [  0,   0,   0,   0, 161,   4, 162,   8,   4, 233],\n",
       "       [  0,   0,   0, 161,   4, 162,   8,   4, 233,   8],\n",
       "       [  0,   0, 161,   4, 162,   8,   4, 233,   8,   4],\n",
       "       [  0, 161,   4, 162,   8,   4, 233,   8,   4,   5],\n",
       "       [161,   4, 162,   8,   4, 233,   8,   4,   5,  43],\n",
       "       [  4, 162,   8,   4, 233,   8,   4,   5,  43,   7],\n",
       "       [162,   8,   4, 233,   8,   4,   5,  43,   7, 234],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  92],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  92,   4],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  92,   4,  39],\n",
       "       [  0,   0,   0,   0,   0,   0,  92,   4,  39,   6],\n",
       "       [  0,   0,   0,   0,   0,  92,   4,  39,   6,   3],\n",
       "       [  0,   0,   0,   0,  92,   4,  39,   6,   3, 121],\n",
       "       [  0,   0,   0,  92,   4,  39,   6,   3, 121,   8],\n",
       "       [  0,   0,  92,   4,  39,   6,   3, 121,   8,   4],\n",
       "       [  0,  92,   4,  39,   6,   3, 121,   8,   4,  40],\n",
       "       [ 92,   4,  39,   6,   3, 121,   8,   4,  40,   2]], dtype=int32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, x2 = X\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  'when'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  'when',\n",
       "  'i'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  'when',\n",
       "  'i',\n",
       "  'get'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  'when',\n",
       "  'i',\n",
       "  'get',\n",
       "  'to'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  'when',\n",
       "  'i',\n",
       "  'get',\n",
       "  'to',\n",
       "  'the'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  'when',\n",
       "  'i',\n",
       "  'get',\n",
       "  'to',\n",
       "  'the',\n",
       "  'bottom'],\n",
       " ['<pad>', '<pad>', '<pad>', 'when', 'i', 'get', 'to', 'the', 'bottom', 'i'],\n",
       " ['<pad>', '<pad>', 'when', 'i', 'get', 'to', 'the', 'bottom', 'i', 'go'],\n",
       " ['<pad>', 'when', 'i', 'get', 'to', 'the', 'bottom', 'i', 'go', 'back'],\n",
       " ['when', 'i', 'get', 'to', 'the', 'bottom', 'i', 'go', 'back', 'to'],\n",
       " ['i', 'get', 'to', 'the', 'bottom', 'i', 'go', 'back', 'to', 'the'],\n",
       " ['get', 'to', 'the', 'bottom', 'i', 'go', 'back', 'to', 'the', 'top'],\n",
       " ['to', 'the', 'bottom', 'i', 'go', 'back', 'to', 'the', 'top', 'of'],\n",
       " ['the', 'bottom', 'i', 'go', 'back', 'to', 'the', 'top', 'of', 'the'],\n",
       " ['bottom', 'i', 'go', 'back', 'to', 'the', 'top', 'of', 'the', 'slide'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  'where'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  'where',\n",
       "  'i'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  'where',\n",
       "  'i',\n",
       "  'stop'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  'where',\n",
       "  'i',\n",
       "  'stop',\n",
       "  'and'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  'where',\n",
       "  'i',\n",
       "  'stop',\n",
       "  'and',\n",
       "  'i'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  'where',\n",
       "  'i',\n",
       "  'stop',\n",
       "  'and',\n",
       "  'i',\n",
       "  'turn'],\n",
       " ['<pad>', '<pad>', '<pad>', 'where', 'i', 'stop', 'and', 'i', 'turn', 'and'],\n",
       " ['<pad>', '<pad>', 'where', 'i', 'stop', 'and', 'i', 'turn', 'and', 'i'],\n",
       " ['<pad>', 'where', 'i', 'stop', 'and', 'i', 'turn', 'and', 'i', 'go'],\n",
       " ['where', 'i', 'stop', 'and', 'i', 'turn', 'and', 'i', 'go', 'for'],\n",
       " ['i', 'stop', 'and', 'i', 'turn', 'and', 'i', 'go', 'for', 'a'],\n",
       " ['stop', 'and', 'i', 'turn', 'and', 'i', 'go', 'for', 'a', 'ride'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  'till'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  'till',\n",
       "  'i'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  'till',\n",
       "  'i',\n",
       "  'get'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  'till',\n",
       "  'i',\n",
       "  'get',\n",
       "  'to'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  'till',\n",
       "  'i',\n",
       "  'get',\n",
       "  'to',\n",
       "  'the'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  'till',\n",
       "  'i',\n",
       "  'get',\n",
       "  'to',\n",
       "  'the',\n",
       "  'bottom'],\n",
       " ['<pad>', '<pad>', '<pad>', 'till', 'i', 'get', 'to', 'the', 'bottom', 'and'],\n",
       " ['<pad>', '<pad>', 'till', 'i', 'get', 'to', 'the', 'bottom', 'and', 'i'],\n",
       " ['<pad>', 'till', 'i', 'get', 'to', 'the', 'bottom', 'and', 'i', 'see'],\n",
       " ['till', 'i', 'get', 'to', 'the', 'bottom', 'and', 'i', 'see', 'you']]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[index_to_word.get(i, '<pad>') for i in x] for x in x1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum(axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_heads = 8\n",
    "n_layers = 6\n",
    "d_model = 64*n_heads\n",
    "vocab_size = tokenizer.num_words\n",
    "sequence_len = 8\n",
    "warmup_steps = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_gen = batch_generator(files, sequence_len, splitter=splitter, sequencer=sequencer, epoch_end=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_batch_size = 0\n",
    "min_batch_size = 1e10\n",
    "mean_batch_size = 0\n",
    "steps_per_epoch = 0\n",
    "\n",
    "# loop over batch generator until we hit the end of the epoch\n",
    "# and compute some stats along the way\n",
    "for batch in batch_gen:\n",
    "    try:\n",
    "        (x1, x2), y = batch\n",
    "    except TypeError:\n",
    "        break\n",
    "    max_batch_size = max(max_batch_size, len(x1))\n",
    "    min_batch_size = min(min_batch_size, len(x1))\n",
    "    mean_batch_size += len(x1)\n",
    "    steps_per_epoch += 1\n",
    "mean_batch_size /= steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps per epoch 65\n",
      "max batch size 42\n",
      "min batch size 31\n",
      "mean batch size 33.89230769230769\n"
     ]
    }
   ],
   "source": [
    "print('steps per epoch', steps_per_epoch)\n",
    "print('max batch size', max_batch_size)\n",
    "print('min batch size', min_batch_size)\n",
    "print('mean batch size', mean_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = (X for X in batch_gen if not X == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TerminateOnNaN\n",
    "callbacks = [TerminateOnNaN()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Transformer\n",
    "model = Transformer(\n",
    "        n_heads=n_heads, encoder_layers=n_layers, decoder_layers=n_layers,\n",
    "        d_model=d_model, vocab_size=vocab_size, sequence_len=sequence_len,\n",
    "        layer_normalization=True, dropout=True,\n",
    "        residual_connections=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 8, 512)       5120000     encoder_input[0][0]              \n",
      "                                                                 decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "positional_encoding_4 (Position (None, 8, 512)       0           embedding[0][0]                  \n",
      "                                                                 embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_scalar (Scalar)       (None, 8, 512)       0           positional_encoding_4[0][0]      \n",
      "                                                                 positional_encoding_4[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_97 (Dropout)            (None, 8, 512)       0           embedding_scalar[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer1_mha (MultiHeadAt (None, 8, 512)       262144      dropout_97[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_99 (Dropout)            (None, 8, 512)       0           encoder_layer1_mha[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer1_residual1 (Add)  (None, 8, 512)       0           dropout_97[0][0]                 \n",
      "                                                                 dropout_99[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer1_layernorm1 (Laye (None, 8, 512)       8192        encoder_layer1_residual1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer1_ffn1 (Dense)     (None, 8, 512)       262656      encoder_layer1_layernorm1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer1_ffn2 (Dense)     (None, 8, 512)       262656      encoder_layer1_ffn1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_100 (Dropout)           (None, 8, 512)       0           encoder_layer1_ffn2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer1_residual2 (Add)  (None, 8, 512)       0           encoder_layer1_layernorm1[0][0]  \n",
      "                                                                 dropout_100[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer1_layernorm2 (Laye (None, 8, 512)       8192        encoder_layer1_residual2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer2_mha (MultiHeadAt (None, 8, 512)       262144      encoder_layer1_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_101 (Dropout)           (None, 8, 512)       0           encoder_layer2_mha[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer2_residual1 (Add)  (None, 8, 512)       0           encoder_layer1_layernorm2[0][0]  \n",
      "                                                                 dropout_101[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer2_layernorm1 (Laye (None, 8, 512)       8192        encoder_layer2_residual1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer2_ffn1 (Dense)     (None, 8, 512)       262656      encoder_layer2_layernorm1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer2_ffn2 (Dense)     (None, 8, 512)       262656      encoder_layer2_ffn1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_102 (Dropout)           (None, 8, 512)       0           encoder_layer2_ffn2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer2_residual2 (Add)  (None, 8, 512)       0           encoder_layer2_layernorm1[0][0]  \n",
      "                                                                 dropout_102[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer2_layernorm2 (Laye (None, 8, 512)       8192        encoder_layer2_residual2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer3_mha (MultiHeadAt (None, 8, 512)       262144      encoder_layer2_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_103 (Dropout)           (None, 8, 512)       0           encoder_layer3_mha[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer3_residual1 (Add)  (None, 8, 512)       0           encoder_layer2_layernorm2[0][0]  \n",
      "                                                                 dropout_103[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer3_layernorm1 (Laye (None, 8, 512)       8192        encoder_layer3_residual1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer3_ffn1 (Dense)     (None, 8, 512)       262656      encoder_layer3_layernorm1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer3_ffn2 (Dense)     (None, 8, 512)       262656      encoder_layer3_ffn1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_104 (Dropout)           (None, 8, 512)       0           encoder_layer3_ffn2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer3_residual2 (Add)  (None, 8, 512)       0           encoder_layer3_layernorm1[0][0]  \n",
      "                                                                 dropout_104[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer3_layernorm2 (Laye (None, 8, 512)       8192        encoder_layer3_residual2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer4_mha (MultiHeadAt (None, 8, 512)       262144      encoder_layer3_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_105 (Dropout)           (None, 8, 512)       0           encoder_layer4_mha[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer4_residual1 (Add)  (None, 8, 512)       0           encoder_layer3_layernorm2[0][0]  \n",
      "                                                                 dropout_105[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer4_layernorm1 (Laye (None, 8, 512)       8192        encoder_layer4_residual1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer4_ffn1 (Dense)     (None, 8, 512)       262656      encoder_layer4_layernorm1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer4_ffn2 (Dense)     (None, 8, 512)       262656      encoder_layer4_ffn1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_106 (Dropout)           (None, 8, 512)       0           encoder_layer4_ffn2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer4_residual2 (Add)  (None, 8, 512)       0           encoder_layer4_layernorm1[0][0]  \n",
      "                                                                 dropout_106[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer4_layernorm2 (Laye (None, 8, 512)       8192        encoder_layer4_residual2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer5_mha (MultiHeadAt (None, 8, 512)       262144      encoder_layer4_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_107 (Dropout)           (None, 8, 512)       0           encoder_layer5_mha[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer5_residual1 (Add)  (None, 8, 512)       0           encoder_layer4_layernorm2[0][0]  \n",
      "                                                                 dropout_107[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer5_layernorm1 (Laye (None, 8, 512)       8192        encoder_layer5_residual1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer5_ffn1 (Dense)     (None, 8, 512)       262656      encoder_layer5_layernorm1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer5_ffn2 (Dense)     (None, 8, 512)       262656      encoder_layer5_ffn1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_108 (Dropout)           (None, 8, 512)       0           encoder_layer5_ffn2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer5_residual2 (Add)  (None, 8, 512)       0           encoder_layer5_layernorm1[0][0]  \n",
      "                                                                 dropout_108[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer5_layernorm2 (Laye (None, 8, 512)       8192        encoder_layer5_residual2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer6_mha (MultiHeadAt (None, 8, 512)       262144      encoder_layer5_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_109 (Dropout)           (None, 8, 512)       0           encoder_layer6_mha[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer6_residual1 (Add)  (None, 8, 512)       0           encoder_layer5_layernorm2[0][0]  \n",
      "                                                                 dropout_109[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer6_layernorm1 (Laye (None, 8, 512)       8192        encoder_layer6_residual1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_98 (Dropout)            (None, 8, 512)       0           embedding_scalar[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer6_ffn1 (Dense)     (None, 8, 512)       262656      encoder_layer6_layernorm1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer1_mha1 (MultiHeadA (None, 8, 512)       262144      dropout_98[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer6_ffn2 (Dense)     (None, 8, 512)       262656      encoder_layer6_ffn1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_111 (Dropout)           (None, 8, 512)       0           decoder_layer1_mha1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_110 (Dropout)           (None, 8, 512)       0           encoder_layer6_ffn2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer1_residual1 (Add)  (None, 8, 512)       0           dropout_98[0][0]                 \n",
      "                                                                 dropout_111[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer6_residual2 (Add)  (None, 8, 512)       0           encoder_layer6_layernorm1[0][0]  \n",
      "                                                                 dropout_110[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer1_layernorm1 (Laye (None, 8, 512)       8192        decoder_layer1_residual1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer6_layernorm2 (Laye (None, 8, 512)       8192        encoder_layer6_residual2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer1_mha2 (MultiHeadA (None, 8, 512)       262144      decoder_layer1_layernorm1[0][0]  \n",
      "                                                                 encoder_layer6_layernorm2[0][0]  \n",
      "                                                                 encoder_layer6_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_112 (Dropout)           (None, 8, 512)       0           decoder_layer1_mha2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer1_residual2 (Add)  (None, 8, 512)       0           decoder_layer1_layernorm1[0][0]  \n",
      "                                                                 dropout_112[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer1_layernorm2 (Laye (None, 8, 512)       8192        decoder_layer1_residual2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer1_ffn1 (Dense)     (None, 8, 512)       262656      decoder_layer1_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer1_ffn2 (Dense)     (None, 8, 512)       262656      decoder_layer1_ffn1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_113 (Dropout)           (None, 8, 512)       0           decoder_layer1_ffn2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer1_residual3 (Add)  (None, 8, 512)       0           decoder_layer1_layernorm2[0][0]  \n",
      "                                                                 dropout_113[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer1_layernorm3 (Laye (None, 8, 512)       8192        decoder_layer1_residual3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer2_mha1 (MultiHeadA (None, 8, 512)       262144      decoder_layer1_layernorm3[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_114 (Dropout)           (None, 8, 512)       0           decoder_layer2_mha1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer2_residual1 (Add)  (None, 8, 512)       0           decoder_layer1_layernorm3[0][0]  \n",
      "                                                                 dropout_114[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer2_layernorm1 (Laye (None, 8, 512)       8192        decoder_layer2_residual1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer2_mha2 (MultiHeadA (None, 8, 512)       262144      decoder_layer2_layernorm1[0][0]  \n",
      "                                                                 encoder_layer6_layernorm2[0][0]  \n",
      "                                                                 encoder_layer6_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_115 (Dropout)           (None, 8, 512)       0           decoder_layer2_mha2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer2_residual2 (Add)  (None, 8, 512)       0           decoder_layer2_layernorm1[0][0]  \n",
      "                                                                 dropout_115[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer2_layernorm2 (Laye (None, 8, 512)       8192        decoder_layer2_residual2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer2_ffn1 (Dense)     (None, 8, 512)       262656      decoder_layer2_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer2_ffn2 (Dense)     (None, 8, 512)       262656      decoder_layer2_ffn1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_116 (Dropout)           (None, 8, 512)       0           decoder_layer2_ffn2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer2_residual3 (Add)  (None, 8, 512)       0           decoder_layer2_layernorm2[0][0]  \n",
      "                                                                 dropout_116[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer2_layernorm3 (Laye (None, 8, 512)       8192        decoder_layer2_residual3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer3_mha1 (MultiHeadA (None, 8, 512)       262144      decoder_layer2_layernorm3[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_117 (Dropout)           (None, 8, 512)       0           decoder_layer3_mha1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer3_residual1 (Add)  (None, 8, 512)       0           decoder_layer2_layernorm3[0][0]  \n",
      "                                                                 dropout_117[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer3_layernorm1 (Laye (None, 8, 512)       8192        decoder_layer3_residual1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer3_mha2 (MultiHeadA (None, 8, 512)       262144      decoder_layer3_layernorm1[0][0]  \n",
      "                                                                 encoder_layer6_layernorm2[0][0]  \n",
      "                                                                 encoder_layer6_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_118 (Dropout)           (None, 8, 512)       0           decoder_layer3_mha2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer3_residual2 (Add)  (None, 8, 512)       0           decoder_layer3_layernorm1[0][0]  \n",
      "                                                                 dropout_118[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer3_layernorm2 (Laye (None, 8, 512)       8192        decoder_layer3_residual2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer3_ffn1 (Dense)     (None, 8, 512)       262656      decoder_layer3_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer3_ffn2 (Dense)     (None, 8, 512)       262656      decoder_layer3_ffn1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_119 (Dropout)           (None, 8, 512)       0           decoder_layer3_ffn2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer3_residual3 (Add)  (None, 8, 512)       0           decoder_layer3_layernorm2[0][0]  \n",
      "                                                                 dropout_119[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer3_layernorm3 (Laye (None, 8, 512)       8192        decoder_layer3_residual3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer4_mha1 (MultiHeadA (None, 8, 512)       262144      decoder_layer3_layernorm3[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_120 (Dropout)           (None, 8, 512)       0           decoder_layer4_mha1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer4_residual1 (Add)  (None, 8, 512)       0           decoder_layer3_layernorm3[0][0]  \n",
      "                                                                 dropout_120[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer4_layernorm1 (Laye (None, 8, 512)       8192        decoder_layer4_residual1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer4_mha2 (MultiHeadA (None, 8, 512)       262144      decoder_layer4_layernorm1[0][0]  \n",
      "                                                                 encoder_layer6_layernorm2[0][0]  \n",
      "                                                                 encoder_layer6_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_121 (Dropout)           (None, 8, 512)       0           decoder_layer4_mha2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer4_residual2 (Add)  (None, 8, 512)       0           decoder_layer4_layernorm1[0][0]  \n",
      "                                                                 dropout_121[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer4_layernorm2 (Laye (None, 8, 512)       8192        decoder_layer4_residual2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer4_ffn1 (Dense)     (None, 8, 512)       262656      decoder_layer4_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer4_ffn2 (Dense)     (None, 8, 512)       262656      decoder_layer4_ffn1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_122 (Dropout)           (None, 8, 512)       0           decoder_layer4_ffn2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer4_residual3 (Add)  (None, 8, 512)       0           decoder_layer4_layernorm2[0][0]  \n",
      "                                                                 dropout_122[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer4_layernorm3 (Laye (None, 8, 512)       8192        decoder_layer4_residual3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer5_mha1 (MultiHeadA (None, 8, 512)       262144      decoder_layer4_layernorm3[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_123 (Dropout)           (None, 8, 512)       0           decoder_layer5_mha1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer5_residual1 (Add)  (None, 8, 512)       0           decoder_layer4_layernorm3[0][0]  \n",
      "                                                                 dropout_123[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer5_layernorm1 (Laye (None, 8, 512)       8192        decoder_layer5_residual1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer5_mha2 (MultiHeadA (None, 8, 512)       262144      decoder_layer5_layernorm1[0][0]  \n",
      "                                                                 encoder_layer6_layernorm2[0][0]  \n",
      "                                                                 encoder_layer6_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_124 (Dropout)           (None, 8, 512)       0           decoder_layer5_mha2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer5_residual2 (Add)  (None, 8, 512)       0           decoder_layer5_layernorm1[0][0]  \n",
      "                                                                 dropout_124[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer5_layernorm2 (Laye (None, 8, 512)       8192        decoder_layer5_residual2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer5_ffn1 (Dense)     (None, 8, 512)       262656      decoder_layer5_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer5_ffn2 (Dense)     (None, 8, 512)       262656      decoder_layer5_ffn1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_125 (Dropout)           (None, 8, 512)       0           decoder_layer5_ffn2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer5_residual3 (Add)  (None, 8, 512)       0           decoder_layer5_layernorm2[0][0]  \n",
      "                                                                 dropout_125[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer5_layernorm3 (Laye (None, 8, 512)       8192        decoder_layer5_residual3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer6_mha1 (MultiHeadA (None, 8, 512)       262144      decoder_layer5_layernorm3[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_126 (Dropout)           (None, 8, 512)       0           decoder_layer6_mha1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer6_residual1 (Add)  (None, 8, 512)       0           decoder_layer5_layernorm3[0][0]  \n",
      "                                                                 dropout_126[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer6_layernorm1 (Laye (None, 8, 512)       8192        decoder_layer6_residual1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer6_mha2 (MultiHeadA (None, 8, 512)       262144      decoder_layer6_layernorm1[0][0]  \n",
      "                                                                 encoder_layer6_layernorm2[0][0]  \n",
      "                                                                 encoder_layer6_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_127 (Dropout)           (None, 8, 512)       0           decoder_layer6_mha2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer6_residual2 (Add)  (None, 8, 512)       0           decoder_layer6_layernorm1[0][0]  \n",
      "                                                                 dropout_127[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer6_layernorm2 (Laye (None, 8, 512)       8192        decoder_layer6_residual2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer6_ffn1 (Dense)     (None, 8, 512)       262656      decoder_layer6_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer6_ffn2 (Dense)     (None, 8, 512)       262656      decoder_layer6_ffn1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_128 (Dropout)           (None, 8, 512)       0           decoder_layer6_ffn2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer6_residual3 (Add)  (None, 8, 512)       0           decoder_layer6_layernorm2[0][0]  \n",
      "                                                                 dropout_128[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer6_layernorm3 (Laye (None, 8, 512)       8192        decoder_layer6_residual3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "shared_weights_4 (SharedWeights (None, 8, 10000)     0           decoder_layer6_layernorm3[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 16,388,096\n",
      "Trainable params: 16,388,096\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras.backend as K\n",
    "# def loss(y_true, y_pred):\n",
    "#    return K.categorical_crossentropy(y_true[:,-1:,:], y_pred[:,-1:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 'categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRScheduler:\n",
    "    def __init__(self, d_model, warmup_steps):\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.epoch = 1\n",
    "\n",
    "    def lr(self, epoch):\n",
    "        lr = self.d_model**-.5 * min(self.epoch**-.5, epoch*(self.warmup_steps**-1.5))\n",
    "        self.epoch += 1\n",
    "        return lr\n",
    "lr_scheduler = LRScheduler(d_model, warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler\n",
    "callbacks.append(LearningRateScheduler(lr_scheduler.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import adam\n",
    "model.compile(loss=loss, optimizer=adam(lr=1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method ScopedTFStatus.__del__ of <tensorflow.python.framework.c_api_util.ScopedTFStatus object at 0x7f8e0c02a358>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dante/venvs/default/lib/python3.5/site-packages/tensorflow/python/framework/c_api_util.py\", line 36, in __del__\n",
      "    c_api.TF_DeleteStatus(self.status)\n",
      "AttributeError: 'ScopedTFStatus' object has no attribute 'status'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "65/65 [==============================] - 15s 230ms/step - loss: 9.2157\n",
      "Epoch 2/500\n",
      "65/65 [==============================] - 11s 169ms/step - loss: 9.1639\n",
      "Epoch 3/500\n",
      "65/65 [==============================] - 11s 166ms/step - loss: 8.9975\n",
      "Epoch 4/500\n",
      "65/65 [==============================] - 11s 169ms/step - loss: 8.7227\n",
      "Epoch 5/500\n",
      "65/65 [==============================] - 11s 168ms/step - loss: 8.3949\n",
      "Epoch 6/500\n",
      "65/65 [==============================] - 11s 167ms/step - loss: 7.9648\n",
      "Epoch 7/500\n",
      "65/65 [==============================] - 11s 166ms/step - loss: 7.3920\n",
      "Epoch 8/500\n",
      "65/65 [==============================] - 11s 167ms/step - loss: 6.6762\n",
      "Epoch 9/500\n",
      "65/65 [==============================] - 11s 170ms/step - loss: 6.0043\n",
      "Epoch 10/500\n",
      "65/65 [==============================] - 11s 166ms/step - loss: 5.5248\n",
      "Epoch 11/500\n",
      "65/65 [==============================] - 11s 168ms/step - loss: 5.2568\n",
      "Epoch 12/500\n",
      "65/65 [==============================] - 11s 171ms/step - loss: 5.0521\n",
      "Epoch 13/500\n",
      "65/65 [==============================] - 11s 169ms/step - loss: 4.8866\n",
      "Epoch 14/500\n",
      "65/65 [==============================] - 11s 168ms/step - loss: 4.7217\n",
      "Epoch 15/500\n",
      "65/65 [==============================] - 11s 167ms/step - loss: 4.6028\n",
      "Epoch 16/500\n",
      "65/65 [==============================] - 11s 165ms/step - loss: 4.3969\n",
      "Epoch 17/500\n",
      "65/65 [==============================] - 11s 166ms/step - loss: 4.2862\n",
      "Epoch 18/500\n",
      "65/65 [==============================] - 11s 168ms/step - loss: 4.0937\n",
      "Epoch 19/500\n",
      "65/65 [==============================] - 11s 169ms/step - loss: 3.9557\n",
      "Epoch 20/500\n",
      "65/65 [==============================] - 11s 170ms/step - loss: 3.8183\n",
      "Epoch 21/500\n",
      "65/65 [==============================] - 11s 168ms/step - loss: 3.6724\n",
      "Epoch 22/500\n",
      "65/65 [==============================] - 11s 169ms/step - loss: 3.6669\n",
      "Epoch 23/500\n",
      "65/65 [==============================] - 11s 167ms/step - loss: 3.4114\n",
      "Epoch 24/500\n",
      "65/65 [==============================] - 11s 167ms/step - loss: 3.4937\n",
      "Epoch 25/500\n",
      "65/65 [==============================] - 11s 165ms/step - loss: 3.3795\n",
      "Epoch 26/500\n",
      "65/65 [==============================] - 11s 168ms/step - loss: 3.3959\n",
      "Epoch 27/500\n",
      "65/65 [==============================] - 11s 168ms/step - loss: 3.3019\n",
      "Epoch 28/500\n",
      "65/65 [==============================] - 11s 168ms/step - loss: 3.2975\n",
      "Epoch 29/500\n",
      "14/65 [=====>........................] - ETA: 8s - loss: 3.1450"
     ]
    }
   ],
   "source": [
    "n_epochs = 500\n",
    "model.fit_generator(train_gen, steps_per_epoch=steps_per_epoch, epochs=n_epochs,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed = ['you']\n",
    "tokens = [[tokenizer.word_index[i] for i in seed]]\n",
    "s = pad_sequences(tokens, maxlen=sequence_len)\n",
    "for _ in range(30):\n",
    "    X = [s, s]\n",
    "    y_hat = model.predict(X)\n",
    "    i = np.argmax(y_hat[0][-1])\n",
    "    tokens[0].append(i)\n",
    "    s = pad_sequences(tokens, maxlen=30)\n",
    "' '.join([index_to_word.get(x, '<pad>') for x in tokens[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = ['you', 'are']\n",
    "tokens = [[tokenizer.word_index[i] for i in seed]]\n",
    "print(tokens)\n",
    "s = pad_sequences(tokens, maxlen=sequence_len)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, _ = X\n",
    "[index_to_word.get(i, '<pad>') for i in x1[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = [s, s]\n",
    "y_hat = model.predict(X)\n",
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[index_to_word.get(i, '<pad>') for i in np.argmax(y_hat[-1], axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(y_hat[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
